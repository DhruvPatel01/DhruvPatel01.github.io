<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on Dhruv Patel</title><link>https://dhruvpatel.dev/notes/</link><description>Recent content in Notes on Dhruv Patel</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Dhruv Patel</copyright><lastBuildDate>Sat, 14 Jan 2023 04:02:28 +0000</lastBuildDate><atom:link href="https://dhruvpatel.dev/notes/index.xml" rel="self" type="application/rss+xml"/><item><title>Quantum Mechanics Notes</title><link>https://dhruvpatel.dev/notes/physics/theoretical_minimum/qm-notes/</link><pubDate>Sat, 14 Jan 2023 04:02:28 +0000</pubDate><guid>https://dhruvpatel.dev/notes/physics/theoretical_minimum/qm-notes/</guid><description>Quantum Mechanics Differences from Classical Mechanics States have different logical structure than CM. States and measurements are different unlike CM. e.g., Position and Momemntum can be determined by experiments in CM. Spins Particles have properties attached to it. e.g., mass, electric charge.
Even a specific particle is not completely specified by its position.
Attached to electron is an extra degree of freedom, called spin. Spin is as quantum mechanical as it can and we should not try to visualize it.</description><content>&lt;h1 id="quantum-mechanics">Quantum Mechanics&lt;/h1>
&lt;h2 id="differences-from-classical-mechanics">Differences from Classical Mechanics&lt;/h2>
&lt;ol>
&lt;li>States have different logical structure than CM.&lt;/li>
&lt;li>States and measurements are different unlike CM. e.g., Position and Momemntum can be determined by experiments in CM.&lt;/li>
&lt;/ol>
&lt;h2 id="spins">Spins&lt;/h2>
&lt;p>Particles have properties attached to it. e.g., mass, electric charge.&lt;/p>
&lt;p>Even a specific particle is not completely specified by its position.&lt;/p>
&lt;p>Attached to electron is an extra degree of freedom, called spin. Spin is as quantum mechanical as it can and we should not try to visualize it.&lt;/p>
&lt;h2 id="testing">Testing&lt;/h2>
&lt;p>Propositions:
$$
\begin{align*}
A &amp;amp;: \sigma_z = +1 \\
B &amp;amp;: \sigma_x = +1 \\
\end{align*}
$$&lt;/p>
&lt;h3 id="classically">Classically,&lt;/h3>
&lt;p>To test (A or B), one could first &lt;strong>gently&lt;/strong> test $\sigma_z$. If it is -1, one would &lt;strong>gently&lt;/strong> test $\sigma_x$. The result of doing it otherway (i.e., B or A) will be the same as doing (A or B). The reason is that classically, measurements are gentle. They don&amp;rsquo;t change the state of the system.&lt;/p>
&lt;h3 id="in-quantum-mechanics">In Quantum Mechanics,&lt;/h3>
&lt;p>If some entity prepares the spin in $\sigma_z = +1$ state, and we measure &lt;code>A or B&lt;/code> (whether we use short circuit or not), we will measure it to be true. However, if we measure &lt;code>B or A&lt;/code>, there is 25% chance that we will measure it to be false.&lt;/p>
&lt;p>What about &lt;code>A and B&lt;/code>? If we conclude that &lt;code>A and B&lt;/code> is true, can we confirm it again? Answer is no. Since to compute B, we had to measure $\sigma_x$, which ruined measurement of A. Thus we can&amp;rsquo;t confirm it. i.e., experiment is not reproducible.&lt;/p>
&lt;h2 id="complex-numbers">Complex Numbers&lt;/h2>
&lt;p>Two ways to represent them.&lt;/p>
&lt;p>In cartesian coordinates, $z = x + iy$.&lt;/p>
&lt;p>In polar coordinates, $z = re^{i\theta}$.&lt;/p>
&lt;p>$ x_1 x_2 = (r_1e^{i\theta_1})(r_2e^{i\theta_2}) = r_1 r_2 e^{i(\theta_1 + \theta_2)}$.&lt;/p>
&lt;p>$z = x + iy = re^{i\theta}$&lt;/p>
&lt;p>$z^\ast = x - iy = re^{-i\theta}$&lt;/p>
&lt;p>$z^\ast z = r^2$, i.e., a real number&lt;/p>
&lt;h3 id="phase-factors">Phase Factors&lt;/h3>
&lt;p>These are complex numbers whose r componenet is 1. Following holds for them,&lt;/p>
&lt;p>$$
\begin{align}
z^\ast z &amp;amp;= 1 \\
z &amp;amp;= e^{i\theta}\\
z &amp;amp;= \cos\theta + i \sin\theta
\end{align}
$$&lt;/p>
&lt;p>$$\renewcommand{\bra}[1]{\left\langle{#1}\right|}$$
$$\renewcommand{\ket}[1]{\left|{#1}\right\rangle}$$
$$\renewcommand{\braket}[1]{\left\langle{#1}\right\rangle}$$&lt;/p>
&lt;h2 id="vector-spaces">Vector Spaces&lt;/h2>
&lt;p>Vector spaces is familiar concept from abstract linear algebra.&lt;/p>
&lt;h3 id="complex-conjugate-of-space-v">Complex Conjugate of space V&lt;/h3>
&lt;p>For every $\ket{A}$ there exists $\bra{A}$ in conjugate space. This space has following properties.&lt;/p>
&lt;ol>
&lt;li>The conjugate of $\ket{A} + \ket{B}$ is $\bra{A}+\bra{B}$.&lt;/li>
&lt;li>Conjugate of $z\ket{A}$ is $z^\ast \bra{A} = \bra{A}z^\ast $.&lt;/li>
&lt;/ol>
&lt;p>In the concrete case where ket space is column vectors, bra space is denoted as row vectors.&lt;/p>
&lt;p>i.e., if&lt;/p>
&lt;p>$$
\begin{align*}
\ket{A} = \begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
\vdots \\
\alpha_n
\end{bmatrix}
\end{align*}
$$&lt;/p>
&lt;p>then
$$
\begin{align*}
\bra{A} = \begin{bmatrix}
\alpha_1^\ast &amp;amp; \alpha_2^\ast &amp;amp; \dots &amp;amp; \alpha_n^\ast
\end{bmatrix}.
\end{align*}
$$&lt;/p>
&lt;h3 id="inner-products">Inner Products&lt;/h3>
&lt;p>Inner product is always between bras and kets. It is written like $\braket{B|A}$. The result is a complex numbers.&lt;/p>
&lt;h4 id="axioms-of-inner-product">Axioms of inner product.&lt;/h4>
&lt;ol>
&lt;li>Inner product is linear. $\bra{C} + (\ket{A}+\ket{B}) = \braket{C|A} + \braket{C|B}$.&lt;/li>
&lt;li>$\braket{B|A} = \braket{A|B}^\ast $.&lt;/li>
&lt;/ol>
&lt;h4 id="special-vectors">Special vectors&lt;/h4>
&lt;ol>
&lt;li>Normalized: $\braket{A|A} = 1$.&lt;/li>
&lt;li>Orhthogonal: $\braket{B|A} = 0$.&lt;/li>
&lt;/ol>
&lt;h3 id="orhtonormal-basis">Orhtonormal Basis.&lt;/h3>
&lt;p>Let our space be N dimensional. And let the orthonomal basis denoted by $\ket{i}$.&lt;/p>
&lt;p>$$
\ket{A} = \sum_i \alpha_i \ket{i},
$$&lt;/p>
&lt;p>where, $\alpha_j = \braket{j|A}$. (To derive this, multiply both sides by $\bra{j}$.)&lt;/p>
&lt;h1 id="states">States&lt;/h1>
&lt;p>In CM, knowing the state means knowing everything that is necessary to predict the future.&lt;/p>
&lt;p>In QM, knowing the state means knowing as much as can be known about how the system was prepared.&lt;/p>
&lt;p>Apparatus $\cal{A}$ can be oriented on any axis. If we orient it along z axis, the measured spin will either be +1 or -1.&lt;/p>
&lt;p>$\sigma_z = \pm 1$. We can denote +1 as state $\ket{u}$ and -1 as $\ket{d}$.&lt;/p>
&lt;p>Similarly $\sigma_x = \pm 1$, can be denoted by $\ket{r}$ and -1 as $\ket{l}$. And $\sigma_y = \pm 1$, can be denoted by $\ket{o}$ and -1 as $\ket{i}$.&lt;/p>
&lt;p>If two states are orthogonal then these two states can be determined together. For example, if $\sigma_z$ was prepared to be in $\ket{u}$, for any subsequent measurements the probability that $\ket{d}$ is detected is 0. Thus, for binary spin, the state space is two dimensional. For now we can take $\ket{u}, \ket{d}$ as the basis vectors.&lt;/p>
&lt;p>Then, the generic state $\ket{A} = \alpha_u \ket{u} + \alpha_d \ket{d}$. Where $\alpha_i = \braket{i|A}$.&lt;/p>
&lt;p>The meaning of,&lt;/p>
&lt;ul>
&lt;li>$\alpha_u^\ast \alpha_u$: If the spin was prepared in $\ket{A}$ state, $\alpha_u^\ast \alpha_u$ is the probability that $\sigma_z = +1$.&lt;/li>
&lt;li>$\alpha_d^\ast \alpha_d$: is the probability that $\sigma_z = -1$.&lt;/li>
&lt;/ul>
&lt;p>Since probabilities must add to 1, $\alpha_u^\ast \alpha_u + \alpha_d^\ast \alpha_d = 1$. It is equivalent to saying that $\ket{A}$ is normalized, i.e., $\braket{A|A} = 1$.&lt;/p>
&lt;p>General principle of quantum systems: the state of a system is represented by a unit (normalized) vector in a vector space of states. Moreover, the squared magnitudes of the components of the state-vector, &lt;strong>along particular basis vectors&lt;/strong>, represent probabilities for various experimental outcomes.&lt;/p>
&lt;h3 id="representing-ketr-and-ketl-using-above-basis-vectors">Representing $\ket{r}$ and $\ket{l}$ using above basis vectors&lt;/h3>
&lt;p>We know that if A initially prepares the state in $\ket{r}$, $\sigma_z = \pm 1$ with equal probability. Hence, $\alpha_u^\ast \alpha_u =\alpha_d^\ast \alpha_d = \frac{1}{2}$. One choice is to have $\alpha_u = \alpha_d = \frac{1}{\sqrt{2}}$.&lt;/p>
&lt;p>$\ket{r} = \frac{1}{\sqrt{2}} \ket{u} + \frac{1}{\sqrt{2}} \ket{d}$. (There are is still ambiguity, called phase ambiguity.)&lt;/p>
&lt;p>To solve for $\ket{l}$ the above process repeats. But, in addition, $\braket{l|r} = 0$. One choice is $[\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}]$. But it is not the only choice. Even for fixed choice for $\ket{r}$, you can multiply above choice by a phase factor ($z = e^{i\theta}$), and still satisfy the two constraints. Later, we will find out that no measurable quantity is sensitive to the overall phase-factor, and therefore we can ignore it when specifying states.&lt;/p>
&lt;h3 id="representing-keti-and-keto-using-above-basis-vectors">Representing $\ket{i}$ and $\ket{o}$ using above basis vectors&lt;/h3>
&lt;p>To solve for $\ket{i}$ and $\ket{o}$, we need same conditions as we needed above. But we also need additional constrains. For example, if A prepares the state in $\ket{i}$,$\sigma_x = \pm1$, with equal probability. Also $\braket{i|o} = 0$.&lt;/p>
&lt;p>The following solution solves for these constraints (up to phase-factor ambiguity).&lt;/p>
&lt;p>$\ket{i} = \frac{1}{\sqrt{2}} \ket{u} + \frac{i}{\sqrt{2}} \ket{d}$.&lt;/p>
&lt;p>$\ket{o} = \frac{1}{\sqrt{2}} \ket{u} - \frac{i}{\sqrt{2}} \ket{d}$.&lt;/p>
&lt;p>With the previous discussion, the vectors can be represented in column format as below.&lt;/p>
&lt;p>$$
\begin{align*}
\ket{u} &amp;amp;= \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \ket{d} = \begin{bmatrix} 0 \\ 1 \end{bmatrix} \\
\end{align*}
$$&lt;/p>
&lt;h3 id="matricies">Matricies&lt;/h3>
&lt;p>If we have a basis $\ket{i}$, a vector $\ket{A}$ can be rewritten as,&lt;/p>
&lt;p>$$ \ket{A} = \sum_i \alpha_i \ket{i} = \sum_i \ket{i} \braket{i|A} $$.&lt;/p>
&lt;p>Similarly,
$$ \bra{A} = \sum_i \braket{A|i}\bra{i} $$.&lt;/p>
&lt;p>Axiom: Physical observables are described by linear operators.&lt;/p>
&lt;p>Observables are the things that we can measure. e.g., coordinates of a particle; the energy, momentum, or angular momentum of a system; or the electric field at a point in space.&lt;/p>
&lt;p>$$
\begin{align*}
M \ket{A} &amp;amp;= \ket{B} \\
M \sum_j \alpha_j \ket{j} &amp;amp;= \sum_j \beta_j \ket{j} \\
\sum_j \alpha_j M\ket{j} &amp;amp;= \sum_j \beta_j \ket{j} \text{;;assuming M is linear} \\
\sum_j \alpha_j \bra{k} M \ket{j} &amp;amp;= \sum_j \beta_j \braket{k|j} \text{;;multiply both sides by} \bra{k} \\
\sum_j \alpha_j m_{kj} &amp;amp;= \beta_k\\
\end{align*}
$$&lt;/p>
&lt;p>Note that each $m_{kj}$ is a complex number. We can think of M in terms of matrix (defined by a choice of basis vectors).&lt;/p>
&lt;h4 id="eigenvectors-and-eigenvalues">Eigenvectors and Eigenvalues&lt;/h4>
&lt;p>$M \ket{\lambda} = \lambda \ket{\lambda}$. $\lambda$ is an eigenvalue, and $\ket{\lambda}$ is an eigenvector.&lt;/p>
&lt;h4 id="linear-operators-on-bra-vectors">Linear operators on bra vectors&lt;/h4>
&lt;p>$$
\begin{align*}
\bra{B} &amp;amp;= \begin{bmatrix} b_1^\ast &amp;amp; b_2^\ast &amp;amp; b_3^\ast \end{bmatrix} \\
M &amp;amp;= \begin{bmatrix}
m_{11} &amp;amp; m_{12} &amp;amp; m_{13} \\
m_{21} &amp;amp; m_{22} &amp;amp; m_{23} \\
m_{31} &amp;amp; m_{32} &amp;amp; m_{33} \\
\end{bmatrix}
\end{align*}
$$&lt;/p>
&lt;p>Than $\bra{B} M$ is just row vector multiplied by matrix M.&lt;/p>
&lt;h4 id="hermitian-conjugate">Hermitian Conjugate&lt;/h4>
&lt;p>$$
\begin{align*}
M^\dagger &amp;amp;= (M^T)^\ast \\
M \ket{A} &amp;amp;= \ket{B} \\
\bra{A} M^\dagger &amp;amp;= \bra{B} \\
\end{align*}
$$&lt;/p>
&lt;h4 id="hermitian-operators">Hermitian Operators&lt;/h4>
&lt;ul>
&lt;li>Observables quantities in classcial mechanics are real numbers. i.e., they are their own complex conjugate.&lt;/li>
&lt;li>Observables in quantum mechanics (i.e., linear operators) are also their own complex conjugates. Such operators are called Hermitian Operators. $M^\dagger = M$.&lt;/li>
&lt;/ul>
&lt;h5 id="properties-of-hermitian-operators">Properties of Hermitian Operators&lt;/h5>
&lt;ul>
&lt;li>Their eigenvalues are real.&lt;/li>
&lt;li>Their eigenvectors form an orthonormal basis. (i.e., their eigenvectors are orthonormal and they form a basis)&lt;/li>
&lt;/ul>
&lt;h2 id="principles">Principles&lt;/h2>
&lt;ol>
&lt;li>The observable or measurable quantities of QM are represented by a linear operator L.&lt;/li>
&lt;li>The possible readings of the measurements are eigenvalues $\lambda_i$. The state for which reading is &lt;strong>unambiguously&lt;/strong> $\lambda_i$ is the corresponding eigenvector $\ket{\lambda_i}$.&lt;/li>
&lt;li>Unambiguously distinguishable states are represented by orthogonal vectors. e.g., $\braket{u|d} = 0$.&lt;/li>
&lt;li>If $\ket{A}$ is the state vector of the system, and the observable L is measured, the probability of observing $\lambda_i$ is given by $\braket{A|\lambda_i}\braket{\lambda_i|A}$.&lt;/li>
&lt;/ol>
&lt;p>Since the readings (i.e., eigenvalues) are real and eigenvectors are orthogonal, the operator L must be hermitian.&lt;/p>
&lt;p>P1 says that $\sigma_x, \sigma_y, \text{and} \sigma_z$ are identified with a specific linear operator in 2D space of states describing the states.
P2 says that the actual measurments can take discrete values. e.g., energy of atom will be one of the established energy levels of the atom.&lt;/p>
&lt;h3 id="3-vector-operator-sigma">3-Vector Operator $\sigma$&lt;/h3>
&lt;ul>
&lt;li>Just as a spin-measuring apparatus can only answer questions about a spin&amp;rsquo;s orientation in a specific direction, a spin operator can only provide information about the spin component in a specific direction.&lt;/li>
&lt;li>To physically measure spin in a different direction, we need to rotate the apparatus to point in the new direction. The same idea applies to the spin operator—if we want it to tell us about the spin component in a new direction, it too must be &amp;ldquo;rotated&amp;rdquo; but this kind of rotation is accomplished mathematically.&lt;/li>
&lt;/ul>
&lt;h3 id="operator-matrices">Operator Matrices&lt;/h3>
&lt;p>Using above four principles, and solving linear equations, we can derive them.&lt;/p>
&lt;p>$$
\sigma_z = \begin{bmatrix}
1 &amp;amp; 0 \\
0 &amp;amp; -1
\end{bmatrix}, \sigma_x = \begin{bmatrix}
0 &amp;amp; 1 \\
1 &amp;amp; 0
\end{bmatrix}, \sigma_y = \begin{bmatrix}
0 &amp;amp; -i \\
i &amp;amp; 0
\end{bmatrix} \text{.}
$$&lt;/p>
&lt;p>These along with Identity matrix are called Pauli Matrices.&lt;/p>
&lt;p>IMPORTANT: Applying the operator L to state $\ket{A}$ does not change the state to $L\ket{A}$. $L\ket{A}$ is actually a supuerposition and tells us the probabilities of basis states. When we actually measure using L, the system is changed to one of the eignestates unambiguously.&lt;/p>
&lt;p>There is nothing special about these three operators. We can take any direction $\hat{n} = (n_x, n_y, n_z)$, orient the apparatus A along $\hat n$, activate A, and measure the component of the spin along $\hat n$. That means there has to be an operator that represents this operation. Indeed, this operator is given by $\sigma_n = \sigma \cdot \hat n$, where $\sigma = (\sigma_x, \sigma_y, \sigma_z)$.&lt;/p>
&lt;p>$$
\sigma_n = \begin{bmatrix}
n_z &amp;amp; (n_x - i n_y) \\
(n_x + i n_y) &amp;amp; -n_z
\end{bmatrix}
$$&lt;/p>
&lt;h3 id="spin-polarization-principle">Spin-Polarization Principle&lt;/h3>
&lt;p>For any state $\ket{A} = \alpha_u \ket{u} + \alpha_d \ket{d}$, there exists some direction $\hat n$ such that $\sigma \cdot \hat n = \ket{A}$.&lt;/p>
&lt;p>States of the spins are characterized by a polarization vector, and along the polarization vector the component of the spin is predictably +1.&lt;/p>
&lt;p>General spin state is $cos(\frac{\beta}{2}) \ket{u} + e^{i\phi} sin(\frac{\beta}{2}) \ket{d}$.&lt;/p>
&lt;h3 id="time-development-operator">Time Development Operator&lt;/h3>
&lt;p>Let the closed system be in state $\ket{\Psi(t)}$. Thus, state can be different at different times.&lt;/p>
&lt;p>The time development operator $U(t)$ tells us how the system evolves with time.&lt;/p>
&lt;p>$$\ket{\Psi(t)} = U(t) \ket{\Psi(0)}$$.&lt;/p>
&lt;p>Thus, the time evolution of the state is deterministic.&lt;/p>
&lt;p>Quantum mechanics assumes,&lt;/p>
&lt;ol>
&lt;li>that U(t) is a linear operator.&lt;/li>
&lt;li>If two states are orthonormal, they remain orthonormal in the evolution. i.e., $\braket{\Psi(0)|\Phi(0)} =0 \implies \braket{\Psi(t)|\Phi(t)} = 0$.&lt;/li>
&lt;/ol>
&lt;p>As a consequence of these two assumptions, it is easy to prove that $U(t)^{\dagger}U(t) = I$. Such operator is called unitary operator.&lt;/p>
&lt;p>Principle 5: Time evolution of state vectors is unitary.&lt;/p>
&lt;p>Unitarity also implies that as time evolves, the overlap (inner product) between two states remains the same.&lt;/p>
&lt;p>Quantum Mechanics also assumes that time evolution is continuous.&lt;/p>
&lt;p>Thus, for small time $\epsilon, U(\epsilon) = I - i \epsilon H$. Using the unitarity condition $U(t)^{\dagger}U(t) = I$, we can show that $H^{\dagger} = H$.&lt;/p>
&lt;p>Thus, H is hermitian. i.e., it is an obeservable, and has complete set of orthonormal eigenvectors.&lt;/p>
&lt;p>We will see that, the eigenvalues of H are the values that result from measuring the energy levels of the quantum system. Thus, it has resemblance to the hamiltonian from the classical mechanics.&lt;/p>
&lt;p>Using the continuity assumption inside $\ket{\Psi(t)} = U(t) \ket{\Psi(0)}$, we can derive,&lt;/p>
&lt;p>$$ \frac{d \ket{\Psi}}{dt} = -iH\ket{\Psi}$$.&lt;/p>
&lt;p>This equation is known as Generalized Schrodinger equation or time-dependent schrodinger&amp;rsquo;s equation. If we know the Hamiltonian of the system, we can know how the state of the undisturbed system evolves with the time.&lt;/p>
&lt;h4 id="planks-constant">Plank&amp;rsquo;s constant&lt;/h4>
&lt;p>h = 6.6 * 1e-34 kgm^2/s1.&lt;/p>
&lt;p>$\hbar = \frac{h}{2\pi} = 1.054571726 \dots x 10^{-34} \frac{kgm^2}{s}$&lt;/p>
&lt;p>In the gen. schrodinger&amp;rsquo;s eqn, lhs has units of 1/time, whereas rhs has units of energy (kgm^2/s^2, because H is hamiltonian). This is wrong. However multiplying lhs by Plank&amp;rsquo;s constant, units are proper. Thus, the correct Generalized schrodinger&amp;rsquo;s equation is $$ \hbar \frac{d \ket{\Psi}}{dt} = -iH\ket{\Psi}$$.&lt;/p>
&lt;h4 id="expectation">Expectation&lt;/h4>
&lt;p>If L is an observable, the expectation is defined as $\braket{L} = \sum_i P(\lambda_i) \lambda_i$.&lt;/p>
&lt;p>When state of the system is $\ket{A} = \sum_i \alpha_i \lambda_i$, the expecation can be computed as $\braket{L} = \sum_i \alpha_i^\ast \alpha_i \lambda_i = \braket{A|L|A}$.&lt;/p>
&lt;p>Try to apply this for state $\ket{r}$ and observable =$\sigma_z$. The arithmetic should result in value 0.&lt;/p>
&lt;h4 id="effect-of-the-phase-factor">Effect of the phase factor.&lt;/h4>
&lt;p>We can multiply the state vectors by a phase factor $e^{i\theta}$ for any $\theta$. This does no make any difference. Although, probability amplitude will change i.e., $\alpha_i \to e^{i\theta} \alpha_i$, the probability does not change, i.e., $\alpha_i^\ast \alpha_i$ remains unchanged. Similarly, the expectation of the observable does not change.&lt;/p>
&lt;h4 id="change-in-the-expectation">Change in the expectation&lt;/h4>
&lt;p>$\frac{d \braket{\Psi(t)|L|\Psi(t)}}{dt} = \braket{\dot\Psi(t)|L|\Psi(t)} + \braket{\Psi(t)|L|\dot\Psi(t)}$. Using generalized schrodinger&amp;rsquo;s equations,&lt;/p>
&lt;p>$\frac{d \braket{\Psi(t)|L|\Psi(t)}}{dt} = \frac{i}{\hbar} \braket{\Psi(t)|HL - LH|\Psi(t)}$.&lt;/p>
&lt;p>Linear operators don&amp;rsquo;t commute, so HL != LH.&lt;/p>
&lt;p>&lt;strong>Commutator&lt;/strong>: Given two operators L, and M, LM - ML is called the commutator of L with M, and is denoted by [L, M]. Note that [L, M] = -[M, L].&lt;/p>
&lt;p>Thus, $\frac{d}{dt} \braket{L} = \frac{-i}{\hbar} \braket{[L, H]}$.&lt;/p>
&lt;p>It is easy to prove that $i[L, H]$ is also Hermitian. Thus, a valid observable.&lt;/p>
&lt;p>This has resemblance to classical mechanics. $\dot{F} = \{F, H\}$.&lt;/p>
&lt;h3 id="conservation-in-quantum-mechanics">Conservation in quantum mechanics&lt;/h3>
&lt;p>To say that an observable Q is conserved is to say that expected value $\braket{Q}$ does not change with time. A stronger condition is that any moment $\braket{Q^m}$ does not change with time.&lt;/p>
&lt;p>$\braket{Q}$ does not change ammounts to $[Q, H] = 0$. That is Q commutes with H. Using the properties of commutator, it is easy to prove that $[Q^m, H] = 0$ for any $m \ge 1$.&lt;/p>
&lt;p>It turns out that if $[Q, H] = 0$ then for &lt;strong>any&lt;/strong> function of Q, $[f(Q), H] = 0$.&lt;/p>
&lt;p>H is also conserved, as $[H, H] = 0$. H is defined to be the energy of the quantum system.&lt;/p>
&lt;h3 id="solving-schrodingers-equation">Solving Schrodinger&amp;rsquo;s equation&lt;/h3>
&lt;p>Time dependent Schrodinger&amp;rsquo;s equation is $ \hbar \frac{d \ket{\Psi}}{dt} = -iH\ket{\Psi}$.&lt;/p>
&lt;p>Since H represents enrgy, the oberservable values of energy are eigenvalues of H. Call them $\ket{E_j}$ and $E_j$.&lt;/p>
&lt;p>$$
H \ket{E_j} = E_j \ket{E_j}
$$&lt;/p>
&lt;p>These are call time-independent schrodinger&amp;rsquo;s equations.&lt;/p>
&lt;p>We can write $\ket{\Psi{(t)}} = \sum_j \alpha_j(t) \ket{E_j}$.&lt;/p>
&lt;p>Thus, $\ket{\dot\Psi{(t)}} = \sum_j \dot\alpha_j(t) \ket{E_j}$.&lt;/p>
&lt;p>Using time-dependent Schrodinger&amp;rsquo;s equation, we can solve for $\dot\alpha_j(t)$. It has the form $\dot\alpha_j(t) = -\frac{i}{\hbar}E_j \alpha_j(t)$. Which has the solution $\alpha_j(t) = \alpha_j(0) e^{\frac{-i}{\hbar} E_j t}$.&lt;/p>
&lt;p>But, $\alpha_j(0) = \braket{E_j|\Psi(0)}$.&lt;/p>
&lt;p>So, $\Psi(t) = \sum_j \ket{E_j}\braket{E_j|\Psi(0)} e^{-\frac{i}{\hbar}E_j t}$.&lt;/p>
&lt;h4 id="general-recipe">General Recipe&lt;/h4>
&lt;ol>
&lt;li>Get the H somehow.&lt;/li>
&lt;li>Find the $\ket{E_j}$ and $E_j$.&lt;/li>
&lt;li>Prepare the system in initial state $\Psi(0)$.&lt;/li>
&lt;li>Use above solution to find the state at any later time.&lt;/li>
&lt;li>If you have some other operator L, you can &amp;ldquo;predict&amp;rdquo; the outcome of measuring future state using the eigenvectors of L. i.e., the probability the outcome of measuring L is $\lambda_i$ is precisely $|\braket{\lambda_i|\Psi(t)}|^2$.&lt;/li>
&lt;/ol>
&lt;h2 id="complex-systems">Complex systems&lt;/h2>
&lt;p>There are systems for which there are more than two observables, and they all can be measured simultaneously. Single spin is &lt;strong>not&lt;/strong> such system. Though there are three observables $\sigma_{\{x,y,z\}}$. Only one of them can be observed at a time.&lt;/p>
&lt;p>Particle moving in three dimensions, all three dimensions x, y, and z can be measured simultaneously.&lt;/p>
&lt;p>Other such system is a system of two particles. Let&amp;rsquo;s denote these two observables as L and M.
If we measure both spins, we leave the system in a state which is simultaneously eigenvector of both L and M. Let the eigenvectors of L be denoted as $\lambda_i$ and same for M with $\mu_a$.&lt;/p>
&lt;p>In order to have simultaneous eigenvectors $\ket{\lambda, \mu}$, L and M must commute with each other. i.e., $LM \ket{\lambda, \mu} = ML\ket{\lambda, \mu}$. i.e., $[L, M]\ket{\lambda, \mu} = 0$. Since this statement is true of any basis eigenvector $\ket{\lambda, \mu}$, we know that $[L, M] = 0$.&lt;/p>
&lt;p>Thus, if we have complete basis of simultaneous eigenvectors, the observables commute. Converse is also true. If observables commute, they have a complete basis of simultaneous eigenvectors. This statement is true for any number of observables in the system, and not just two.&lt;/p>
&lt;p>Suppose, we have a basis of states $\ket{a,b,c, \dots}$ for some complex system.&lt;/p>
&lt;p>$$\ket{\Psi} = \sum_{a,b,c,\dots} \psi{(a,b,c, \dots)} \ket{a,b,c, \dots}$$&lt;/p>
&lt;p>Where, $\psi{(a,b,c, \dots)} = \braket{a,b,c, \dots|\Psi}$. This $\psi$ is called a wave function.&lt;/p>
&lt;p>$P(a,b,c, \dots) = \psi^\ast{(a,b,c, \dots)} \psi{(a,b,c, \dots)}$. This is the probability of observing the value $a,b,c,\dots$ of commuting observables $A,B,C,\dots$.&lt;/p>
&lt;p>If a state is an eigenvector of operator A, which does not commute with B, then it will not be an eigenvector for B. Thus, if A and B don&amp;rsquo;t commute, there will be uncertainity in either A or B if not both.&lt;/p>
&lt;h3 id="quantifying-uncertainty">Quantifying Uncertainty&lt;/h3>
&lt;p>The system is in state $\ket\Psi$. We want to observe some A. A can be thought as a random variable. The possible outcomes are eigenvalues of A, with predefined probabilities. Thus $\braket{A}$ is well defined, and is the expected value of A. We can also find the variance of A! This quantifies the uncertainty of A.&lt;/p>
&lt;p>First define $\bar{A} = A - \braket{A}I$. This is also an Hermitian operator, so has real eigenvalues. If $a$ is an eigenvalue of A, $\bar{a} = a - \braket{A}$ is an eigenvalue of $\bar{A}$.&lt;/p>
&lt;p>Thus the variance, denoted as $(\Delta A)^2 = \sum_a \bar{a}^2 P(a)$.&lt;/p>
&lt;p>A little bit of algebra shows that this is equivalent to $\braket{\bar{A}^2} = \braket{\Psi|\bar{A}^2|\Psi}$.&lt;/p>
&lt;h3 id="uncertainty-principle">Uncertainty Principle&lt;/h3>
&lt;p>From triangle inequality $|x| + |y| \ge |x+y|$, one can derive $2|x||y| \ge |\braket{x|y} + \braket{y|x}|$. (Hint: Square both sides, and expand).&lt;/p>
&lt;p>Putting, $x = A\ket\Psi$ and $y = iB\ket\Psi$, we get $\Delta{A} \Delta{B} \ge \frac{1}{2} | \braket{\Psi|[A, B]|\Psi}$. (Hint: Though formula is true in general, it is easy to derive by assuming that $\braket{A} = \braket{B} = 0$, in which case $(\Delta{A})^2 = \braket{A^2}$.)&lt;/p>
&lt;h2 id="composite-system">Composite System&lt;/h2>
&lt;p>Atom is made of nucleons and electrons, each is a quantum system of it&amp;rsquo;s own. We want to study composite system.&lt;/p>
&lt;p>For the moment we assume there are two systems, A and B (a.k.a. Alices&amp;rsquo;s sytem and Bob&amp;rsquo;s sytem, respectively).
A has states from staet space $S_A$, which has dimensionality of $N_A$. Same for system B.&lt;/p>
&lt;p>The combined system has statespace $S = S_A \otimes S_B$, which has dimensionality $N_A N_B$. $\otimes$ is a tensor product. The states of S are denoted as $\ket{ab}$, where $a \in S_A$ and $b \in S_B$.&lt;/p>
&lt;p>If an operator M acts on the states of the composite system, M has $N_A N_B$ rows and colums. The entry $M_{a&amp;rsquo;b&amp;rsquo;, ab}$ is given by $\braket{a&amp;rsquo;b&amp;rsquo;|M|ab}$.&lt;/p>
&lt;p>If A, and B are 2x2 matrices, their product is defined as
$$\left[\begin{matrix}A_{11} B_{11} &amp;amp; A_{11} B_{12} &amp;amp; A_{12} B_{11} &amp;amp; A_{12} B_{12}\\ A_{11} B_{21} &amp;amp; A_{11} B_{22} &amp;amp; A_{12} B_{21} &amp;amp; A_{12} B_{22}\\ A_{21} B_{11} &amp;amp; A_{21} B_{12} &amp;amp; A_{22} B_{11} &amp;amp; A_{22} B_{12}\\ A_{21} B_{21} &amp;amp; A_{21} B_{22} &amp;amp; A_{22} B_{21} &amp;amp; A_{22} B_{22}\end{matrix}\right]
$$&lt;/p>
&lt;p>The basis vectors $\ket{ab}$ are taken to be orthonormal. $\braket{ab|a&amp;rsquo;b&amp;rsquo;} = [a = a&amp;rsquo;, b = b&amp;rsquo;]$.&lt;/p>
&lt;p>Thus, any state in the composite system is written as $\ket\Psi = \sum_{a,b} \psi(a, b) \ket{ab}$.&lt;/p>
&lt;h3 id="two-spin-system">Two Spin System&lt;/h3>
&lt;p>Let&amp;rsquo;s have four state vectors $\ket{uu}, \ket{ud}, \ket{du}, \ket{dd}$.&lt;/p>
&lt;h4 id="product-state">Product State&lt;/h4>
&lt;p>A product state is the result of completely independent preparations of two spins, each with its&amp;rsquo; own apparatus to prepare a spin. If the state is not a product state than it is entangled.&lt;/p>
&lt;p>Suppose spin A is prepared in $\alpha_u \ket{u} + \alpha_d \ket{d}$ and B is prepared in $\beta_u \ket{u} + \beta_d \ket{d}$ Where $\alpha^\ast_u \alpha_u + \alpha^\ast_d \alpha_d = \beta^\ast_u \beta_u + \beta^\ast_d \beta_d = 1$. In such case we need 4 real parameters to define the state (2 for each spin).&lt;/p>
&lt;p>Thus, the combined product state is written as $\{\alpha_u \ket{u} + \alpha_d \ket{d} \} \otimes \{ \beta_u \ket{u} + \beta_d \ket{d} \}$. This can be expanded, to rewrite in the basis vectors of the combined system, as $\alpha_u \beta_u \ket{uu} + \alpha_u \beta_d \ket{ud} + \alpha_d \beta_u \ket{du} + \alpha_d \beta_d \ket{dd}$.&lt;/p>
&lt;p>The main feature of a product state is that each subsystem behaves independently of the other.&lt;/p>
&lt;h4 id="general-tensor-product-space">General Tensor product space.&lt;/h4>
&lt;p>The general space is written as $\psi_{uu} \ket{uu} + \psi_{ud} \ket{ud} + \psi_{du} \ket{du} + \psi_{dd} \ket{dd}$. Unlike product space, we need 6 real parameters (8(to define complex numbers) - 1 (normalization) - 1(phase factor)).&lt;/p>
&lt;p>Thus, there has to be some states which can not be written as product state. Singlet, and triplets are such states.&lt;/p>
&lt;p>Singlet: $\ket{sing} = \frac{1}{\sqrt{2}} (\ket{ud} - \ket{du})$.&lt;/p>
&lt;p>Triplets:&lt;/p>
&lt;ol>
&lt;li>$T_1 = \frac{1}{\sqrt{2}} (\ket{ud} + \ket{du})$.&lt;/li>
&lt;li>$T_2 = \frac{1}{\sqrt{2}} (\ket{uu} + \ket{dd})$.&lt;/li>
&lt;li>$T_3 = \frac{1}{\sqrt{2}} (\ket{uu} - \ket{dd})$.&lt;/li>
&lt;/ol>
&lt;p>It is easy to prove that these states can&amp;rsquo;t be written as product states. (Hint: for product state you have to show that $\alpha_x \beta_y = c$, but this will be impossible as some other requirement $\alpha_x \beta_w = 0$ or $\alpha_z \beta_y = 0$ will be violated.)&lt;/p>
&lt;p>These states are maximally entangled.&lt;/p>
&lt;h3 id="observables-in-combined-system">Observables in combined system.&lt;/h3>
&lt;p>The observables in the case of single sping system, still apply in the combined system. They just modify their half.&lt;/p>
&lt;p>e.g., if $\sigma_x$ is observable of spin A, $\sigma_x \ket{ud} = \ket{dd}$.&lt;/p>
&lt;p>Note that $\sigma_x$ for single spin system and $\sigma_x$ for combined systems are different operators. This is easy to see in the matrix forms. The two matrices are different.&lt;/p>
&lt;p>In the case of single spin system (or product state), there is a direction where the measurement is guaranteed to be +1. Thus, $\braket{\vec{\sigma} \cdot \vec{n}} = \braket{\Psi|\vec{\sigma} \cdot \vec{n}|\Psi} = 1$, for that particular $\vec{n}$. Thus we know that none of the $\braket{\sigma_x}, \braket{\sigma_y}, \braket{\sigma_z}$ can be zero(which would lead to contradiction).&lt;/p>
&lt;p>On the other hand, if you compute $\braket{\sigma_x}$ for $\ket{sing}$, you will find it to be zero. In fact, for singlet state, $\braket{\sigma_x} = \braket{\sigma_y} = \braket{\sigma_z} = 0$. Since the components are zero on expectation, we are equaly likely to get +1 and -1 for any measurement. Thus, although the state is known, we don&amp;rsquo;t know the outcome.&lt;/p>
&lt;hr>
&lt;p>The operators that act on the two separate factors commute with one another.&lt;/p>
&lt;p>The operator $\tau_z \sigma_z$, is a product of two oprators. Little math tells us that $\ket{sing}$ is an eigenvector for $\tau_z \sigma_z$. $\tau_z \sigma_z \ket{sing} = -\ket{sing}$. This means that the product of observations is always -1. Thus both measurements always have opposite signs. In fact, $\ket{sing}$ is also an eigenvector for operators $\tau_x \sigma_x$ and $\tau_y \sigma_y$, with eigenvalue -1.&lt;/p>
&lt;p>On the other hand if you try to compute $\braket{\tau_x \sigma_y}$, you will find that to be 0.&lt;/p>
&lt;p>For the triplets:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>$\ket{sing}$&lt;/th>
&lt;th>$\ket{T_1}$&lt;/th>
&lt;th>$\ket{T_2}$&lt;/th>
&lt;th>$\ket{T_3}$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\braket{\sigma_z \tau_z}$&lt;/td>
&lt;td>-1&lt;/td>
&lt;td>-1&lt;/td>
&lt;td>+1&lt;/td>
&lt;td>+1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\braket{\sigma_x \tau_x}$&lt;/td>
&lt;td>-1&lt;/td>
&lt;td>+1&lt;/td>
&lt;td>+1&lt;/td>
&lt;td>-1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\braket{\sigma_y \tau_y}$&lt;/td>
&lt;td>-1&lt;/td>
&lt;td>+1&lt;/td>
&lt;td>-1&lt;/td>
&lt;td>+1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Thus, singlet and triplets are eigenvalues for the product operators.&lt;/p>
&lt;hr>
&lt;p>$\vec{\sigma} \cdot \vec{\tau}$ is an observable that can not be measured by measuring two spins by individual apparatuses. How can we measure such thing? Some atoms have spins. When two of these atoms are close to each other (e.g., in a lattice), in some situations their Hamiltonian is proportional to $\vec{\sigma} \cdot \vec{\tau}$.&lt;/p>
&lt;p>The four vectors above are eigenvectors for $\vec{\sigma} \cdot \vec{\tau}$. The singlet has eigenvalue -3 and each triplet has eigenvalue +1.&lt;/p>
&lt;p>Comment: Generating singlet state is easy. Two spins prefer to be anti-aligned. Now bring these two spins together, they will radiate a photon and result in this state, as this state has low energy.&lt;/p>
&lt;p>For product state it is easy to prove $\braket{AB} = \braket{A} \braket{B}$. Thus, we can see that $\braket{\sigma_w \tau_w}$ can not be product states. Because for them $\braket{\sigma_w} = 0$.&lt;/p>
&lt;h3 id="outer-products">Outer Products&lt;/h3>
&lt;p>$\ket{\Psi} \bra{\Phi}$ is an operator. Which can be applied to bras or kets.&lt;/p>
&lt;p>$\ket{\Psi} \bra{\Phi} \ket{A} = \ket{\Psi} \braket{\Phi|A}$&lt;/p>
&lt;p>Specificaly if $\ket\Psi$ is normalized, &lt;strong>Projection Operator&lt;/strong> is defined as $\ket{\Psi}\bra{\Psi}$.&lt;/p>
&lt;p>Properties of Projection Operators.&lt;/p>
&lt;ul>
&lt;li>They are Hermitian.&lt;/li>
&lt;li>$\Psi$ is an eigenvector with eigenvalue +1.&lt;/li>
&lt;li>All perpendicular vectors to $\Psi$ are eigenvectors with eigenvalue 0.&lt;/li>
&lt;li>Square of projection operator is the projection operator itself. $(\ket\Psi \bra\Psi)^2 = \ket\Psi \bra\Psi$.&lt;/li>
&lt;li>Trace($Tr(L) = \sum_i \braket{i|L|i})$ of projection operator is 1.&lt;/li>
&lt;li>If we add all proj. operators for all basis vectors we get identity operator. $\sum_i \ket{i}\bra{i} = I$.&lt;/li>
&lt;li>Expectation of any operator L, for state $\Psi$ is $Tr (\ket\Psi \bra\Psi L)$.&lt;/li>
&lt;/ul>
&lt;h3 id="density-matrix">Density Matrix&lt;/h3>
&lt;p>Suppose we don&amp;rsquo;t know the exact state of the system, but we know that it is either $\Psi$ of $\Phi$ with equal probability. Then, the expected value is $ \frac{\braket{\Psi|L|\Psi} + \braket{\Phi|L|\Phi}}{2}$. If we define a new operator $\rho = \frac{1}{2}\ket\Psi \bra\Psi + \frac{1}{2} \ket\Phi \bra\Phi$, the expected value can be computed as $Tr (\rho L)$.&lt;/p>
&lt;p>The definition for $\rho$ is general, and holds for more than two states. On the other hand, if we knew exactly that the state is $\Psi$, the last property of the projection operator still applies. In any case, $\braket{L} = Tr (\rho L)$.&lt;/p>
&lt;p>In a pure state, density matrix is just a projection operator. In a mixed state, it is a combination of multiple projection operators.&lt;/p>
&lt;p>In the matrix form, for particular basis, $\rho_{aa&amp;rsquo;} = \braket{a|\rho|a&amp;rsquo;}$. In this basis, $\braket{L} = \sum_{a,a&amp;rsquo;} \rho_{aa&amp;rsquo;} L_{a&amp;rsquo;a}$.&lt;/p>
&lt;p>In classical system, two particles in a line for example, if we know state of the complete system (i.e., $x_1, x_2, p_1, p_2$), we know the state of the constituents (i.e., $x_1, p_1$ and $x_2, p_2$).
(Comment: This is classical pure state. But, sometimes we don&amp;rsquo;t know the exact $x_1, x_2, p_1, p_2$, but have some distribution of $\rho(x_1, x_2, p_1, p_2)$. This is mixed state.)&lt;/p>
&lt;p>This is not true in QM when there is entanglement. In such cases, even if the combined system can be in pure state $\psi(a,b)$, each of it&amp;rsquo;s constituent states must be described by a mixed state (unlike classical setting).&lt;/p>
&lt;p>Say we have the full knowledge of the combined system(A, B). That is we know $\psi(a, b)$ of the combined system. And we want to know what we can about A. Let&amp;rsquo;s pick an operator L which only acts on A.&lt;/p>
&lt;p>Here, $\braket{L} = \sum_{ab, a&amp;rsquo;b&amp;rsquo;} \psi^{\ast}{(a&amp;rsquo;b&amp;rsquo;)} L_{a&amp;rsquo;b&amp;rsquo;, ab} \psi{(ab)}$.&lt;/p>
&lt;p>Using the rules of matrix construction(Kronecker Product), $L_{a&amp;rsquo;b&amp;rsquo;, ab} = L_{a&amp;rsquo;a} \delta_{b&amp;rsquo;b}$.&lt;/p>
&lt;p>Thus, $$
\begin{align*}
\braket{L} &amp;amp;= \sum_{a, b, a&amp;rsquo;} \psi^{\ast}{(a&amp;rsquo;b)} L_{a&amp;rsquo;a} \psi{(ab)} \\
&amp;amp;= \sum_{a, a&amp;rsquo;} L_{a&amp;rsquo;a} \sum_{b}\psi^{\ast}{(a&amp;rsquo;b&amp;rsquo;)} \psi{(ab)} \\
&amp;amp;= \sum_{a, a&amp;rsquo;} \rho_{aa&amp;rsquo;} L_{a&amp;rsquo;a} \psi{(ab)} \\
\end{align*}
$$&lt;/p>
&lt;p>Where, we defined $\rho_{aa&amp;rsquo;} = \sum_{b}\psi^{\ast}{(a&amp;rsquo;b)} \psi{(ab)}$.&lt;/p>
&lt;p>Note the difference in the definition of the $\rho$s. If in the above equation, we had product state (i.e., non entangled state $\psi(a, b) = \psi(a) \phi(b)$), we will get $\rho_{aa&amp;rsquo;} = \psi^{\ast}{(a&amp;rsquo;)} \psi{(a)} \sum_{b}\phi^{\ast}{(b)} \phi{(b)} = \psi^{\ast}{(a&amp;rsquo;)} \psi{(a)}$. Thus, we get our projection operator (i.e., pure state density matrix) back.&lt;/p>
&lt;p>But in general, even if we had pure state for combined system ($\psi(a, b)$), we may not get pure state for constituent system.&lt;/p>
&lt;p>For a single spin system, with pure state, the density matrix looks like below.&lt;/p>
&lt;p>$$\left(\begin{matrix}
\alpha^\ast \alpha &amp;amp;&amp;amp; \alpha \beta^\ast \\
\alpha^\ast \beta &amp;amp;&amp;amp; \beta^\ast \beta
\end{matrix}\right)
$$&lt;/p>
&lt;h4 id="properties-of-density-matrix">Properties of Density Matrix&lt;/h4>
&lt;ol>
&lt;li>Diagonal entries tell us the probability of the observation. That is, $P(a) = \rho_{aa}$. This is in line with traditional probability theory. The probability of observing eigenvalues for a, b is $P(a, b) = \psi^\ast(ab) \psi(ab)$. Marginalizing out b gives us $P(a) = \sum_b \psi^\ast(ab) \psi(ab)$. This is also easy to see in the density matrix for pure state shown above.&lt;/li>
&lt;li>Density matrix is Hermitian.&lt;/li>
&lt;li>Trace of density matrix is 1.&lt;/li>
&lt;li>All Eigenvalues are between 0 and 1 inclusive. Since the trace is 1, if one eigenvalue is 1, the rest are 0.&lt;/li>
&lt;li>For a &lt;strong>pure&lt;/strong> state, $\rho^2 = \rho$ and $Tr(\rho^2) = 1$.&lt;/li>
&lt;li>For a mixed or entangled state, $\rho^2 \ne \rho$ and $Tr(\rho^2) &amp;lt; 1$.&lt;/li>
&lt;/ol>
&lt;p>Proving 5 is easy. Simple algebra. But, I found proving 6 to be little bit difficult. So here is the proof.&lt;/p>
&lt;details>
&lt;summary> Proof for point 6&lt;/summary>
&lt;p>We will first prove that $Tr(\rho^2) &amp;lt; 1$.&lt;/p>
&lt;p>Assume that our mixed state is $\sum_i p_i \ket{i} \bra{i}$.&lt;/p>
&lt;p>$$
\begin{align*}
\rho^2 &amp;amp;= \sum_{i, j} p_i p_j \ket{i} \bra{i} \ket{j} \bra{j} \\
&amp;amp;= \sum_{i,j} p_i p_j \braket{i|j} \ket{i} \bra{j} \\
Tr(\rho^2) &amp;amp;= \sum_{i, j} p_i p_j \braket{i|j} Tr(\ket{i} \bra{j}) \text{ // linearity of trace} \\
&amp;amp;= \sum_{i, j} p_i p_j \braket{i|j} \sum_k (i_k j^\ast_k) \\
&amp;amp;= \sum_{i, j} p_i p_j \braket{i|j} \braket{j|i} \\
&amp;amp;= \sum_{i, j} p_i p_j |\braket{i|j}|^2 \\
&amp;amp;&amp;lt; \sum_{i, j} p_i p_j \text{ //Cauchy Schwarz Inequalty} \\
&amp;amp;= \sum_i p_i \sum_j p_j \\
&amp;amp;= 1 \blacksquare
\end{align*}
$$&lt;/p>
&lt;p>Now, $\rho^2 = \rho$ would imply that $Tr(\rho^2) = Tr(\rho) = 1$. Which contradicts what we just proved. So $\rho^2 \ne \rho$.&lt;/p>
&lt;p>Some facts used in this proof are,&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)">Trace is a linear operator.&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Cauchy-Swartz Inequality&lt;/a>
$ |\braket{i|j}|^2 \le \braket{i|i} \braket{j|j}$ Moreover, equality holds only when both vectors are linearly dependent.&lt;/li>
&lt;/ul>
&lt;/details>
&lt;details>
&lt;summary> Examples of Density matrix (Exercise 7.8)&lt;/summary>
&lt;ol>
&lt;li>$\ket{\Psi_1} = \frac{1}{2}(\ket{uu} + \ket{ud} + \ket{du} + \ket{dd})$
Density matrix for system A&lt;/li>
&lt;/ol>
&lt;p>$$
\left(\begin{matrix}
\frac{1}{2} &amp;amp;&amp;amp; \frac{1}{2} \\
\frac{1}{2} &amp;amp;&amp;amp; \frac{1}{2} \\
\end{matrix}\right)
$$&lt;/p>
&lt;p>Density matrix for system B&lt;/p>
&lt;p>$$
\left(\begin{matrix}
\frac{1}{2} &amp;amp;&amp;amp; \frac{1}{2} \\
\frac{1}{2} &amp;amp;&amp;amp; \frac{1}{2} \\
\end{matrix}\right)
$$&lt;/p>
&lt;p>For both the systems $\rho^2 = \rho$. So is pure state.&lt;/p>
&lt;ol start="2">
&lt;li>$\ket{\Psi_2} = \frac{1}{\sqrt2}(\ket{uu} + \ket{dd}) $&lt;/li>
&lt;/ol>
&lt;p>Density matrix for system A&lt;/p>
&lt;p>$$
\left(\begin{matrix}
\frac{1}{2} &amp;amp;&amp;amp; 0 \\
0 &amp;amp;&amp;amp; \frac{1}{2} \\
\end{matrix}\right)
$$&lt;/p>
&lt;p>Density matrix for system B&lt;/p>
&lt;p>$$
\left(\begin{matrix}
\frac{1}{2} &amp;amp;&amp;amp; 0 \\
0 &amp;amp;&amp;amp; \frac{1}{2} \\
\end{matrix}\right)
$$&lt;/p>
&lt;p>For both the systems $\rho^2 \ne \rho$. So is mixed state.&lt;/p>
&lt;ol start="3">
&lt;li>$\ket{\Psi_3} = \frac{1}{5}(3\ket{uu} + 4\ket{ud})$&lt;/li>
&lt;/ol>
&lt;p>$$
\left(\begin{matrix}
1 &amp;amp;&amp;amp; 0 \\
0 &amp;amp;&amp;amp; 0 \\
\end{matrix}\right)
$$&lt;/p>
&lt;p>Density matrix for system B&lt;/p>
&lt;p>$$
\left(\begin{matrix}
\frac{9}{25} &amp;amp;&amp;amp; \frac{12}{25} \\
\frac{12}{25} &amp;amp;&amp;amp; \frac{16}{25} \\
\end{matrix}\right)
$$&lt;/p>
&lt;p>For both the systems $\rho^2 = \rho$. So is pure state.&lt;/p>
&lt;/details>
&lt;h3 id="test-for-entanglement">Test for entanglement&lt;/h3>
&lt;p>We have a wave function $\psi(a, b)$, and we want to test if that state is entangled or not.&lt;/p>
&lt;h4 id="correlation-test">Correlation Test&lt;/h4>
&lt;p>Say there are two observables A and B, for two systems. The correlation between them is defined as $C(A, B) = \braket{AB} - \braket{A} \braket{B}$.&lt;/p>
&lt;p>If a system is any state for which $C(A, B) \ne 0$, the state is entangled.&lt;/p>
&lt;h4 id="using-density-matrix">Using Density Matrix&lt;/h4>
&lt;p>&lt;strong>Theorem&lt;/strong>: For any product state, the density matrix of any constituent subsystem has exactly one non-zero eigenvalue (thus it has to be 1). Moreover, the eigenvector for this eigenvalue is the factor wave function for that subsystem.&lt;/p>
&lt;p>E.g., if A&amp;rsquo;s wave function is $\psi$, and B&amp;rsquo;s wave function is $\phi$, such that $\psi(a, b) = \psi(a) \phi(b)$, $\psi$ is the eigenvector with eigenvalue 1 for A&amp;rsquo;s density matrix.&lt;/p>
&lt;p>Conversly, if there are at least two eigenvalues($\lambda_j &amp;gt; 0, \lambda_k &amp;gt; 0$ (and since the Trace = 1, they are less than 1), one can prove that $\rho^2 \ne \rho$. And since $\rho^2 = \rho$ is necessary and sufficient condition for product state, we know that the state must be entangled.&lt;/p>
&lt;details>
&lt;summary> Proof that at least two positive eigenvalues implies $\\rho^2 \\ne \\rho$.&lt;/summary>
Proof by contradiction.
&lt;p>Assume $\rho^2 = \rho$.&lt;/p>
&lt;p>$$
\rho^2 - \rho = 0 = \sum_i (\lambda_i^2 - \lambda_i) \ket{\Psi_i} \bra{\Psi_i}
$$&lt;/p>
&lt;p>Since, in particular $\lambda_k^2 - \lambda_k &amp;lt; 0$ and $\Psi_i$ are eigenbasis,&lt;/p>
&lt;p>$$
\begin{align*}
\braket{\Psi_k|0|\Psi_k} = 0 &amp;amp;= \sum_i (\lambda_i^2 - \lambda_i) \braket{\Psi_k|\Psi_i} \braket{\Psi_i|\Psi_k} \\
&amp;amp;= \lambda_k^2 - \lambda_k \\
&amp;amp;&amp;lt; 0
\end{align*}
$$&lt;/p>
&lt;p>Which is a contradiction.&lt;/p>
&lt;/details>
&lt;p>In a maximally entangled state, all the eigenvalues of the density matrix are equal. $\rho_{aa} = \frac{1}{N_A}$, where $N_A$ is the number of states in subsystem A. Although in such state we don&amp;rsquo;t know anything about one particular subsystem (uniform probability distribution), there is correlation. If we measure one subsytem, we know the outcome of the experiment on other subsystem.&lt;/p>
&lt;h2 id="continuous-domain">Continuous Domain&lt;/h2>
&lt;p>So far we have talked about discrete case, where observable was discrete. There are observables like position which are continuous.&lt;/p>
&lt;p>Our wave function is discrete complex-valued function. $\psi(\lambda)$, where for spin $\lambda$ was either up or down. The state $\Psi$&amp;rsquo;s wave function depended upon the basis. In the up-down basis $\Psi = \psi(\ket{u}) \ket{u} + \psi(\ket{d}) \ket{d}$.&lt;/p>
&lt;p>For cases like particle moving on a line (x-axis), we need wave function that is continuous. We can write it as $\psi(x)$, where x is a real number. Notice that functions like this also form a vector space. Two functions can be added. It can be multiplied with a complex number to get another function, and so on.&lt;/p>
&lt;dl>
&lt;dt>Inner Product&lt;/dt>
&lt;dd>$\braket{\Phi|\Psi} = \int_{-\infty}^{\infty} \phi^\ast(x)\psi(x) dx$&lt;/dd>
&lt;dt>Probability Density&lt;/dt>
&lt;dd>$P(x) = \psi^\ast(x) \psi(x)$ is NOT the probability of observing x. It is instead a density around x.&lt;/dd>
&lt;dt>Dirac Delta Function&lt;/dt>
&lt;dd>$\delta$ is defined such that $\int_{-\infty}^{\infty} \delta(x - x&amp;rsquo;) F(x&amp;rsquo;) dx&amp;rsquo; = F(x)$.&lt;/dd>
&lt;/dl>
&lt;h3 id="integration-by-parts">Integration By Parts&lt;/h3>
&lt;p>$ FG |_a^b - \int_a^b G dF = \int_a^b F dG$.&lt;/p>
&lt;p>Generally $F \to 0 \text{ as } |x| \to \infty$ for wave functions to be properly normalized. And same for G. So $FG |_a^b = 0$ in such cases. So the formula is quite easy to remember in QM.&lt;/p>
&lt;p>$$- \int_{-\infty}^\infty G \frac{dF}{dx} dx = \int_{-\infty}^\infty F \frac{dG}{dx} dx$$&lt;/p>
&lt;p>Move the differentiation to another function by changing the sign.&lt;/p>
&lt;h3 id="linear-operators">Linear Operators&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>X&lt;/strong>
: Multiply by x operator, $X \psi(x) = x \psi(x)$.&lt;/li>
&lt;li>&lt;strong>D&lt;/strong>
: Diffrentiation operator, $D \psi(x) = \frac{d \psi(x)}{dx}$.&lt;/li>
&lt;li>&lt;strong>P&lt;/strong>
: $P = -i \hbar D$.&lt;/li>
&lt;/ul>
&lt;h4 id="hermitian-operators-1">Hermitian Operators.&lt;/h4>
&lt;p>L is Hertmitian if $\braket{\Psi|L|\Phi} = \braket{\Phi|L|\Psi}^\ast$.&lt;/p>
&lt;p>Since x (domain of $\psi, \phi$) is real, it is easy to show that &amp;ldquo;Multiply by X&amp;rdquo; $X$ operator is Hermitian.&lt;/p>
&lt;p>On the other hand, you will find that D is not Hermitian. $\braket{\Psi|D|\Phi} = - \braket{\Phi|D|\Psi}^\ast$. Operators like this (where $D^\dagger = -D$ are called anti-hermitian. For any anti-hermitian operator A, both $iA$ and $-iA$ are hermitian.&lt;/p>
&lt;p>Thus, we define an operator $P = -i \hbar D$ such that $-i \hbar D \ \psi(x) = -i \hbar \frac{d\psi(x)}{dx}$. This operator is Hermitian.&lt;/p>
&lt;h3 id="particle-state">Particle State&lt;/h3>
&lt;h4 id="formal-prose">Formal Prose&lt;/h4>
&lt;p>In classical mechanics, for a particle moving on x-axis, if the Hamiltonian equations are known, given (x, p) (i.e., position, and momenta p = mv) we know the flow in the phasespace.&lt;/p>
&lt;p>However, decades of experience tells us that in Quantum Mechanics, one does not have states where both the components are specified. We know from experience that &lt;code>x AND p&lt;/code> is not knowable, but &lt;code>x OR p&lt;/code> can be known.&lt;/p>
&lt;h4 id="details">Details&lt;/h4>
&lt;p>Since position, and momentum are observable, they would have Hermitian Operators associated with them. For position, this operator is &lt;strong>X&lt;/strong>.&lt;/p>
&lt;h5 id="eigenvalues-and-eigenvectors-for-x">Eigenvalues, and Eigenvectors for &lt;strong>X&lt;/strong>.&lt;/h5>
&lt;p>Trying to solve the eigenvalue equation,&lt;/p>
&lt;p>$$
\begin{align*}
x \psi(x) &amp;amp;= x_0 \psi(x) \\
(x - x_0)\psi(x) &amp;amp;=0
\end{align*}
$$&lt;/p>
&lt;p>This holds true for all possible values of x. When $x \ne x_0$, this means that $\psi(x) = 0$. At $x = x_0$ it can take non zero value. We know such function, it is Dirac&amp;rsquo;s Delta function, $\delta(x - x_0)$.&lt;/p>
&lt;p>Thus &lt;em>every&lt;/em> $x&amp;rsquo; \in \mathbb{R}$ is an eigenvalue for &lt;strong>X&lt;/strong>, with eigenfunction $\delta(x - x&amp;rsquo;)$.&lt;/p>
&lt;p>$\braket{x_0|\Psi} = \int_{-\infty}^{\infty} \delta(x-x_0) \psi(x) dx = \psi(x_0)$.&lt;/p>
&lt;p>Thus $\braket{x|\Psi} = \psi(x)$.&lt;/p>
&lt;p>Wave function in the position representation: is denoted as $\psi(x)$ and is the projection of the state on the eigenvectors for position, i.e., $\braket{x|\Psi} = \psi(x)$.&lt;/p>
&lt;h5 id="eigenvalues-and-eigenvectors-for-p-the-momentum-operator">Eigenvalues, and Eigenvectors for &lt;strong>P&lt;/strong>, the Momentum Operator&lt;/h5>
&lt;p>NOTE: Connection with classical mass times velocity will become clear later.&lt;/p>
&lt;p>If we solve for eigenvalue equation, $P \psi(x) = -i \hbar D \psi(x) = p \psi(x)$, we get the solution $\psi_p(x) = A e^{\frac{ipx}{\hbar}}$. Subscript p denotes the eigenfunction associated with eigenvalue p. The A is a normalizing constant, required to make the integration 1. It turns out to be, $A = \frac{1}{\sqrt{2 \pi}}$. Note that, this eigenfunction is written in basis of position.&lt;/p>
&lt;p>Thus,&lt;/p>
&lt;p>$\braket{x|p} = \frac{1}{\sqrt{2 \pi}} e^{\frac{ipx}{\hbar}}$&lt;/p>
&lt;p>And, $\braket{p|x} = \frac{1}{\sqrt{2 \pi}} e^{\frac{-ipx}{\hbar}}$.&lt;/p>
&lt;h5 id="waves">Waves?&lt;/h5>
&lt;p>The momentum function $\frac{1}{\sqrt{2 \pi}} e^{\frac{ipx}{\hbar}}$ has sin and cosine. The A&amp;ndash;the constant&amp;ndash; is not important in the frequency of the wave, it just changes the mangnitude. $e^{\frac{ipx}{\hbar}}$ has a wavelength of $\frac{2 \pi \hbar}{p}$. (Because: $e^{\frac{ip(x+\lambda)}{\hbar}} = e^{\frac{ipx}{\hbar}} e^{i 2 \pi} = e^{\frac{ipx}{\hbar}}$).&lt;/p>
&lt;p>In 20th century, scientists wanted to detect smaller and smaller particles. One can&amp;rsquo;t resolve objects much smaller than the wavelength one using to look at them. So in 20th century, scientists wanted to find light of smaller and smaller wavelengths. We will later see that light of a given wavelength is composed of photons whose momentum is related to the wavelength by the relation $\lambda = \frac{2 \pi \hbar}{p}$. But, as per this relation, to get smaller wavelength, one must increase the momentum. This requires high energy. Hence, particle acclerators were/are required.&lt;/p>
&lt;h3 id="momentum-basis">Momentum Basis&lt;/h3>
&lt;p>We saw the eigenfunction associated with momentum operator was a function of x! That is because we wrote it in position basis. If for the state $\Psi$, we want to measure it&amp;rsquo;s momentum, we can do so by using the probability density function $\psi^\ast(p) \psi(p)$. $\psi(p)$ (not to be confused with $\psi(x)$) is a function of p. In the momentum basis it is just $\braket{p|\Psi}$.&lt;/p>
&lt;p>Both wave function $\psi(x)$ and $\psi(p)$ represent the same state $\Psi$, just in different basis. It is possible to convert between two using fourier transform.&lt;/p>
&lt;p>$$
\begin{align*}
\psi(p) &amp;amp;= \frac{1}{\sqrt{2 \pi}} \int e^{\frac{-ipx}{\hbar}} \psi(x) dx \\
\psi(x) &amp;amp;= \frac{1}{\sqrt{2 \pi}} \int e^{\frac{ipx}{\hbar}} \psi(p) dp \\
\end{align*}
$$&lt;/p>
&lt;details>
&lt;summary> Derivation of the transform&lt;/summary>
&lt;p>$$
\begin{align*}
\psi(p) &amp;amp;= \braket{p|\Psi} \\
&amp;amp;= \braket{p\left|\int dx \ket{x}\bra{x} \right| \Psi} \\
&amp;amp;= \int dx \braket{p|x}\braket{x|\Psi} \\
&amp;amp;= \frac{1}{\sqrt{2 \pi}} \int e^{\frac{-ipx}{\hbar}} \psi(x) dx \\
\psi(x) &amp;amp;= \braket{x|\Psi} \\
&amp;amp;= \braket{x\left|\int dp \ket{p}\bra{p} \right| \Psi} \\
&amp;amp;= \int dp \braket{x|p}\braket{p|\Psi} \\
&amp;amp;= \frac{1}{\sqrt{2 \pi}} \int e^{\frac{ipx}{\hbar}} \psi(p) dp
\end{align*}
$$&lt;/p>
&lt;/details>
&lt;h3 id="relation-with-poisson-brackets">Relation with Poisson Brackets&lt;/h3>
&lt;p>earlier we show that $[L, M] = i\hbar\{L, M\}$.&lt;/p>
&lt;p>We can compute the commutator $[X, P] = XP - PX$.&lt;/p>
&lt;p>$$
\begin{align*}
XP \psi(x) &amp;amp;= -i \hbar x \frac{d \psi(x)}{dx} \\
PX \psi(x) &amp;amp;= -i \hbar \frac{d x \psi(x)}{dx} \\
&amp;amp;= -i \hbar x \frac{d \psi(x)}{dx} - i\hbar \psi(x) \\
[X, P]\psi(x) &amp;amp;= i \hbar \psi(x) \\
[X, P] &amp;amp;= i \hbar
\end{align*}
$$&lt;/p>
&lt;p>Thus, the commutator is a number, which is non-zero. That is, X and P don&amp;rsquo;t commute. You can&amp;rsquo;t measure one without disturbing the other.&lt;/p>
&lt;p>This also implies that Poisson bracket {X, P} = 1. This was proved in classical mechanics course. This is the link between classical momentum and quantum momentum P.&lt;/p>
&lt;h3 id="heisenbergs-uncertainty-principle">Heisenberg&amp;rsquo;s Uncertainty Principle&lt;/h3>
&lt;p>Earlier we had derived $\Delta{A} \Delta{B} \ge \frac{1}{2} |\braket{\Psi|[A, B]|\Psi}|$.&lt;/p>
&lt;p>Thus, $\Delta{X} \Delta{P} \ge \frac{1}{2} |\braket{\Psi|[X, P]|\Psi}| = \frac{1}{2} \hbar$.&lt;/p>
&lt;p>Let&amp;rsquo;s look at example. I think this section requires more mathematical rigor, but I am not at the point where I can justify much of what I am writing.&lt;/p>
&lt;p>Let&amp;rsquo;s assume that our state is eigenstate of the momentum. That is, $\psi(p_0) = \delta (p-p_0)$. Then, as per Fourier formulas, $$
\begin{align*}
\psi(x) &amp;amp;= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{\frac{ipx}{\hbar}} \psi(p) dp \\
&amp;amp;= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{\frac{ipx}{\hbar}} \delta(p-p_0) dp \\
&amp;amp;= \frac{1}{\sqrt{2 \pi}} e^{\frac{i p_0 x}{\hbar}} \\
\psi^\ast(x) &amp;amp;= \frac{1}{\sqrt{2 \pi}} e^{\frac{-i p_0 x}{\hbar}} \\
\psi^\ast(x)\psi(x) &amp;amp;= \frac{1}{2 \pi}
\end{align*}
$$&lt;/p>
&lt;p>Thus, the density function for position is as uncertain as it can get, it is uniform.&lt;/p>
&lt;p>In this case, there is no uncertainty in $\Delta P$. That is $\Delta P = 0$. However, $\Delta X = \infty$. Obviously, the product is not defined. But, maybe in the limits the product is greater than $\hbar /2$?&lt;/p>
&lt;p>See discussion at &lt;a href="https://physics.stackexchange.com/questions/553145/does-the-heisenbergs-uncertainty-equation-holds-when-one-of-the-observable-have">https://physics.stackexchange.com/questions/553145/does-the-heisenbergs-uncertainty-equation-holds-when-one-of-the-observable-have&lt;/a>.&lt;/p>
&lt;h2 id="time-evolution-of-particles">Time Evolution of Particles&lt;/h2>
&lt;h3 id="simple-example">Simple Example&lt;/h3>
&lt;p>Recall that time evolution of particle is given by generalized Schrodinger&amp;rsquo;s equation $i\hbar \frac{\partial \psi(x, t)}{\partial t} = H \psi(x, t)$. Where H is Hamiltonian, and is energy of the quantum system.&lt;/p>
&lt;p>A simplest Hamiltonian is $H = cP$, where c is a fixed number. If we apply above equation here, we get,
$i\hbar \frac{\partial \psi(x, t)}{\partial t} = -c i \hbar \frac{\partial \psi(x, t)}{\partial x}$. Which leads to $\frac{\partial \psi(x, t)}{\partial t} = -c \frac{\partial \psi(x, t)}{\partial x}$. Any function $\psi(x - ct)$ solves this equation.&lt;/p>
&lt;p>At T = 0, we have some function $\psi(x)$. Note that due to normalization contraints, this function has to go to zero towards the ends. The wave function might look something like this.&lt;/p>
&lt;p>&lt;img src="images/attachment:7891febe-a9f7-46e4-b840-9a4ce96ad8c7.png" alt="image.png">&lt;/p>
&lt;p>At later time, T = t, $\psi(x-ct)$ has the same shape as $\psi(x)$ but shifted to the right by ct. It is equivalent to say that the wave has translated to the right in space. This translation happens at uniform velocity. If we let c to be the speed of the light, this Hamiltonian almost describes the 1d neutrinos, except that our H only describes particle moving to the right.&lt;/p>
&lt;p>Since the wave function moves rigidly to the right, the expected value also moves to the right at the same velocity. In classical mechanics if let $H = cP$, the Hamiltonian equations give us following equations.&lt;/p>
&lt;p>$$
\begin{align*}
\frac{\partial H}{\partial p} &amp;amp;= \dot{x} = c \\
\frac{\partial H}{\partial x} &amp;amp;= -\dot{p} = 0 \\
\end{align*}
$$&lt;/p>
&lt;p>Thus classically, the momentum is conserved, and the particle moves with the constant velocity c.&lt;/p>
&lt;h3 id="non-relativistic-particles">Non relativistic Particles&lt;/h3>
&lt;p>Particles that have some mass, can not travel at the speed of light. Classically, a free particle (i.e., zero potential energy) has the Hamiltonian $\frac{p^2}{2m}$.&lt;/p>
&lt;p>Inspired from it, we can imagine that QM has hamiltonian $\frac{P^2}{2m}$. When we put definition of P into time-dependent Schrodinger&amp;rsquo;s equation, we get $\frac{\partial \psi}{\partial t} = \frac{i \hbar}{2m} \frac{\partial^2 \psi}{\partial x^2}$.&lt;/p>
&lt;p>Susskind&amp;rsquo;s book says that waves with different wavelength travel with different velocities, so this wave changes the shape with time. I don&amp;rsquo;t understand this chain of thoughts.&lt;/p>
&lt;p>There are two explanations I found on the Internet. &lt;a href="https://physics.stackexchange.com/questions/77860/why-do-wave-packets-spread-out-over-time">Here&lt;/a>.&lt;/p>
&lt;ol>
&lt;li>One explanation is connection with heat equation. The Schrodinger&amp;rsquo;s equation above, is partial differential equation in the form of heat equation. Heat equation says that waves where the curvature is high, dissipates faster.&lt;/li>
&lt;li>Another explanation is that initially we have some uncertainity in the position, and some uncertainity in the momentum (i.e., velocity). Smaller the uncertainity in position, larger the uncertainity in velocity. So overtime, this uncertainity in position increases(due to initial uncertainty in velocity), and the wavefunction spreads.&lt;/li>
&lt;/ol>
&lt;h4 id="time-evolution-of-the-non-relativistic-particle">Time evolution of the non-relativistic particle.&lt;/h4>
&lt;p>We follow the recipe outlined earlier.&lt;/p>
&lt;ol>
&lt;li>Find the H somehow. Done: $H = \frac{P^2}{2m}$.&lt;/li>
&lt;li>Find the eigenvalues for this H. This is easy. Eigenvectors $\frac{P^2}{2m}$ are same as eigenvectors of P. Just the eigenvalues change to $\frac{p^2}{2m}$. $E(p) = e^{\frac{ipx}{\hbar}}$.&lt;/li>
&lt;li>Prepare the state in the initial state $\Psi$.&lt;/li>
&lt;li>Use the continuous counterpart of $\Psi(t) = \sum_j \ket{E_j}\braket{E_j|\Psi(0)} e^{-\frac{i}{\hbar}E_j t}$ to find the state at later time. So,&lt;/li>
&lt;/ol>
&lt;p>$$
\begin{align*}
\psi(x, t) &amp;amp;= \int \text{exp}\left(\frac{i p x}{\hbar}\right) \psi(p) \text{exp}\left({-\frac{i p^2 t}{\hbar 2 m}}\right) dp \\
&amp;amp;= \int \psi(p) \text{exp}\left({\frac{i\left(px - \frac{p^2 t}{2 m}\right)}{\hbar}}\right) dp
\end{align*}
$$&lt;/p>
&lt;p>Of course, I have abused the notation. A lot.&lt;/p>
&lt;p>If we compare the last equation to fourier formula, we get $\psi(p, t) = \psi(p) \text{exp}\left({\frac{-i\frac{p^2}{2 m}}{\hbar}}\right)$.&lt;/p>
&lt;p>Note: This formula might be wrong. Book has it $\psi(p, t) = \psi(p) \text{exp}\left({\frac{i\left(px - \frac{p^2}{2 m}\right)}{\hbar}}\right) dp$. But, it didn&amp;rsquo;t mention how it got it. Things are becoming crazy. I might have missed something. Or it is just a typo in the book.&lt;/p>
&lt;p>Anyway, both the formulas suggest that P(p, t) is equivalent to P(p, 0). This is because momentum is conserved when there is no external force.&lt;/p>
&lt;h4 id="expected-change-in-position">Expected Change in Position&lt;/h4>
&lt;p>Since the wave function changes with time, the expected value of x, i.e., $\braket{\Psi|X|\Psi}$ also changes with time. There are two ways to compute this quantity. One is bruteforce, and one is to use commutators.&lt;/p>
&lt;p>Using commutators is easy. We use the relationship proved &lt;a href="#change-in-the-expectation">here&lt;/a>. $\frac{d \braket{X}}{dt} = \frac{-i}{\hbar 2 m} \braket{[P^2, X]} = \frac{-i}{\hbar 2 m} \braket{P[P, X] + [P, X]P} = \frac{-i}{\hbar 2 m} \braket{-2 i \hbar P} = \frac{\braket{P}}{m}$.&lt;/p>
&lt;p>This is similar to classical v = p/m.&lt;/p>
&lt;p>I will probably not do the bruteforce. But the in the 10th Lecture Prof. Susskind does that.&lt;/p>
&lt;h3 id="non-relativistic-particle-with-some-forces-on-it">Non relativistic particle, with some forces on it&lt;/h3>
&lt;p>In CM, we replace all the forces acting on the particle by a potential function. That is $F(x) = -\frac{\partial V}{\partial x}$. The Hamiltonian in CM is K + V. So, our quantum Hamiltonian should have V operator. The V operator is defined on wave functions, and is, $V\psi(x) = V(x)\psi(x)$. Just multiply by V(x).&lt;/p>
&lt;p>Classically, the momentum does not depend upon forces on the particle. It is just v/m. Thus our update in the Hamiltonian should not change $\frac{d \braket{X}}{dt}$ that was derived earlier. Indeed, the new commutator will be just $[P^2 + V, X] = [P^2, X] + [V, X]$. Remember, commutators are linear, and so are expectations. Also, V and X commute with each other. That is easy to see when you use the definitions of V and X. So $[V, X] = 0$. Hence, $\frac{d \braket{X}}{dt} = \frac{\braket{P}}{m}$ still holds.&lt;/p>
&lt;p>Classically, $\frac{dp}{dt} = F = -\frac{\partial V}{\partial x}$. Let&amp;rsquo;s see what happens in the quantum realm.&lt;/p>
&lt;p>$\frac{d \braket{P}}{dt} = \frac{i}{\hbar} \braket{[P^2+V, P]} = \frac{i}{\hbar} \braket{[V, P]}$.&lt;/p>
&lt;p>It is easy exercise to show that $[V, P] = i \hbar \frac{\partial V}{\partial x}$.&lt;/p>
&lt;p>And hence, $\frac{d \braket{P}}{dt} = - \braket{ \frac{\partial V}{\partial x}}$.&lt;/p>
&lt;p>It is important to note that $\frac{d \braket{P}}{dt} \ne - \frac{\partial \braket V}{\partial x}$.&lt;/p>
&lt;p>For example, take a bimodal distribution, where one peak is at -5 and another one at 5. Let $F(x) = x^2$. Since, the expected value of x is 0, $F(\braket{x}) = F(0) = 0$. On the other hand, $\braket{F(x)}$ is not zero.&lt;/p>
&lt;p>When the wave function is concentrated over a fairly narrow range, only then $\braket{F(x)} = F(\braket{x})$.&lt;/p>
&lt;p>Following are statements from the book. I am unable to prove it or find the proofs in the book. So pasting them verbatimly.&lt;/p>
&lt;pre>&lt;code>If V varies rapidly across the wave packet, the classical approximation will break down. In fact, in that situation a nice, narrow wave packet will get broken up into a badly scattered wave that has no resemblance to the original wave packet.
&lt;/code>&lt;/pre>
&lt;h1 id="path-integrals">Path Integrals&lt;/h1>
&lt;p>In classical mechanics, given that particle is at x1 at time t1, and is at x2 given t2, the path the particle takes is the one that minimizes the action.&lt;/p>
&lt;p>Similar notion is for quantum mechanics, called path integrals. Here, we say that given that particle is at x1 at time t1, what is the probability (or amplitude) at x2, t2.&lt;/p>
&lt;p>The initial state is $\ket{\Psi(t_1)} = \ket{x_1}$. The time evolution gives us $\ket{\Psi(t_2)} = e^{-i H (t_2-t_1)} \ket{x_1}$. So the amplitude at position x2 is given by $\braket{x_2|\Psi(t_2)} = \braket{x_2|e^{-i H (t_2-t_1)} | x_1}$. We replace t2-t1 by t.&lt;/p>
&lt;p>$\braket{x_2|\Psi(t_2)} = \braket{x_2|e^{-i H t} | x_1}$. Using the decomposition $e^{-i H t} = e^{-i H \frac{t}{2}}e^{-i H \frac{t}{2}}$, and $I = \int dx \ket{x}\bra{x}$, we get, $\braket{x_2|\Psi(t_2)} = \int {dx \braket{x_2|e^{-i H \frac{t}{2}} | x} \braket{x|e^{-i H \frac{t}{2}} | x_2}}$.&lt;/p>
&lt;p>But, there is nothing special about t/2. We can divide the path into N segments, each with length $\epsilon$. If we denote $U = e^{-i\epsilon H}$, $\braket{x_2|\Psi(t_2)} = \braket{x_2|U^N|x_1}$.&lt;/p>
&lt;p>What Feynman discovered was that for each path, there is an action A. And that $\braket{x_2|\Psi(t_2)} = \int_{paths} e^{iA/\hbar}$.&lt;/p>
&lt;h1 id="harmonic-oscillators">Harmonic Oscillators&lt;/h1>
&lt;p>Harmonic oscillators is a mathematical framework. Many natural phenomenon have oscillation property. Ideallized spring for example follows Hook&amp;rsquo;s law. That is, force is proportional to the displacement. $F = -kx$. Thus, $V = kx^2/2$.&lt;/p>
&lt;p>Many energy functions approximate quadratic function, around minima.&lt;/p>
&lt;p>Examples,&lt;/p>
&lt;ul>
&lt;li>Waves: Sound waves and water waves. At particular position, water molecules oscillates.&lt;/li>
&lt;li>Electromagnetic waves: Same mathematics as normal waves.&lt;/li>
&lt;li>Electric current in a low resistance often oscillates with a charateristic frequency.&lt;/li>
&lt;/ul>
&lt;p>All these, have the same mathematics.&lt;/p>
&lt;h2 id="classical-spring">Classical Spring&lt;/h2>
&lt;p>$ L = K - V = \frac{m \dot{y}^2}{2} - \frac{k y^2}{2}$. If we let $x = \sqrt{m}y$, the lagrangian becomes, $L = \frac{ \dot{x}^2}{2} - \frac{\omega^2 x^2}{2}$, where $ \omega = \sqrt{\frac{k}{m}}$. Omega happens to be the frequency of the oscillation.&lt;/p>
&lt;p>We changed the variable to convert into the common form. In the common form, every osillator just changes the omega.&lt;/p>
&lt;p>For one dimensional motion, Lagrange&amp;rsquo;s equation is
$\frac{\partial L}{\partial x} = \frac{d}{dt} \frac{\partial L}{\partial \dot x}$. Solving this gives us $-\omega^2 x = \ddot{x}$. The minus sign is there to tell us that acceleration is opposite to the displacement. This equation is exactly same as $F = ma$. The general solution of this is $x = Acos(\omega t) + Bsin(\omega t)$. This confirms that, omega is the frequency.&lt;/p>
&lt;h2 id="quantum-spring">&amp;ldquo;Quantum Spring&amp;rdquo;&lt;/h2>
&lt;pre>&lt;code>Many molecules consist of two atoms - for example, a heavy atom and a light one. There are forces holding the molecule in equilibrium with the atoms separated by a certain distance. When the light atom is displaced, it will be attracted back to the equilibrium location.
&lt;/code>&lt;/pre>
&lt;p>States in 1-dimensional motion, for small particles, is given by state vectors $\psi(x)$. This $\psi$ has to follow certain conditions, like normality, and waning to zero at infinities.&lt;/p>
&lt;p>The canonical momentum conjugate to x is defined as $p = \frac{\partial L}{\partial \dot{x}} = \dot{x}$. Thus, the classical Hamiltonian is given by $H = p\dot{x} - \mathcal{L}$. One can use the definition of Lagrangian to figure out the Hamiltonian. It turns out to be $H = \frac{\dot{x}^2}{2} + \frac{\omega^2 x^2}{2}$. But, the velocity does not have quantum operator, so we replace velocity by canonical conjugate momentum. $H = \frac{p^2}{2} + \frac{\omega^2 x^2}{2}$.&lt;/p>
&lt;p>The Quantum counterpart is, $H = \frac{P^2}{2} + \frac{\omega^2 X^2}{2}$.&lt;/p>
&lt;p>The algebra will tell us that, $H \ket{\psi(x)} = -\frac{\hbar^2}{2} \frac{\partial^2 \psi(x)}{\partial x^2} + \frac{1}{2} \omega^2 x^2 \psi(x)$.&lt;/p>
&lt;h3 id="time-dependent-shrodingers-equation">Time dependent Shrodinger&amp;rsquo;s Equation&lt;/h3>
&lt;p>$$
\begin{align*}
i \frac{\partial \psi}{\partial t} &amp;amp;= \frac{1}{\hbar} H \psi \\
&amp;amp;= -\frac{\hbar}{2} \frac{\partial^2 \psi}{\partial x^2} + \frac{1}{2\hbar} \omega^2 x^2 \psi \\
\end{align*}
$$&lt;/p>
&lt;p>One can use computational methods to solve these equations. But, another way to solve is to use time-independent schrodinger&amp;rsquo;s equations.&lt;/p>
&lt;h3 id="time-independent-schrodingers-equations">Time independent Schrodinger&amp;rsquo;s equations&lt;/h3>
&lt;p>We can find eigenvalues and eigenvectors of the H, and follow the recipe outlined earlier.&lt;/p>
&lt;p>$$
\begin{align*}
H \ket{\Psi_E} &amp;amp;= E \ket{\Psi_E} \\
-\frac{\hbar^2}{2} \frac{\partial^2 \psi_E(x)}{\partial x^2} + \frac{1}{2} \omega^2 x^2 \psi_E(x) &amp;amp;= \psi_E(x)\\
\end{align*}
$$&lt;/p>
&lt;p>We want to find E, and associated $\Psi_E$ that solve this equation. But, as it turns out, most of the solutions for E, including all the complenumbers, have associated $\Psi_E$ which blows up as x approaches one or both infinities. So, we want the E, and associated $\Psi_E$ where $\Psi_E$ is normalizable.&lt;/p>
&lt;h4 id="smallest-eigenvalue">Smallest eigenvalue&lt;/h4>
&lt;p>Classical oscillator does not have negative energy. The reason for it is that it has square terms $x^2$ and $p^2$ in it. But, zero is the minimum energy, arrived when $x = p = 0$.&lt;/p>
&lt;p>Quantum oscillator, has $X^2$ and $P^2$, both of which has non-negative eigenvalues. So the minimum energy has to be non-negative. (Because we have $P^2/2 + \omega^2/2 X^2$. A non-negative sum of positive semi-definite matrices is also positive semi-definite. I assume that argument would hold true for complex operators as well.) But, in QM, as per heisenberg&amp;rsquo;s uncertainy principle, we can&amp;rsquo;t set $X = P = 0$. The state associated with the lowest positive eigenvalue is known as ground state, and is denoted as $\Psi_0$.&lt;/p>
&lt;p>There is a theorem which helps us find the eigenfunction associated with the smallest eigenvalue.&lt;/p>
&lt;pre>&lt;code>The ground-state wave function for any potential has no zeros and it’s the only energy eigenstate that has no nodes.
&lt;/code>&lt;/pre>
&lt;p>One such function is $\psi(x) = e^{\frac{-\omega}{2\hbar}x^2}$. This function, is concentrated around x = 0, as would be required for minimum valued eigenfunction.&lt;/p>
&lt;p>When we put this function in the time independent schrodinger&amp;rsquo;s equation, we will get $\frac{\hbar}{2} \omega e^{\frac{-\omega}{2\hbar}x^2} = E e^{\frac{-\omega}{2\hbar}x^2}$. The only solution is to set $E = \frac{\omega \hbar}{2}$.&lt;/p>
&lt;p>Thus we have groundstate $\psi_0(x) = e^{\frac{-\omega}{2\hbar}x^2}$ and groundstate energy $E_0 = \frac{\omega \hbar}{2}$.&lt;/p>
&lt;h4 id="other-way-to-look">Other way to look&lt;/h4>
&lt;p>So far we have looked at functions. Another way to look is abstract method. Our operator is $H = \frac{1}{2} (P^2 + \omega^2 X^2)$.&lt;/p>
&lt;p>It is easy to write $H = \frac{1}{2}(P + i\omega X)(P - i\omega X) + \frac{\omega \hbar}{2}$. (Hint: $[X, P] = i\hbar$).&lt;/p>
&lt;p>Due to the history, we define $a^{-} = \frac{i}{\sqrt{2\omega \hbar}} (P - i \omega X)$ and $a^{+} = \frac{-i}{\sqrt{2\omega \hbar}} (P + i \omega X)$.&lt;/p>
&lt;p>Properties of $a^+, a^-$.&lt;/p>
&lt;ul>
&lt;li>$(a^-)^\dagger = a^+$&lt;/li>
&lt;li>$[a^-, a^+] = 1$&lt;/li>
&lt;/ul>
&lt;p>After these definitions, $H = \omega \hbar (a^+ a^- + 1/2)$. If we define $N = a^+ a^-$, $H = \omega \hbar (N + 1/2)$.&lt;/p>
&lt;ul>
&lt;li>$[a^-, N] = a^-$&lt;/li>
&lt;li>$[a^+, N] = - a^+$.&lt;/li>
&lt;/ul>
&lt;h5 id="why-are-these-important">Why are these important?&lt;/h5>
&lt;p>Say you had and eigenpair for N, $N \ket{n} = n \ket{n}$, what happens if I apply N to $a^+ \ket{n}$?
$$
\begin{align*}
N (a^+\ket{n}) &amp;amp;= (a^+ N - (a^+ N - N^a+)) \ket{n} \\
&amp;amp;= n a^+ \ket{n} - [a^+, N]\ket{n} \\
&amp;amp;= n a^+ \ket{n} + a^+ \ket{n} \\
&amp;amp;= (n+1) a^+ \ket{n}
\end{align*}
$$&lt;/p>
&lt;p>Thus, $a^+ \ket{n}$ is the eigenvector with next eigenvalue. i.e., $(a^+ \ket{n}) = \ket{n+1}$.&lt;/p>
&lt;p>Similarly, $N a^- \ket{n} = (n-1) a^- \ket{n}$, and $a^-\ket{n} = \ket{n-1}$.&lt;/p>
&lt;p>Because of these, $a^+$ is known as raising operator, and $a^-$ is known as lowering operator.&lt;/p>
&lt;p>By the way, since $H = \omega \hbar (N + 1/2)$, lowering can&amp;rsquo;t continue indefinitely, otherwise H might end up with negative eigenvalues. Thus, for the smallest eigenvector, call it $\ket{0}$, $a^- \ket{0} = 0$. This vector, solves the time-independent schrodinger&amp;rsquo;s equation for $E_0 = \omega \hbar /2$.&lt;/p>
&lt;p>$E_n = \omega \hbar (1/2, 3/2, 5/2, \dots)$.&lt;/p>
&lt;h5 id="but-what-is-the-ground-state">But, what is the ground state?&lt;/h5>
&lt;p>We can find the ground state by using the lowering operator on the $a^-$. $ \frac{i}{\sqrt{2 \pi \hbar}} (P - i \omega \hbar X) \psi_0{x} = 0$.&lt;/p>
&lt;p>$$
\begin{align*}
\frac{i}{\sqrt{2 \pi \hbar}} (P - i \omega \hbar X) \psi_0{(x)} &amp;amp;= 0 \\
(P - i \omega \hbar X) \psi_0{(x)} &amp;amp;= 0 \\
-i \hbar \frac{\partial \psi_0{(x)}}{\partial x} - i \omega x \psi_0{(x)} &amp;amp;= 0 \\
\frac{\partial \psi_0{(x)}}{\partial x} = -\frac{\omega x}{\hbar} \psi_0{(x)}
\end{align*}
$$&lt;/p>
&lt;p>This solution has equation $\psi_0{(x)} = e^{-\frac{\omega}{2 \hbar} x^2}$.&lt;/p>
&lt;p>The next eigenvectors can be found by using $a^+$ operator on the ground state. Not doing the algebra here, but the solutions for the next two states are below.&lt;/p>
&lt;p>$$
\begin{align*}
\psi_1(x) &amp;amp;= \frac{2 \omega x}{\sqrt{2 \pi \hbar}} e^{-\frac{\omega}{2 \hbar} x^2} \\
\psi_2(x) &amp;amp;= \frac{\omega}{\pi \hbar} (-\hbar + 2 \omega x^2) e^{-\frac{\omega}{2 \hbar} x^2} \\
\end{align*}
$$&lt;/p>
&lt;p>It is easy to notice that for each step higher in the eigenstate, we have higher degree polynomial multiplied by the ground state. As the degree of the polynomial increases, the zeros of the statefunction, and nodes of the amplitudes increase. Thus, higher energy wave functions have higher frequency, implying higher momentum. Also, since the exponential goes to zero faster than the polynomial, the wave function are &amp;ldquo;good&amp;rdquo;, i.e., they go to zero towards the infinities.&lt;/p>
&lt;p>&lt;img src="images/attachment:3fce27c0-2578-4ab1-8d39-7c3096802b26.png" alt="image.png">&lt;/p>
&lt;p>Although, they approch zero towards infinities, they are not quite zero. That means there is positive probability of finding particles outside of the &amp;ldquo;potential bowl&amp;rdquo;. This phenomenon is known as quantum tunneling.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>CM-Notes</title><link>https://dhruvpatel.dev/notes/physics/theoretical_minimum/cm-notes/</link><pubDate>Wed, 05 Oct 2022 02:01:42 +0000</pubDate><guid>https://dhruvpatel.dev/notes/physics/theoretical_minimum/cm-notes/</guid><description>Notation $ F_i(\{q\}) $ is the force on ith particle. $ \{q\} $ is the set of cordinates of all particles.
If there are N particles in the system,
Configration space: 3N dimensional space, records positions.
State space: 6N dimensional space, records position and velocity.
Phase space: 6N dimesnional space, records position and momentum.
Newton&amp;rsquo;s second law gives us 6N equations.
$$ \begin{align*} \dot{p}_i &amp;amp;= F_i(\{q\}) \\ \dot{q}_i &amp;amp;= \frac{p_i}{m_i} \end{align*} $$</description><content>&lt;h2 id="notation">Notation&lt;/h2>
&lt;p>$ F_i(\{q\}) $ is the force on ith particle. $ \{q\} $ is the set of cordinates of all particles.&lt;/p>
&lt;p>If there are N particles in the system,&lt;/p>
&lt;p>Configration space: 3N dimensional space, records positions.&lt;/p>
&lt;p>State space: 6N dimensional space, records position and velocity.&lt;/p>
&lt;p>Phase space: 6N dimesnional space, records position and momentum.&lt;/p>
&lt;h2 id="newtons-second-law">Newton&amp;rsquo;s second law&lt;/h2>
&lt;p>gives us 6N equations.&lt;/p>
&lt;p>$$
\begin{align*}
\dot{p}_i &amp;amp;= F_i(\{q\}) \\
\dot{q}_i &amp;amp;= \frac{p_i}{m_i}
\end{align*}
$$&lt;/p>
&lt;p>Thus, if forces are known, we can compute trajectory in 6N dimensional configration space.&lt;/p>
&lt;p>if we denote $ p = \sum_{i=1}^{i=N} p_i $,&lt;/p>
&lt;h2 id="newtons-third-law">Newton&amp;rsquo;s third law,&lt;/h2>
&lt;pre>&lt;code>every force has equal and opposite reaction
&lt;/code>&lt;/pre>
&lt;p>can be used to derive conservation of momentum.&lt;/p>
&lt;p>$\dot{p} = 0$.&lt;/p>
&lt;p>What this tells us is if we start our trajectory at some 6N-D point in configration space, all the points in the trajectory have the same momentum. In some sense, system evolves on a contour of constant momentum.&lt;/p>
&lt;h2 id="energy">Energy&lt;/h2>
&lt;p>Potential Energy Principle: All forces derive from a potential energy function $V(\{q\})$.&lt;/p>
&lt;p>For particle moving in 1 dimension,&lt;/p>
&lt;p>$ F(q) = -\frac{dV(q)}{dq}$.&lt;/p>
&lt;p>Potential energy can be computed as $V(q) = - \int_{-\infty}^q F(q&amp;rsquo;) dq&amp;rsquo;$.&lt;/p>
&lt;p>Potential energy is not conserved. Sum of potential energy and kinetic energy are conserved.&lt;/p>
&lt;p>Kinetic energy: $T = \frac{1}{2} mv^2$.&lt;/p>
&lt;p>More than one particles, three dimensions. If the system has N particles, the i in $q_i$ can index any of the 3N elements of the configration space.&lt;/p>
&lt;p>Principle:
For &lt;strong>any&lt;/strong> system there exists a potential function $V(\{q\})$, such that,&lt;/p>
&lt;p>$$
F_i(\{q\}) = - \frac{\partial V(\{q\})}{\partial q_i}
$$&lt;/p>
&lt;p>In general mathematics such function need not exist for a collection of $F_i$s. This law represents conservation of energy.&lt;/p>
&lt;h2 id="lagrangian">Lagrangian&lt;/h2>
&lt;h3 id="for-one-particle-in-one-dimension">For one particle in one dimension&lt;/h3>
&lt;p>We are given $q(t_0)$ and $q(t_1)$.&lt;/p>
&lt;p>$L(q, \dot{q}) = T - V = \frac{1}{2}m\dot{q}^2 - V(\{q\})$&lt;/p>
&lt;p>Action $A = \int_{t_0}^{t_1} L dt$.&lt;/p>
&lt;p>Principle of least action tells that the particle chooses a trajectory (the function q) which minimizes the the action.&lt;/p>
&lt;p>Principle of least action gives Euler-Lagrange equations.&lt;/p>
&lt;p>$$
\frac{d}{dt} \frac{\partial L}{\partial \dot{q}} - \frac{\partial L}{\partial {q}} = 0
$$&lt;/p>
&lt;h3 id="for-multidimensional-motion-of-many-particles">For multidimensional motion of many particles&lt;/h3>
&lt;p>Euler-Lagrange equations are given by,&lt;/p>
&lt;p>$L(\{q\}, \{\dot{q}\}) = \sum_i \frac{1}{2}m_i\dot{q}_i^2 - V(\{q\})$.&lt;/p>
&lt;p>$$
\frac{d}{dt} \frac{\partial L}{\partial \dot{q}_i} - \frac{\partial L}{\partial {q_i}} = 0
$$&lt;/p>
&lt;p>Lagrangian packs all the equations of the motions concisely.&lt;/p>
&lt;p>$\frac{\partial L}{\partial \dot{q}_i}$ is called generalized momentum conjugate to $q_i$. This can be motiviated by thinking of $q_i$ as cartesian coordinates and $L = \frac{1}{2}m\dot{x}^2$. Depending upon the Lagrangian, conjugate momentum may not have familiar form, but it is always difined by the formula $p_i =\frac{\partial L}{\partial \dot{q}_i}$.&lt;/p>
&lt;p>So, if the Lagrangian does not depend on $q_i$, $\dot{p}_i = 0$, i.e. the conjugate momentum is conserved. Such coordinates are called cyclic coordinates.&lt;/p>
&lt;h4 id="another-example-of-cylic-coordinates">Another example of cylic coordinates&lt;/h4>
&lt;p>$ L = \frac{m}{2}(\dot{x}_1^2 + \dot{x}_2^2 ) + V(x_1 - x_2)$. It does look like that L is a function of $x_1$ and $x_2$, so neither of these is cyclic coordinate. But, if we do change of variables,
$$
\begin{align*}
x_+ &amp;amp;= \frac{x_1+x_2}{2} \\
x_- &amp;amp;= \frac{x_1-x_2}{2}
\end{align*}
$$&lt;/p>
&lt;p>the Lagrangian can be rewritten as, $L = m(\dot{x}_+^2 + \dot{x}_-^2 ) + V(2x_-)$. Now the momentum conjugate to $x_+$ is conserved. $p_+ = 2m\dot{x}_+ = m(\dot{x}_1 + \dot{x}_2)$, so the total momentum is conserved.&lt;/p>
&lt;p>If $ L = \frac{m}{2}(\dot{q_1}^2 + \dot{q_2}^2 ) + V(a q_1 - b q_2)$, then&lt;/p>
&lt;p>$$
\begin{align*}
\dot{p}_1 &amp;amp;= -a V(a q_1 - b q_2) \\
\dot{p}_2 &amp;amp;= b V(a q_1 - b q_2) \\
\end{align*} \text{.}
$$&lt;/p>
&lt;p>Law of conservation of momentum has changed. Instead of conserving $p_1 + p_2$, $bp_1+ap_2$ is conserved. If V was a non linear function of $q_1, q_2$, there wouldn&amp;rsquo;t be law of conservation.&lt;/p>
&lt;h2 id="symmetries">Symmetries&lt;/h2>
&lt;p>$q_i&amp;rsquo; = q_i&amp;rsquo;(q_i)$. We are moving the whole system to the new location. This change changes the system. For example, potential energy(so the Lagrangian) may change.&lt;/p>
&lt;p>Symmetry is the coordinate transformation that does not change the value of the Lagrangian.&lt;/p>
&lt;h3 id="examples">Examples&lt;/h3>
&lt;p>$L = \frac{1}{2} \dot{q}^2$. And the transformation $ q \rightarrow q + \delta$. Here the $\dot{q}$ does not change, so L also doesn&amp;rsquo;t change.&lt;/p>
&lt;p>If the Lagrangian had a potential ($V(q)$) term in it, unless the potential is a constant independent of q, change in q changes the potential. In that case there is no symmetry.&lt;/p>
&lt;h4 id="example-sym1">Example Sym.1&lt;/h4>
&lt;p>If, potential was a function $V(q_1 - q_2)$, then under the transformation
$$
\begin{align*}
q_1 &amp;amp;\rightarrow q_1 + \delta \\
q_2 &amp;amp;\rightarrow q_2 + \delta
\end{align*}
$$
L is symmetric.&lt;/p>
&lt;h4 id="example-sym2">Example Sym.2&lt;/h4>
&lt;p>If, potential was a function $V(aq_1 + bq_2)$, then under the transformation
$$
\begin{align*}
q_1 &amp;amp;\rightarrow q_1 + b\delta \\
q_2 &amp;amp;\rightarrow q_2 - a\delta
\end{align*}
$$
L is symmetric.&lt;/p>
&lt;h4 id="example-sym3">Example Sym.3&lt;/h4>
&lt;p>If, $ L = \frac{1}{2} (\dot{x}^2 + \dot{y}^2) + V(x^2 + y^2)$, then there is rotational symmetry. That is, rotating the point around origin by angle $\theta$ does not change the lagrangian.&lt;/p>
&lt;p>$$
\begin{align*}
x &amp;amp;\rightarrow x cos\theta + y sin\theta \\
y &amp;amp;\rightarrow -x sin\theta + y cos\theta
\end{align*}
$$&lt;/p>
&lt;p>For small angle $\delta$, $sin\delta = \delta$ and $cos\delta = 1$.
$$
\begin{align*}
x &amp;amp;\rightarrow x + y \delta \\
y \rightarrow -x \delta + y &amp;amp;= y - x\delta
\end{align*}
$$
Plugging this into the lagrangian, we can see that (in the first order of $\delta$) lagrangian does not change.&lt;/p>
&lt;h3 id="general-notion-of-symmetry">General notion of symmetry&lt;/h3>
&lt;p>In general, the shift is parameterized by infinitesimal $\delta$ and is given by $\delta q_i = f_i(q) \delta$.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>example&lt;/th>
&lt;th>$f_1$&lt;/th>
&lt;th>$f_2$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Sym.1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sym.2&lt;/td>
&lt;td>b&lt;/td>
&lt;td>-a&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sym.3&lt;/td>
&lt;td>y&lt;/td>
&lt;td>-x&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Change in velocity then is given by $\delta \dot{q_i} = \frac{d}{dt} \delta q_i$.&lt;/p>
&lt;p>A continuous symmetry is an infinitesimal transformation of the coordinates for which the change in the Lagrangian is zero.&lt;/p>
&lt;p>If we assume that system evolves along a trajectory that satisfies Euler-Lagrangian equations, we can prove that symmetry implies $\cal{Q}$ is conserved. Where $\cal{Q} = \sum_i p_i f_i(q)$.&lt;/p>
&lt;p>Applying this to &lt;strong>Sym.3&lt;/strong>, we see that $l = p_x y - p_y x$, aka angular momentum, is conserved.&lt;/p>
&lt;h2 id="time-translation-invariance">Time translation invariance&lt;/h2>
&lt;p>A system is time-translation invariant if there is no explicit time dependence in its Lagrangian.&lt;/p>
&lt;p>e.g. harmonic motion due to spring $L(x, \dot{x}) = \frac{1}{2} (m \dot{x}^2 - k x^2)$. Here neither the mass m, nor the spring constant k depend on the time.&lt;/p>
&lt;p>If spring constant changes with time i.e. k(t), there would be no time translation invariance.&lt;/p>
&lt;p>Now if $L = L(q_i, \dot{q}_i, t)$,&lt;/p>
&lt;p>$$
\frac{dL}{dt} = \sum_i \left( \frac{\partial{L}}{\partial{q_i}} \dot{q}_i + \frac{\partial{L}}{\partial{\dot{q}_i}} \ddot{q}_i \right) + \frac{\partial{L}}{\partial{t}}
$$&lt;/p>
&lt;p>Using Euler-Lagrangian equations, we can simplify above to&lt;/p>
&lt;p>$$
\frac{dL}{dt} = \frac{d}{dt} \sum_i p_i \dot{q}_i + \frac{\partial{L}}{\partial{t}} \text{.}
$$&lt;/p>
&lt;p>If we define $H = \sum_i p_i \dot{q}_i - L$, we see that
$$\frac{dH}{dt} = -\frac{\partial{L}}{\partial{t}} \text{.}$$&lt;/p>
&lt;p>Conclusion: H changes only if L has &lt;strong>explicit&lt;/strong> time dependence. In other words, if the system is time-translaction invariant then quantity H is conserved.&lt;/p>
&lt;p>H is called Hamiltonian, and is an energy of the system.&lt;/p>
&lt;h3 id="example-motion-of-a-particle-in-a-potential">Example: Motion of a particle in a potential&lt;/h3>
&lt;p>$$
\begin{align*}
L &amp;amp;= \frac{1}{2}m \dot{x}^2 - V(x) \\
p &amp;amp;= \frac{\partial{L}}{\partial{\dot{x}}} = m \dot{x} \\
H &amp;amp;= p \dot{x} - L \\
&amp;amp;= m \dot{x}^2 - \frac{1}{2}m \dot{x}^2 + V(x) \\
&amp;amp;= \frac{1}{2}m \dot{x}^2 + V(x) \\
\end{align*}
$$&lt;/p>
&lt;p>There are systems for which the Lagrangian has a more intricate form than just T - V. For some of those cases, it is not possible to identify a clear separation into kinetic and potential energy.&lt;/p>
&lt;p>General Definition of Energy: Energy equals Hamiltonian.&lt;/p>
&lt;p>In Lagrangian formulation, the focus is on the trajectory in the configuration space. here, the equations are second order. So knowing just the $q_i$s is not enough. We also need initial velocities.&lt;/p>
&lt;p>In Hamiltonian formulation, the focus in on the trajectory in the phase space.&lt;/p>
&lt;p>The first step in the Hamiltonian formulation is to replace $\dot{q}$&amp;rsquo;s with $p$&amp;rsquo;s. This is easy to do in normal cartesian coordinates.&lt;/p>
&lt;p>in the particle on a line, $ H = \frac{1}{2} m \dot{x}^2 + V(x)$. Replacing $\dot{x} = \frac{p}{m}$, we get, $ H = \frac{p^2}{2m} + V(x)$.&lt;/p>
&lt;p>$$
\frac{\partial H}{\partial x} = -\frac{dV}{dx}
$$&lt;/p>
&lt;p>Using $f = ma$ we can rewrite the above equations as
$$
\dot{p} = -\frac{\partial H}{\partial x}
$$&lt;/p>
&lt;p>Thus we have two equations,&lt;/p>
&lt;p>$$
\begin{align*}
-\frac{\partial H}{\partial x} &amp;amp;= \dot{p} \\
\frac{\partial H}{\partial p} = \frac{p}{m} &amp;amp;= \dot{x}
\end{align*}
$$&lt;/p>
&lt;h3 id="general-system">General system&lt;/h3>
&lt;p>$$
\begin{align*}
H &amp;amp;= H(q_i, p_i) \\
\dot{p_i} &amp;amp;= -\frac{\partial H}{\partial q_i} \\
\dot{q_i} &amp;amp;= \frac{\partial H}{\partial p}
\end{align*}
$$&lt;/p>
&lt;p>So we see that for each direction in phase space, there is a single first-order equation. If we know initial $p, q$, using above equations we can predict the future.&lt;/p>
&lt;h4 id="harmonic-oscillator">Harmonic Oscillator&lt;/h4>
&lt;p>$$
L = \frac{m \dot{x}^2}{2} - \frac{kx^2}{2}
$$&lt;/p>
&lt;p>With the change of variable $q = (km)^{\frac{1}{4}}$,&lt;/p>
&lt;p>$$
L = \frac{\dot{q}^2}{2\omega} - \frac{\omega q^2}{2}
$$&lt;/p>
&lt;p>The conjugate momentum $p = \frac{\partial L}{\partial \dot{q}} = \frac{\dot{q}}{\omega}$. This gives us $H = \frac{\omega}{2} (p^2 + q^2)$. (Exercise. Recall: $H = \sum_i p_i \dot{q}_i - L$.)&lt;/p>
&lt;p>From that,&lt;/p>
&lt;p>$$
\begin{align}
\dot{p} &amp;amp;= -\omega q \\
\dot{q} &amp;amp;= \omega p
\end{align}
$$&lt;/p>
&lt;p>Thus Hamiltonian formulation gives us two first order equations.&lt;/p>
&lt;p>Solving Euler-Lagrangian equation, on the other hand gives us single second order equation. $\ddot{q} = \omega \dot{p}$. These two are equivalent, and can be seen by substituting the first equation in the time derivative of the second equation of the Hamiltonian.&lt;/p>
&lt;p>Notice that because of constant energy, in the phase space the particle moves along a circle of fixed radius.&lt;/p>
&lt;p>&lt;img src="images/attachment:767ea6b5-3f7f-4b7f-9d23-a82fe7abc155.png" alt="image.png">&lt;/p>
&lt;h2 id="phase-space-fluid">Phase Space Fluid&lt;/h2>
&lt;p>One can imagine a trajectory starting with arbitrary point &lt;code>(p, q)&lt;/code> in the phase following the hamiltonian equations.&lt;/p>
&lt;p>$$
\begin{align*}
\dot{p_i} &amp;amp;= -\frac{\partial H}{\partial q_i} \\
\dot{q_i} &amp;amp;= \frac{\partial H}{\partial p}
\end{align*}
$$&lt;/p>
&lt;p>We can imagine the phase space made of infinite points. This can be seen as a fluid in a phase space. The fluid moves using hamiltonian equations.&lt;/p>
&lt;p>This flow has certain features.&lt;/p>
&lt;ol>
&lt;li>If a point starts at given energy H(q, p), it remains with the same value of energy.
The surfaces of the energy are defined by $H(q, p) = E$. For each value of E, we have a surface.&lt;/li>
&lt;/ol>
&lt;p>In ordinary 3-d space, a flow can be described as a velocity field $\vec{v}(x, y, z)$. For each point, it defines a velocity at that point. This can also be a function of time.&lt;/p>
&lt;p>&lt;img src="images/attachment:beee651f-3d52-42fb-95fb-028ec81d6cf7.png" alt="image.png">&lt;/p>
&lt;dl>
&lt;dt>Incompressible fluid&lt;/dt>
&lt;dd>a given amount of the fluid always occupies the same volume. It also means that the density of the fluid—the number of molecules per unit volume—is uniform and stays that way forever.&lt;/dd>
&lt;/dl>
&lt;p>Divergence of the vector field $\vec{v(t)}$, is defined to be,&lt;/p>
&lt;p>$$
\nabla \cdot \vec{v} = \left( \frac{\partial{v_x}}{\partial{x}} + \frac{\partial{v_y}}{\partial{y}} + \frac{\partial{v_z}}{\partial{z}} \right)
$$&lt;/p>
&lt;p>So in the small cube, if velocity along all three axis remains constant, divergence is zero. Incompressible fluid will have zero divergence.&lt;/p>
&lt;p>But, is the flow through phase space incompressible? &lt;strong>Liouville&amp;rsquo;s Theorem&lt;/strong> says that yes, if the system satisfies Hamilton&amp;rsquo;s equations.&lt;/p>
&lt;p>Example: $H = pq$&lt;/p>
&lt;p>$$
\begin{align*}
\dot{q} &amp;amp;= q \\
\dot{p} &amp;amp;= -p
\end{align*}
$$&lt;/p>
&lt;p>The flow decreases exponentially in p axis, and increases exponentially in q axis. The blob changes the shape extremely, but volume remains constant.&lt;/p>
&lt;p>Lioville&amp;rsquo;s theorem in quantum mechanics is replaced by unitarity.&lt;/p>
&lt;h2 id="poisson-brackets">Poisson Brackets&lt;/h2>
&lt;p>Let $F(q, p)$ be the generic function of q&amp;rsquo;s and p&amp;rsquo;s. E.g., it could be potential energy, kinetic energy, angular momentum, etc.&lt;/p>
&lt;p>As we follow a point in a phase space, we get a trajectory of F. i.e., F is a function of time.&lt;/p>
&lt;p>$$
\begin{align}
\dot{F} &amp;amp;= \sum_{i} \left( \frac{\partial F}{\partial q_i} \dot{q_i} + \frac{\partial F}{\partial p_i} \dot{p_i} \right) \\
\dot{F} &amp;amp;= \sum_{i} \left( \frac{\partial F}{\partial q_i} \frac{\partial H}{\partial p_i} -
\frac{\partial F}{\partial p_i} \frac{\partial H}{\partial q_i} \right)
\end{align}
$$&lt;/p>
&lt;p>For &lt;strong>any&lt;/strong> two functions F, and G in a phase space, Poisson Bracket is defined as,
$$
\{F, G\} = \sum_{i} \left( \frac{\partial F}{\partial q_i} \frac{\partial G}{\partial p_i} -
\frac{\partial F}{\partial p_i} \frac{\partial G}{\partial q_i} \right)
$$&lt;/p>
&lt;p>Thus, $\dot{F}$ could be rewritten as $\{F, H\}$.&lt;/p>
&lt;p>The time derivative of &lt;strong>anything&lt;/strong> is a poisson bracket of itself with the Hamiltonian.&lt;/p>
&lt;ul>
&lt;li>$\dot{q_k} = \{q_k, H\} = \frac{\partial H}{\partial p}$. (Simple application of the notation.)&lt;/li>
&lt;li>$\dot{p_k} = \{p_k, H\} = -\frac{\partial H}{\partial q}$. (Simple application of the notation.)&lt;/li>
&lt;/ul>
&lt;p>Thus, Poisson bracket gives Hamilton&amp;rsquo;s equations back.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>Bloom Filters</title><link>https://dhruvpatel.dev/notes/cs/algorithms/bloom_filters/</link><pubDate>Thu, 29 Sep 2022 04:07:39 +0000</pubDate><guid>https://dhruvpatel.dev/notes/cs/algorithms/bloom_filters/</guid><description>API bf.add(x): adds x in the data structure x in bf: tests if x is in the data structure. Why not use Set or Dict? Bloom filters are more space efficients. They take memory lesser than the keys themselves. Cons can&amp;rsquo;t store associated data. does not support deletions. It is probabilistic data structure. That means, x in bf might have false positives. There are no false negatives. Applications Spell checkers: (40 years ago) Add the dictionary into the filter.</description><content>&lt;h2 id="api">API&lt;/h2>
&lt;ul>
&lt;li>&lt;code>bf.add(x)&lt;/code>: adds x in the data structure&lt;/li>
&lt;li>&lt;code>x in bf&lt;/code>: tests if x is in the data structure.&lt;/li>
&lt;/ul>
&lt;h2 id="why-not-use-set-or-dict">Why not use Set or Dict?&lt;/h2>
&lt;ul>
&lt;li>Bloom filters are more space efficients. They take memory lesser than the keys themselves.&lt;/li>
&lt;/ul>
&lt;h2 id="cons">Cons&lt;/h2>
&lt;ul>
&lt;li>can&amp;rsquo;t store associated data.&lt;/li>
&lt;li>does not support deletions.&lt;/li>
&lt;li>It is probabilistic data structure. That means, &lt;code>x in bf&lt;/code> might have false positives. There are no false negatives.&lt;/li>
&lt;/ul>
&lt;h2 id="applications">Applications&lt;/h2>
&lt;ul>
&lt;li>Spell checkers: (40 years ago) Add the dictionary into the filter. If the word is in the bloom filter, it is higly likely tobe correctly spelled word.&lt;/li>
&lt;li>list of forbidden password. E.g., too common password.&lt;/li>
&lt;li>Modern applications: Routers, a lot of packets incoming.&lt;/li>
&lt;/ul>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;ul>
&lt;li>we have a data set &lt;code>S&lt;/code> of size &lt;code>|S|&lt;/code>.&lt;/li>
&lt;li>have an array of &lt;code>n&lt;/code> bits.&lt;/li>
&lt;li>$b = \frac{n}{|S|}$ bits per element.&lt;/li>
&lt;li>have &lt;code>k&lt;/code> hash functions.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> numpy &lt;span style="color:#66d9ef">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">BitArray&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> __init__(self, n):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>array &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>zeros((n &lt;span style="color:#f92672">&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">3&lt;/span>) &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>, dtype&lt;span style="color:#f92672">=&lt;/span>np&lt;span style="color:#f92672">.&lt;/span>uint8)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">set&lt;/span>(self, i):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> i, j &lt;span style="color:#f92672">=&lt;/span> divmod(i, &lt;span style="color:#ae81ff">8&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>array[i] &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>array[i] &lt;span style="color:#f92672">|&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">&amp;lt;&amp;lt;&lt;/span> j)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">get&lt;/span>(self, i):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> i, j &lt;span style="color:#f92672">=&lt;/span> divmod(i, &lt;span style="color:#ae81ff">8&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> bool(self&lt;span style="color:#f92672">.&lt;/span>array[i] &lt;span style="color:#f92672">&amp;amp;&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">&amp;lt;&amp;lt;&lt;/span> j))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> xxhash
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">get_hashes_for_str&lt;/span>(x, k):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Only h1 and h2 are computed afresh. Remaining hash &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># functions are just linear combinations of h1 and h2.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">assert&lt;/span> k &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> h1 &lt;span style="color:#f92672">=&lt;/span> xxhash&lt;span style="color:#f92672">.&lt;/span>xxh64_intdigest(x, seed&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">42&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> h2 &lt;span style="color:#f92672">=&lt;/span> xxhash&lt;span style="color:#f92672">.&lt;/span>xxh64_intdigest(x, seed&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">84&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hashes &lt;span style="color:#f92672">=&lt;/span> [h1, h2]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> i &lt;span style="color:#f92672">in&lt;/span> range(&lt;span style="color:#ae81ff">2&lt;/span>, k):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hashes&lt;span style="color:#f92672">.&lt;/span>append(h1 &lt;span style="color:#f92672">+&lt;/span> i&lt;span style="color:#f92672">*&lt;/span>h2)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> hashes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">BloomFilter&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> __init__(self, n, k):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>array &lt;span style="color:#f92672">=&lt;/span> BitArray(n)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>k &lt;span style="color:#f92672">=&lt;/span> k
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>n &lt;span style="color:#f92672">=&lt;/span> n
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">add&lt;/span>(self, key):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> h &lt;span style="color:#f92672">in&lt;/span> get_hashes_for_str(key, self&lt;span style="color:#f92672">.&lt;/span>k):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>array&lt;span style="color:#f92672">.&lt;/span>set(h &lt;span style="color:#f92672">%&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>n)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> __contains__(self, key):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> h &lt;span style="color:#f92672">in&lt;/span> get_hashes_for_str(key, self&lt;span style="color:#f92672">.&lt;/span>k):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#f92672">not&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>array&lt;span style="color:#f92672">.&lt;/span>get(h &lt;span style="color:#f92672">%&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>n):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>text &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;correct incorrect&amp;#34;&lt;/span>&lt;span style="color:#f92672">.&lt;/span>split()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bm &lt;span style="color:#f92672">=&lt;/span> BloomFilter(&lt;span style="color:#ae81ff">10&lt;/span>, &lt;span style="color:#ae81ff">5&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> word &lt;span style="color:#f92672">in&lt;/span> text:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> bm&lt;span style="color:#f92672">.&lt;/span>add(word)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> word &lt;span style="color:#f92672">in&lt;/span> text:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">assert&lt;/span> word &lt;span style="color:#f92672">in&lt;/span> bm
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">assert&lt;/span> &lt;span style="color:#e6db74">&amp;#34;notcorrect&amp;#34;&lt;/span> &lt;span style="color:#f92672">not&lt;/span> &lt;span style="color:#f92672">in&lt;/span> bm
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="analysis">Analysis&lt;/h2>
&lt;p>Trade off between erro rate and space required. Data structre will be useful when there is a sweetspot on the tradeoff curve.&lt;/p>
&lt;p>Assumption: The hash functions are independent and they distribute the data uniformly.&lt;/p>
&lt;p>Out of n bits, focus on a particular bit. What is the probability that it has been set?&lt;/p>
&lt;p>It is probability of 1 minus it hasn&amp;rsquo;t been set by any of the &lt;code>|S|&lt;/code> elements, for any of the &lt;code>k&lt;/code> hash functions.&lt;/p>
&lt;p>$p = 1 - (1 - \frac{1}{n})^{k|S|}$.&lt;/p>
&lt;p>As can be seen by the plot below, $1+x$ can be approximated by $e^x$ when x is close to zero. In all cases, $e^x$ is an overestimate of $1+x$.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> matplotlib.pyplot &lt;span style="color:#66d9ef">as&lt;/span> plt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>x &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>linspace(&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>plot(x, &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">+&lt;/span>x, label&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;$1 +x$&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>plot(x, np&lt;span style="color:#f92672">.&lt;/span>exp(x), label&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;$e^x$&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>legend();
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="images/63351a0ed542f328049edc91.png" alt="output image for above cell">&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>&amp;lt;Figure size 640x480 with 1 Axes&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Thus, $p \approx 1 - e^{\frac{-k|S|}{n}} = 1 - e^{\frac{-k}{b}}$. Remember, b is the bits per element.&lt;/p>
&lt;p>As $b \to \inf$, $p \to 0$.&lt;/p>
&lt;p>Thus, the probability of false positive is $\epsilon = (1-e^{\frac{-k}{b}})^k$.&lt;/p>
&lt;h2 id="how-to-set-k">How to set K?&lt;/h2>
&lt;p>Set K optimally. Fix the b, then set k to minimize the error.
Using calculus, $k \approx (ln 2) b$.&lt;/p>
&lt;p>When we plug this back into the p, we get $\epsilon \approx (\frac{1}{2})^\left((ln 2) b\right)$. Notice that error rate is exponential in b.&lt;/p>
&lt;p>Using little bit of algebra we can get $b \approx 1.44 log_2{\frac{1}{\epsilon}}$. (Hint: $1.44 \approx \frac{1}{ln2}$.)&lt;/p>
&lt;h2 id="how-does-theory-differ-from-practice">How does theory differ from practice&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> string
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> random
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> numpy &lt;span style="color:#66d9ef">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>letters &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>array(list(string&lt;span style="color:#f92672">.&lt;/span>ascii_letters &lt;span style="color:#f92672">+&lt;/span> string&lt;span style="color:#f92672">.&lt;/span>digits))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">random_str&lt;/span>():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#e6db74">&amp;#39;&amp;#39;&lt;/span>&lt;span style="color:#f92672">.&lt;/span>join(np&lt;span style="color:#f92672">.&lt;/span>random&lt;span style="color:#f92672">.&lt;/span>choice(letters, np&lt;span style="color:#f92672">.&lt;/span>random&lt;span style="color:#f92672">.&lt;/span>randint(&lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">9&lt;/span>)))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Generate 1M keys, and insert them in the bloom filter. Just for sanity check, I will test if there is no false negative. Then I will generate keys at random, check if it is in actually there, and how often does bloom filter give false positive. Recall that error rate as a function of b = n/10M is $\approx (\frac{1}{2})^\left((ln 2) b\right)$.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>actually_there &lt;span style="color:#f92672">=&lt;/span> set()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>set_size &lt;span style="color:#f92672">=&lt;/span> int(&lt;span style="color:#ae81ff">1e6&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">while&lt;/span> len(actually_there) &lt;span style="color:#f92672">!=&lt;/span> set_size:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> actually_there&lt;span style="color:#f92672">.&lt;/span>add(random_str())
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> tqdm.auto &lt;span style="color:#f92672">import&lt;/span> tqdm
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>bs &lt;span style="color:#f92672">=&lt;/span> [&lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">8&lt;/span>, &lt;span style="color:#ae81ff">12&lt;/span>, &lt;span style="color:#ae81ff">16&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>fpve_rate &lt;span style="color:#f92672">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>theory &lt;span style="color:#f92672">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> b &lt;span style="color:#f92672">in&lt;/span> bs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> theory&lt;span style="color:#f92672">.&lt;/span>append(&lt;span style="color:#ae81ff">.5&lt;/span>&lt;span style="color:#f92672">**&lt;/span>(np&lt;span style="color:#f92672">.&lt;/span>log(&lt;span style="color:#ae81ff">2&lt;/span>)&lt;span style="color:#f92672">*&lt;/span>b))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> bm &lt;span style="color:#f92672">=&lt;/span> BloomFilter(b&lt;span style="color:#f92672">*&lt;/span>set_size, k&lt;span style="color:#f92672">=&lt;/span>int(np&lt;span style="color:#f92672">.&lt;/span>log(&lt;span style="color:#ae81ff">2&lt;/span>)&lt;span style="color:#f92672">*&lt;/span>b))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> key &lt;span style="color:#f92672">in&lt;/span> tqdm(actually_there, &lt;span style="color:#e6db74">&amp;#34;Inserting&amp;#34;&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> bm&lt;span style="color:#f92672">.&lt;/span>add(key)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> key &lt;span style="color:#f92672">in&lt;/span> tqdm(actually_there, &lt;span style="color:#e6db74">&amp;#34;Sanity Checking&amp;#34;&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">assert&lt;/span> key &lt;span style="color:#f92672">in&lt;/span> bm
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fpve, total &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> total &lt;span style="color:#f92672">&amp;lt;&lt;/span> &lt;span style="color:#ae81ff">10_000&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key &lt;span style="color:#f92672">=&lt;/span> random_str()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> key &lt;span style="color:#f92672">in&lt;/span> actually_there: &lt;span style="color:#66d9ef">continue&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> key &lt;span style="color:#f92672">in&lt;/span> bm:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fpve &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> total &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fpve_rate&lt;span style="color:#f92672">.&lt;/span>append(fpve&lt;span style="color:#f92672">/&lt;/span>total)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>Inserting: 0%| | 0/1000000 [00:00&amp;lt;?, ?it/s]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>Sanity Checking: 0%| | 0/1000000 [00:00&amp;lt;?, ?it/s]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>Inserting: 0%| | 0/1000000 [00:00&amp;lt;?, ?it/s]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>Sanity Checking: 0%| | 0/1000000 [00:00&amp;lt;?, ?it/s]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>Inserting: 0%| | 0/1000000 [00:00&amp;lt;?, ?it/s]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>Sanity Checking: 0%| | 0/1000000 [00:00&amp;lt;?, ?it/s]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>Inserting: 0%| | 0/1000000 [00:00&amp;lt;?, ?it/s]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>Sanity Checking: 0%| | 0/1000000 [00:00&amp;lt;?, ?it/s]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>bs_lin &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>linspace(bs[&lt;span style="color:#ae81ff">0&lt;/span>], bs[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>plot(bs_lin, &lt;span style="color:#ae81ff">.5&lt;/span>&lt;span style="color:#f92672">**&lt;/span>(np&lt;span style="color:#f92672">.&lt;/span>log(&lt;span style="color:#ae81ff">2&lt;/span>)&lt;span style="color:#f92672">*&lt;/span>bs_lin), label&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Theory&amp;#34;&lt;/span>, c&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;b&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>plot(bs, theory, &lt;span style="color:#e6db74">&amp;#39;o&amp;#39;&lt;/span>, c&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;b&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>plot(bs, fpve_rate, &lt;span style="color:#e6db74">&amp;#39;-x&amp;#39;&lt;/span>, label&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Practice&amp;#34;&lt;/span>, c&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;orange&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>legend();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>xlabel(&lt;span style="color:#e6db74">&amp;#34;bits per element&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>Text(0.5, 0, &amp;#39;bits per element&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="images/63351a0ed542f328049edc92.png" alt="output image for above cell">&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>&amp;lt;Figure size 640x480 with 1 Axes&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>fpve_rate
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>[0.1551, 0.0211, 0.0024, 0.0003]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>at b=12, false positive rate is already less than 1%&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>Power and Politics in Today’s World</title><link>https://dhruvpatel.dev/notes/politics/power/</link><pubDate>Sat, 10 Sep 2022 09:07:23 +0000</pubDate><guid>https://dhruvpatel.dev/notes/politics/power/</guid><description>Lecture 2 Aug 19, 1991 Moskow: Coup d&amp;rsquo;etat against Mikhail Gorbichov (came power in Feb 1985)
When came to power in 1985, west thought what would become of Soviet Union?
Gorbachev was younger, talked like a western politician, allowed criticism of the regime. His policies were more open. It seemed like it was starting to change. In 1989 while revolution in Eastern Europe, Russia didn&amp;rsquo;t intervene; this was opposite to Russia&amp;rsquo;s earlier policies.</description><content>&lt;h1 id="lecture-2">Lecture 2&lt;/h1>
&lt;p>&lt;strong>Aug 19, 1991 Moskow&lt;/strong>: &lt;a href="https://en.wikipedia.org/wiki/1991_Soviet_coup_d%27%C3%A9tat_attempt">Coup d&amp;rsquo;etat against Mikhail Gorbichov&lt;/a> (came power in Feb 1985)&lt;/p>
&lt;p>When came to power in 1985, west thought what would become of Soviet Union?&lt;/p>
&lt;p>Gorbachev was younger, talked like a western politician, allowed criticism of the regime. His policies were more open. It seemed like it was starting to change. In 1989 while revolution in Eastern Europe, Russia didn&amp;rsquo;t intervene; this was opposite to Russia&amp;rsquo;s earlier policies. He was seen as a radical reformer. He was getting push back for his ideas.&lt;/p>
&lt;p>So in 1991 hardliners from the same party did a coup d&amp;rsquo;etat.&lt;/p>
&lt;p>Yeltsin (then elected president) declared that the coup d&amp;rsquo;état was illegal. The military also said that they didn&amp;rsquo;t support the coup.&lt;/p>
&lt;p>Boris Yelstin&lt;/p>
&lt;ul>
&lt;li>Initially supported Gorbachev&amp;rsquo;s reforms.&lt;/li>
&lt;li>He thought they should have been faster.&lt;/li>
&lt;li>He resigned from communist party in 1987. This was unprecedented then.&lt;/li>
&lt;li>It&amp;rsquo;s said that he even tried to do a suicide at one point.&lt;/li>
&lt;li>Eventually ran for a member of Russian parliament.&lt;/li>
&lt;li>June 1991, he was elected president of Russian parliament.&lt;/li>
&lt;/ul>
&lt;p>Without the support of the military, coup failed. Gorbachev didn&amp;rsquo;t leave the party, and fired the people from the party members involved in the coup. As the aftermath, Yelstin gained more popularity and power moved from Gorbachev to Yeltsin.&lt;/p>
&lt;p>Agenda&lt;/p>
&lt;ol>
&lt;li>why did ussr collapse&lt;/li>
&lt;li>why was it peaceful&lt;/li>
&lt;li>rise of gangster capitalism&lt;/li>
&lt;li>yelstin to putin&lt;/li>
&lt;li>why is Russia so corrupt and lessons from it.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="images/final_days_of_ussr.png" alt="final_days">&lt;/p>
&lt;h2 id="why-did-ussr-collapse">Why did USSR collapse?&lt;/h2>
&lt;ul>
&lt;li>Stores were empty.&lt;/li>
&lt;li>Government department store (7 story building) was empty.&lt;/li>
&lt;li>People making decisions didn&amp;rsquo;t have much/correct data.&lt;/li>
&lt;li>Prices were not set using supply and demand.&lt;/li>
&lt;li>System became unsustainable.&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Containment">Containment&lt;/a> (policy by George Kennan, ambassador to Russia) worked.&lt;/li>
&lt;li>1979: Involved in Afghanistan. Spent 1980s in this war. Failed war was a fiscal drain.&lt;/li>
&lt;li>Ronald Reagan announced Star Wars. Which increased the cost of the arm race.&lt;/li>
&lt;/ul>
&lt;h2 id="why-was-the-collapse-peaceful">Why was the collapse peaceful?&lt;/h2>
&lt;p>Why did elites gave up the power?
Comment from audience: Elites found ways to protect themselves from a collapse.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Exit,_Voice,_and_Loyalty">Albert Hirschman&amp;rsquo;s framework (1970)&lt;/a>&lt;/p>
&lt;p>How do we people response to decline?&lt;/p>
&lt;ol>
&lt;li>leave&lt;/li>
&lt;li>complain and get it changed (voice)&lt;/li>
&lt;li>try to change it by yourself&lt;/li>
&lt;/ol>
&lt;p>Which step to take depends on the cost.&lt;/p>
&lt;p>For example, a public company badly run&lt;/p>
&lt;ul>
&lt;li>Sell the shares&lt;/li>
&lt;li>You might be loyal to the company (get is changed), workers would use voice (cost of exit is high).&lt;/li>
&lt;/ul>
&lt;p>Collapse of loyalty from the elites and citizens by 1991.&lt;/p>
&lt;ul>
&lt;li>Evidence of brazen corruption (by professor&amp;rsquo;s first hand experience)&lt;/li>
&lt;li>Elites didn&amp;rsquo;t believe the ideology of the country&lt;/li>
&lt;/ul>
&lt;p>If you are not loyal, and the cost of exit is low, why stay and change the organization?&lt;/p>
&lt;p>What reduced the cost of exit?&lt;/p>
&lt;ul>
&lt;li>Reduced political costs of exit
&lt;ul>
&lt;li>Yeltsin exited the party to become a president&lt;/li>
&lt;li>Eduard Shevardnadze was a foreign minister of the Soviet Union, became president of Georgia&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Reduced economic costs of exit&lt;/li>
&lt;/ul>
&lt;h2 id="the-rise-of-oligarchs">The rise of oligarchs&lt;/h2>
&lt;p>Result of the collapse.&lt;/p>
&lt;ul>
&lt;li>In 1980s agencies (including KGP) started moving money offshore, to live another day to fight, in case of collapse. In 1991 beurocracy disappeared. Many people found themselves in control of unmanaged, unsupervised bank accounts.&lt;/li>
&lt;li>Theft of state assets.
&lt;ul>
&lt;li>Rem Viakhirev (deputi minister of Oil and Gas) was in charge of Gazprom (state venture controlling gas in Soviet Union)
&lt;ul>
&lt;li>he started giving away large assets to relatives and friends at below market price and pocketed the differences.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Russia had inherited weak and bankrupt state
&lt;ul>
&lt;li>1990s fiscal crisis for USSR, massive debts, fiscally strapped. And many people took advantage of it for example by lending money to the government in return for state assets which they got to keep, because government couldn&amp;rsquo;t pay it back.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="yeltsin-to-putin">Yeltsin to Putin&lt;/h2>
&lt;ul>
&lt;li>Last years of the Soviet Union was a weak state, the party however was strong.&lt;/li>
&lt;/ul>
&lt;p>Symptoms of a weak state?&lt;/p>
&lt;ul>
&lt;li>inability to tax (low capacity to raise revenue, didn&amp;rsquo;t have a capacity to audit oil companies for example)
&lt;ul>
&lt;li>Throughout the 1990s oligargs lobbied NOT to improve capacity to raise oil taxes&lt;/li>
&lt;li>Putin said he was determined the tax system (he did)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Haphazard use of power
&lt;ul>
&lt;li>Putin took down Khodorkovsky, (reclaimed the assets)&lt;/li>
&lt;li>Putin took down Vyakhirev, (reclaimed the assets)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="why-russia-is-corrupt">Why Russia is Corrupt.&lt;/h2>
&lt;ol>
&lt;li>Path dependence (historian&amp;rsquo;s) story&lt;/li>
&lt;li>Resource Curse (aka oil curse) (political economist&amp;rsquo;s) story&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;p>40% of Russian govenments revenue from oil&lt;/p>
&lt;/li>
&lt;li>
&lt;p>leads to corruption.&lt;/p>
&lt;p>If only way to get rich in Russia is to have access to the oil sector, then people who control the access will charge premium to get the access.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>If oil prices fall, government&amp;rsquo;s revenue also falls.&lt;/p>
&lt;h3 id="why-dont-they-diversify">Why don&amp;rsquo;t they diversify?&lt;/h3>
&lt;ul>
&lt;li>Beneficiaries like it this way.&lt;/li>
&lt;li>Government needs it: weak state&lt;/li>
&lt;li>A source of geopolitical power (e.g. germany was a top importer from Russia)&lt;/li>
&lt;/ul></content></item><item><title>Convex Optimization</title><link>https://dhruvpatel.dev/notes/maths/convex_optimization/convex_optimization/</link><pubDate>Fri, 26 Aug 2022 07:31:53 +0000</pubDate><guid>https://dhruvpatel.dev/notes/maths/convex_optimization/convex_optimization/</guid><description>Convex Optimization Note: These notes are not mathematically rigorous. These are meant for quick reference, please read the Convex Optimization book from Boyd and Vandenberghe for more rigorous treatment.
Chapter 2: Convex Sets Sets Affine Sets Affine combination: $\sum_{i=1}^{i=N} \theta_i x_i$ such that $\sum_{i=1}^{i=N} \theta_i = 1$.
Draw a line passing through any two points, if the whole line is in the set, the set is affine.
Every affine set C can be written as $C = V + x_0$ for any $x_0 \in C$, where V is a subspace.</description><content>&lt;h1 id="convex-optimization">Convex Optimization&lt;/h1>
&lt;!--eofm-->
&lt;p>Note:
These notes are not mathematically rigorous. These are meant for quick reference, please read the &lt;a href="https://web.stanford.edu/~boyd/cvxbook/">Convex Optimization book&lt;/a> from Boyd and Vandenberghe for more rigorous treatment.&lt;/p>
&lt;h2 id="chapter-2-convex-sets">Chapter 2: Convex Sets&lt;/h2>
&lt;h3 id="sets">Sets&lt;/h3>
&lt;h4 id="affine-sets">Affine Sets&lt;/h4>
&lt;p>Affine combination: $\sum_{i=1}^{i=N} \theta_i x_i$ such that $\sum_{i=1}^{i=N} \theta_i = 1$.&lt;/p>
&lt;p>Draw a line passing through any two points, if the whole line is in the set, the set is affine.&lt;/p>
&lt;p>Every affine set C can be written as $C = V + x_0$ for any $x_0 \in C$, where V is a subspace.&lt;/p>
&lt;dl>
&lt;dt>Affine Hull of C (&lt;strong>aff&lt;/strong> C)&lt;/dt>
&lt;dd>Smallest affine set, which contains C.&lt;/dd>
&lt;dt>Affine Dimension&lt;/dt>
&lt;dd>Dimension of an affine hull.&lt;/dd>
&lt;dt>Relative Interior&lt;/dt>
&lt;dd>&lt;strong>relint&lt;/strong> C = {x ∈ C | B(x, r) ∩ aff C ⊆ C for some r &amp;gt; 0}.&lt;/dd>
&lt;dt>Boundary&lt;/dt>
&lt;dd>&lt;strong>cl&lt;/strong> C - &lt;strong>relint&lt;/strong> C&lt;/dd>
&lt;/dl>
&lt;h5 id="example">Example&lt;/h5>
&lt;p>Let C = $\{x \in R^3 | −1 \le x_1 \le 1, −1 \le x_2 \le 1, x_3 = 0\}$. That is a square.&lt;/p>
&lt;ul>
&lt;li>Interior: Empty&lt;/li>
&lt;li>Relative interior: square without the border&lt;/li>
&lt;li>Boundary: the perimeter&lt;/li>
&lt;/ul>
&lt;h4 id="convex-sets">Convex Sets&lt;/h4>
&lt;dl>
&lt;dt>Convex combination&lt;/dt>
&lt;dd>$\sum_{i=1}^{i=N} \theta_i x_i$ such that $\sum_{i=1}^{i=N} \theta_i = 1$ and $\theta_i \ge 0$.&lt;/dd>
&lt;/dl>
&lt;p>Draw a line segment between any two points, if the whole line segment is in the set, the set is convex.&lt;/p>
&lt;h4 id="cones">Cones&lt;/h4>
&lt;dl>
&lt;dt>Cone&lt;/dt>
&lt;dd>A set C is called a cone, if for every x ∈ C and θ ≥ 0 we have $\theta x \in C$&lt;/dd>
&lt;dt>Convex Cone&lt;/dt>
&lt;dd>A set C is convex cone, if it is convex and a cone!&lt;/dd>
&lt;dt>Conic Combination&lt;/dt>
&lt;dd>$\sum_{i=1}^{i=N} \theta_i x_i$ such that $\theta_i \ge 0$.&lt;/dd>
&lt;/dl>
&lt;h3 id="some-important-examples">Some Important Examples&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>example&lt;/th>
&lt;th>affine?&lt;/th>
&lt;th>convex?&lt;/th>
&lt;th>convex cone?&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>hyperplane&lt;/td>
&lt;td>Y&lt;/td>
&lt;td>Y&lt;/td>
&lt;td>Y&lt;sup>1&lt;/sup>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>halfspace&lt;/td>
&lt;td>N&lt;/td>
&lt;td>Y&lt;/td>
&lt;td>Y&lt;sup>1&lt;/sup>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>norm ball&lt;/td>
&lt;td>N&lt;/td>
&lt;td>Y&lt;/td>
&lt;td>N&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>norm cones&lt;/td>
&lt;td>N&lt;/td>
&lt;td>Y&lt;/td>
&lt;td>Y&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>polyhedra&lt;/td>
&lt;td>&lt;/td>
&lt;td>Y&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>positive definite cone&lt;/td>
&lt;td>&lt;/td>
&lt;td>Y&lt;/td>
&lt;td>Y&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ol>
&lt;li>Only if 0 is in the set.&lt;/li>
&lt;/ol>
&lt;dl>
&lt;dt>Hyperplane&lt;/dt>
&lt;dd>$\{x | a^T x = b\} = \{x | a^T(x-x_0) = 0\}$ for constant vector a and scalar b. Where $x_0$ is any point in the hyperplane.&lt;/dd>
&lt;dt>Closed Half spaces&lt;/dt>
&lt;dd>$\{x | a^T x \le b\}$&lt;/dd>
&lt;dt>Euclidean Ball&lt;/dt>
&lt;dd>$B(x_c, r) = \{ x \, | \, \| x - x_c \|_2 \le r \} = \{ x_c + ru \, | \, \| u \|_2 \le 1\}$.&lt;/dd>
&lt;dt>Ellipsoid&lt;/dt>
&lt;dd>$ E = \{ x \, | \, (x - x_c)^T P^{-1} (x-x_c) \le 1 \} = \{ x_c + Au \, | \, \| u \|_2 \le 1 \}$. Where, P is positive definite matrix, and A is square and non singular matrix.&lt;/dd>
&lt;dt>Norm Cones&lt;/dt>
&lt;dd>$\{ (x, t) \, | \, \| x \| \le t\} \subset R^{n+1}$.&lt;/dd>
&lt;dt>Second order cone (Lorentz cone)&lt;/dt>
&lt;dd>Norm cone where norm is euclidean norm.&lt;/dd>
&lt;dt>Polyhedron&lt;/dt>
&lt;dd>A polyhedron is thus the intersection of a finite number of halfspaces and hyperplanes.&lt;/dd>
&lt;dt>Simplex&lt;/dt>
&lt;dd>Simplex is a special polyhedron. If $v_0, \dots, v_k$ are affinely independent (that is $v_1-v_0, \dots , v_k-v_0$ are linearly independent) then simplex is just convex hull of $(v_0, \dots, v_k)$.&lt;/dd>
&lt;/dl>
&lt;h3 id="operations-that-preseve-convexity">Operations that preseve convexity&lt;/h3>
&lt;h4 id="intersection">Intersection&lt;/h4>
&lt;p>If $S_\alpha$ is a convex set for for every $\alpha \in \cal{A}$ where $\cal{A}$ is any set (could be uncountable infinite), then $\cap_{\alpha \in \cal{A}} S_\alpha$ is also convex. Converse is also true. Every closed convex set S is a (usually infinite) intersection of halfspaces.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> numpy &lt;span style="color:#66d9ef">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> matplotlib.pyplot &lt;span style="color:#66d9ef">as&lt;/span> plt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>x &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>linspace(&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, num&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">100&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ts &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>linspace(&lt;span style="color:#f92672">-&lt;/span>np&lt;span style="color:#f92672">.&lt;/span>pi&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>, np&lt;span style="color:#f92672">.&lt;/span>pi&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>, num&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">20&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">S&lt;/span>(t):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c1, c2 &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>array([np&lt;span style="color:#f92672">.&lt;/span>cos(t), np&lt;span style="color:#f92672">.&lt;/span>cos(&lt;span style="color:#ae81ff">2&lt;/span>&lt;span style="color:#f92672">*&lt;/span>t)])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c, m &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">/&lt;/span>c2, c1&lt;span style="color:#f92672">/&lt;/span>c2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> (&lt;span style="color:#66d9ef">lambda&lt;/span> x: c &lt;span style="color:#f92672">-&lt;/span> m&lt;span style="color:#f92672">*&lt;/span>x, &lt;span style="color:#66d9ef">lambda&lt;/span> x: &lt;span style="color:#f92672">-&lt;/span>c &lt;span style="color:#f92672">-&lt;/span> m&lt;span style="color:#f92672">*&lt;/span>x)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> t &lt;span style="color:#f92672">in&lt;/span> ts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y1, y2 &lt;span style="color:#f92672">=&lt;/span> S(t)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> plt&lt;span style="color:#f92672">.&lt;/span>plot(x, y1(x), &lt;span style="color:#e6db74">&amp;#39;k&amp;#39;&lt;/span>, lw&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;.5&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> plt&lt;span style="color:#f92672">.&lt;/span>plot(x, y2(x), &lt;span style="color:#e6db74">&amp;#39;k&amp;#39;&lt;/span>, lw&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;.5&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>ylim(&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#f92672">.&lt;/span>xlim(&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>);
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="images/5c5d61291f1a421f53dc37afdf5908e5.png" alt="output image for above cell">&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>&amp;lt;Figure size 432x288 with 1 Axes&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="affine-functions">Affine Functions&lt;/h4>
&lt;p>Function f is affine if f = Ax + b for some A and b.&lt;/p>
&lt;p>If S is convex then the image of S under f (i.e. $f(S) = \{f(x) | x \in S\}$) is also convex.&lt;/p>
&lt;p>If g is a function such that the image of g is S and S is convex, then the inverse image of S (i.e. $\{x | g(x) \in S\}$) is also convex.&lt;/p>
&lt;dl>
&lt;dt>Hyperbolic Cone&lt;/dt>
&lt;dd>$\{ x | x^T P x \le (c^T x)^2, c^T x \ge 0 \}$&lt;/dd>
&lt;/dl>
&lt;h4 id="perspective-function">Perspective Function&lt;/h4>
&lt;p>$P: R^{n+1} \to R^n$, where &lt;strong>dom&lt;/strong> P = $R^n \times R_{++}$. P is defined as $P(z, t) = z/t$.&lt;/p>
&lt;p>If domain of P is convex then the image of P is also convex.&lt;/p>
&lt;h4 id="linear-fractional-function">Linear Fractional Function&lt;/h4>
&lt;p>Combination of Perspective and affine.&lt;/p>
&lt;p>$$
f(x) = \frac{Ax+b}{c^T x + d}
$$&lt;/p>
&lt;p>where $c^Tx+d &amp;gt; 0$.&lt;/p>
&lt;p>If C is convex, then $f(C)$ is also convex.&lt;/p>
&lt;p>If C is convex, then $f^{-1}(C)$ is also convex.&lt;/p>
&lt;h3 id="generalized-inequalities">Generalized Inequalities&lt;/h3>
&lt;dl>
&lt;dt>Proper Cone&lt;/dt>
&lt;dd>A cone is proper if it is convex, closed, solid (non empty interior) and is pointed (it does not contain a line).&lt;/dd>
&lt;dt>Generalized inequality with respect to proper cone K&lt;/dt>
&lt;dd>$ x \le_K y \iff y - x \in K$. Also $ x &amp;lt;_K y \iff y-x \in \textbf{int} K$&lt;/dd>
&lt;/dl>
&lt;p>Examples&lt;/p>
&lt;ol>
&lt;li>$ K = R^n_+ $. Then for vectors x and y, $x \le y$ iff $x_i \le y_i$ for all i.&lt;/li>
&lt;li>$ K = S^n_+ $. Then for matrices A and B, $A \le B$ iff $B - A$ is positive semidefinite.&lt;/li>
&lt;/ol>
&lt;h4 id="minimum-and-minimal">Minimum and Minimal&lt;/h4>
&lt;dl>
&lt;dt>Minimum&lt;/dt>
&lt;dd>$x \in S$ is the minimum if for every $y \in S$ we have $ x \le y$.&lt;/dd>
&lt;dt>Minimal&lt;/dt>
&lt;dd>$x \in S$ is a minimal if $y \in S, y \le x$ only if $y = x$.&lt;/dd>
&lt;/dl>
&lt;h3 id="separating-hyperplane">Separating Hyperplane&lt;/h3>
&lt;p>If C and D are disjoint convex set, there exists a and b such that $a^T x \ge b$ for all x in D and $a^T x \le b$ for all x in C.&lt;/p>
&lt;p>If C is a convex and $x_0 \notin C$. Then $x_0$ and C can be strictly separated.&lt;/p>
&lt;p>Converse of above theorem is not true. That is there may exist convex sets C and D which are separated by a hyperplane, they still might intersect. Simple example is $C = D = \{0\} \subset R$. $x = 0$ separates C and D.&lt;/p>
&lt;p>Converse holds if C and D are convex and C is open set.&lt;/p>
&lt;p>Thus, any two convex sets C and D, at least one of which is open, are disjoint &lt;strong>iff&lt;/strong> there exists a separating hyperplane.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;/code>&lt;/pre>&lt;/div></content></item><item><title>Analysis (Not Rigorous) Notes</title><link>https://dhruvpatel.dev/notes/maths/analysis/</link><pubDate>Sun, 10 Jul 2022 18:37:29 +0530</pubDate><guid>https://dhruvpatel.dev/notes/maths/analysis/</guid><description>Definitions Interior Point $x \in C$ A point is an interior point if we can find a ball around x which is completely inside C. Open Set A set is open, if all points are interior points. Closed Set A set is closed, if its complement is open. Closure Take the set&amp;rsquo;s complement. Find its interior. Take the complement of it. You got a closure. A point is in a closure, if for every $\epsilon$, you can find a point from the original set within $\epsilon$ distance.</description><content>&lt;h2 id="definitions">Definitions&lt;/h2>
&lt;dl>
&lt;dt>Interior Point $x \in C$&lt;/dt>
&lt;dd>A point is an interior point if we can find &lt;strong>a&lt;/strong> ball around x which is completely inside C.&lt;/dd>
&lt;dt>Open Set&lt;/dt>
&lt;dd>A set is open, if all points are interior points.&lt;/dd>
&lt;dt>Closed Set&lt;/dt>
&lt;dd>A set is closed, if its complement is open.&lt;/dd>
&lt;dt>Closure&lt;/dt>
&lt;dd>Take the set&amp;rsquo;s complement. Find its interior. Take the complement of it. You got a closure.&lt;/dd>
&lt;dd>
&lt;p>A point is in a closure, if for &lt;strong>every&lt;/strong> $\epsilon$, you can find a point from the original set within $\epsilon$ distance.&lt;/p>
&lt;/dd>
&lt;dt>Boundary&lt;/dt>
&lt;dd>take the closure and remove the interior.&lt;/dd>
&lt;/dl>
&lt;h2 id="examples">Examples&lt;/h2>
&lt;p>For example, closure of (-1, 1) in real line is given by complement of the interior of the (-inf, 1] U [1, inf). Which is the complement of (-inf, 1) U (1, inf), which is [-1, 1].&lt;/p></content></item></channel></rss>