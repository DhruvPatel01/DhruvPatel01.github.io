<!doctype html><html lang=en><head><title>Quantum Mechanics Notes :: Dhruv Patel</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Quantum Mechanics Differences from Classical Mechanics States have different logical structure than CM. States and measurements are different unlike CM. e.g. Position and Momemntum can be determined by experiments in CM. Spins Particles have properties attached to it. e.g. mass, electric charge.
Even a specific particle is not completely specified by its position.
Attached to electron is an extra degree of freedom, called spin. Spin is as quantum mechanical as it can and we should not try to visualize it."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://dhruvpatel.dev/notes/physics/theoretical_minimum/qm-notes/><link rel=stylesheet href=https://dhruvpatel.dev/assets/style.css><link rel=apple-touch-icon href=https://dhruvpatel.dev/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=https://dhruvpatel.dev/img/favicon/orange.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Quantum Mechanics Notes"><meta property="og:description" content="Quantum Mechanics Differences from Classical Mechanics States have different logical structure than CM. States and measurements are different unlike CM. e.g. Position and Momemntum can be determined by experiments in CM. Spins Particles have properties attached to it. e.g. mass, electric charge.
Even a specific particle is not completely specified by its position.
Attached to electron is an extra degree of freedom, called spin. Spin is as quantum mechanical as it can and we should not try to visualize it."><meta property="og:url" content="https://dhruvpatel.dev/notes/physics/theoretical_minimum/qm-notes/"><meta property="og:site_name" content="Dhruv Patel"><meta property="og:image" content="https://dhruvpatel.dev/img/favicon/orange.png"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2022-11-18 04:16:24 +0000 UTC"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css integrity=sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js integrity=sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4OPoNhVgjdGFwhvN02Ja crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body class=orange><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Dhruv Patel</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about/>About</a></li><li><a href=https://github.com/DhruvPatel01/>Github</a></li><li><a href=/notes>Notes</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about/>About</a></li><li><a href=https://github.com/DhruvPatel01/>Github</a></li><li><a href=/notes>Notes</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=https://dhruvpatel.dev/notes/physics/theoretical_minimum/qm-notes/>Quantum Mechanics Notes</a></h1><div class=post-meta><span class=post-date>2022-11-18</span></div><div class=post-content><div><h1 id=quantum-mechanics>Quantum Mechanics<a href=#quantum-mechanics class=hanchor arialabel=Anchor>&#8983;</a></h1><h2 id=differences-from-classical-mechanics>Differences from Classical Mechanics<a href=#differences-from-classical-mechanics class=hanchor arialabel=Anchor>&#8983;</a></h2><ol><li>States have different logical structure than CM.</li><li>States and measurements are different unlike CM. e.g. Position and Momemntum can be determined by experiments in CM.</li></ol><h2 id=spins>Spins<a href=#spins class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Particles have properties attached to it. e.g. mass, electric charge.</p><p>Even a specific particle is not completely specified by its position.</p><p>Attached to electron is an extra degree of freedom, called spin. Spin is as quantum mechanical as it can and we should not try to visualize it.</p><h2 id=testing>Testing<a href=#testing class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Propositions:
$$
\begin{align*}
A &: \sigma_z = +1 \\
B &: \sigma_x = +1 \\
\end{align*}
$$</p><h3 id=classically>Classically,<a href=#classically class=hanchor arialabel=Anchor>&#8983;</a></h3><p>To test (A or B), one could first <strong>gently</strong> test $\sigma_z$. If it is -1, one would <strong>gently</strong> test $\sigma_x$. The result of doing it otherway (i.e. B or A) will be the same as doing (A or B). The reason is that classically, measurements are gentle. They don&rsquo;t change the state of the system.</p><h3 id=in-quantum-mechanics>In Quantum Mechanics,<a href=#in-quantum-mechanics class=hanchor arialabel=Anchor>&#8983;</a></h3><p>If some entity prepares the spin in $\sigma_z = +1$ state, and we measure <code>A or B</code> (whether we use short circuit or not), we will measure it to be true. However, if we measure <code>B or A</code>, there is 25% chance that we will measure it to be false.</p><p>What about <code>A and B</code>? If we conclude that <code>A and B</code> is true, can we confirm it again? Answer is no. Since to compute B, we had to measure $\sigma_x$, which ruined measurement of A. Thus we can&rsquo;t confirm it. i.e. experiment is not reproducible.</p><h2 id=complex-numbers>Complex Numbers<a href=#complex-numbers class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Two ways to represent them.</p><p>In cartesian coordinates, $z = x + iy$.</p><p>In polar coordinates, $z = re^{i\theta}$.</p><p>$ x_1 x_2 = (r_1e^{i\theta_1})(r_2e^{i\theta_2}) = r_1 r_2 e^{i(\theta_1 + \theta_2)}$.</p><p>$z = x + iy = re^{i\theta}$</p><p>$z^\ast = x - iy = re^{-i\theta}$</p><p>$z^\ast z = r^2$, i.e. a real number</p><h3 id=phase-factors>Phase Factors<a href=#phase-factors class=hanchor arialabel=Anchor>&#8983;</a></h3><p>These are complex numbers whose r componenet is 1. Following holds for them,</p><p>$$
\begin{align}
z^\ast z &= 1 \\
z &= e^{i\theta}\\
z &= \cos\theta + i \sin\theta
\end{align}
$$</p><p>$$\renewcommand{\bra}[1]{\left\langle{#1}\right|}$$
$$\renewcommand{\ket}[1]{\left|{#1}\right\rangle}$$
$$\renewcommand{\braket}[1]{\left\langle{#1}\right\rangle}$$</p><h2 id=vector-spaces>Vector Spaces<a href=#vector-spaces class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Vector spaces is familiar concept from abstract linear algebra.</p><h3 id=complex-conjugate-of-space-v>Complex Conjugate of space V<a href=#complex-conjugate-of-space-v class=hanchor arialabel=Anchor>&#8983;</a></h3><p>For every $\ket{A}$ there exists $\bra{A}$ in conjugate space. This space has following properties.</p><ol><li>The conjugate of $\ket{A} + \ket{B}$ is $\bra{A}+\bra{B}$.</li><li>Conjugate of $z\ket{A}$ is $z^\ast \bra{A} = \bra{A}z^\ast $.</li></ol><p>In the concrete case where ket space is column vectors, bra space is denoted as row vectors.</p><p>i.e. if</p><p>$$
\begin{align*}
\ket{A} = \begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
\vdots \\
\alpha_n
\end{bmatrix}
\end{align*}
$$</p><p>then
$$
\begin{align*}
\bra{A} = \begin{bmatrix}
\alpha_1^\ast & \alpha_2^\ast & \dots & \alpha_n^\ast
\end{bmatrix}.
\end{align*}
$$</p><h3 id=inner-products>Inner Products<a href=#inner-products class=hanchor arialabel=Anchor>&#8983;</a></h3><p>Inner product is always between bras and kets. It is written like $\braket{B|A}$. The result is a complex numbers.</p><h4 id=axioms-of-inner-product>Axioms of inner product.<a href=#axioms-of-inner-product class=hanchor arialabel=Anchor>&#8983;</a></h4><ol><li>Inner product is linear. $\bra{C} + (\ket{A}+\ket{B}) = \braket{C|A} + \braket{C|B}$.</li><li>$\braket{B|A} = \braket{A|B}^\ast $.</li></ol><h4 id=special-vectors>Special vectors<a href=#special-vectors class=hanchor arialabel=Anchor>&#8983;</a></h4><ol><li>Normalized: $\braket{A|A} = 1$.</li><li>Orhthogonal: $\braket{B|A} = 0$.</li></ol><h3 id=orhtonormal-basis>Orhtonormal Basis.<a href=#orhtonormal-basis class=hanchor arialabel=Anchor>&#8983;</a></h3><p>Let our space be N dimensional. And let the orthonomal basis denoted by $\ket{i}$.</p><p>$$
\ket{A} = \sum_i \alpha_i \ket{i},
$$</p><p>where, $\alpha_j = \braket{j|A}$. (To derive this, multiply both sides by $\bra{j}$.</p><h1 id=states>States<a href=#states class=hanchor arialabel=Anchor>&#8983;</a></h1><p>In CM, knowing the state means knowing everything that is necessary to predict the future.</p><p>In QM, knowing the state means knowing as much as can be known about how the system was prepared.</p><p>Apparatus $\cal{A}$ can be oriented on any axis. If we orient it along z axis, the measured spin will either be +1 or -1.</p><p>$\sigma_z = \pm 1$. We can denote +1 as state $\ket{u}$ and -1 as $\ket{d}$.</p><p>Similarly $\sigma_x = \pm 1$, can be denoted by $\ket{r}$ and -1 as $\ket{l}$. And $\sigma_y = \pm 1$, can be denoted by $\ket{o}$ and -1 as $\ket{i}$.</p><p>If two states are orthogonal than these two states can be determined together. For example, if $\sigma_z$ was prepared to be in $\ket{u}$, for any subsequent measurements probability that $\ket{d}$ is detected is 0. Thus for binary spin, the state space is two dimensional. For now we can take $\ket{u}, \ket{d}$ as the basis vectors.</p><p>Then, the generic state $\ket{A} = \alpha_u \ket{u} + \alpha_d \ket{d}$. Where $\alpha_i = \braket{i|A}$.</p><p>The meaning of,</p><ul><li>$\alpha_u^\ast \alpha_u$: If the spin was prepared in $\ket{A}$ state, $\alpha_u^\ast \alpha_u$ is the probability that $\sigma_z = +1$.</li><li>$\alpha_d^\ast \alpha_d$: is the probability that $\sigma_z = -1$.</li></ul><p>Since probabilities must add to 1, $\alpha_u^\ast \alpha_u + \alpha_d^\ast \alpha_d = 1$. It is equivalent to saying that $\ket{A}$ is normalized, i.e. $\braket{A|A} = 1$.</p><p>General principle of quantum systems: the state of a system is represented by a unit (normalized) vector in a vector space of states. Moreover, the squared magnitudes of the components of the state-vector, <strong>along particular basis vectors</strong>, represent probabilities for various experimental outcomes.</p><h3 id=representing-ketr-and-ketl-using-above-basis-vectors>Representing $\ket{r}$ and $\ket{l}$ using above basis vectors<a href=#representing-ketr-and-ketl-using-above-basis-vectors class=hanchor arialabel=Anchor>&#8983;</a></h3><p>We know that if A initially prepares the state in $\ket{r}$, $\sigma_z = \pm 1$ with equal probability. Hence, $\alpha_u^\ast \alpha_u =\alpha_d^\ast \alpha_d = \frac{1}{2}$. One choice is to have $\alpha_u = \alpha_d = \frac{1}{\sqrt{2}}$.</p><p>$\ket{r} = \frac{1}{\sqrt{2}} \ket{u} + \frac{1}{\sqrt{2}} \ket{d}$. (There are is still ambiguity, called phase ambiguity.)</p><p>To solve for $\ket{l}$ the above process repeats. But, in addition, $\braket{l|r} = 0$. One choice is $[\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}]$. But it is not the only choice. Even for fixed choice for $\ket{r}$, you can multiply above choice by a phase factor ($z = e^{i\theta}$), and still satisfy the two constraints. Later, we will find out that no measurable quantity is sensitive to the overall phase-factor, and therefore we can ignore it when specifying states.</p><h3 id=representing-keti-and-keto-using-above-basis-vectors>Representing $\ket{i}$ and $\ket{o}$ using above basis vectors<a href=#representing-keti-and-keto-using-above-basis-vectors class=hanchor arialabel=Anchor>&#8983;</a></h3><p>To solve for $\ket{i}$ and $\ket{o}$, we need same conditions as we needed above. But we also need additional constrains. For example, if A prepares the state in $\ket{i}$,$\sigma_x = \pm1$, with equal probability. Also $\braket{i|o} = 0$.</p><p>The following solution solves for these constraints (up to phase-factor ambiguity).</p><p>$\ket{i} = \frac{1}{\sqrt{2}} \ket{u} + \frac{i}{\sqrt{2}} \ket{d}$.</p><p>$\ket{o} = \frac{1}{\sqrt{2}} \ket{u} - \frac{i}{\sqrt{2}} \ket{d}$.</p><p>With the previous discussion, the vectors can be represented in column format as below.</p><p>$$
\begin{align*}
\ket{u} &= \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \ket{d} = \begin{bmatrix} 0 \\ 1 \end{bmatrix} \\
\end{align*}
$$</p><h3 id=matricies>Matricies<a href=#matricies class=hanchor arialabel=Anchor>&#8983;</a></h3><p>If we have a basis $\ket{i}$, a vector $\ket{A}$ can be rewritten as,</p><p>$$ \ket{A} = \sum_i \alpha_i \ket{i} = \sum_i \ket{i} \braket{i|A} $$.</p><p>Similarly,
$$ \bra{A} = \sum_i \braket{A|i}\bra{i} $$.</p><p>Axiom: Physical observables are described by linear operators.</p><p>Observables are the things that we can measure. e.g., coordinates of a particle; the energy, momentum, or angular momentum of a system; or the electric field at a point in space.</p><p>$$
\begin{align*}
M \ket{A} &= \ket{B} \\
M \sum_j \alpha_j \ket{j} &= \sum_j \beta_j \ket{j} \\
\sum_j \alpha_j M\ket{j} &= \sum_j \beta_j \ket{j} \text{;;assuming M is linear} \\
\sum_j \alpha_j \bra{k} M \ket{j} &= \sum_j \beta_j \braket{k|j} \text{;;multiply both sides by} \bra{k} \\
\sum_j \alpha_j m_{kj} &= \beta_k\\
\end{align*}
$$</p><p>Note that each $m_{kj}$ is a complex number. We can think of M in terms of matrix (defined by a choice of basis vectors).</p><h4 id=eigenvectors-and-eigenvalues>Eigenvectors and Eigenvalues<a href=#eigenvectors-and-eigenvalues class=hanchor arialabel=Anchor>&#8983;</a></h4><p>$M \ket{\lambda} = \lambda \ket{\lambda}$. $\lambda$ is an eigenvalue, and $\ket{\lambda}$ is an eigenvector.</p><h4 id=linear-operators-on-bra-vectors>Linear operators on bra vectors<a href=#linear-operators-on-bra-vectors class=hanchor arialabel=Anchor>&#8983;</a></h4><p>$$
\begin{align*}
\bra{B} &= \begin{bmatrix} b_1^\ast & b_2^\ast & b_3^\ast \end{bmatrix} \\
M &= \begin{bmatrix}
m_{11} & m_{12} & m_{13} \\
m_{21} & m_{22} & m_{23} \\
m_{31} & m_{32} & m_{33} \\
\end{bmatrix}
\end{align*}
$$</p><p>Than $\bra{B} M$ is just row vector multiplied by matrix M.</p><h4 id=hermitian-conjugate>Hermitian Conjugate<a href=#hermitian-conjugate class=hanchor arialabel=Anchor>&#8983;</a></h4><p>$$
\begin{align*}
M^\dagger &= (M^T)^\ast \\
M \ket{A} &= \ket{B} \\
\bra{A} M^\dagger &= \bra{B} \\
\end{align*}
$$</p><h4 id=hermitian-operators>Hermitian Operators<a href=#hermitian-operators class=hanchor arialabel=Anchor>&#8983;</a></h4><ul><li>Observables quantities in classcial mechanics are real numbers. i.e. they are their own complex conjugate.</li><li>Observables in quantum mechanics (i.e. linear operators) are also their own complex conjugates. Such operators are called Hermitian Operators. $M^\dagger = M$.</li></ul><h5 id=properties-of-hermitian-operators>Properties of Hermitian Operators<a href=#properties-of-hermitian-operators class=hanchor arialabel=Anchor>&#8983;</a></h5><ul><li>Their eigenvalues are real.</li><li>Their eigenvectors form an orthonormal basis. (i.e. their eigenvectors are orthonormal and they form a basis)</li></ul><h2 id=principles>Principles<a href=#principles class=hanchor arialabel=Anchor>&#8983;</a></h2><ol><li>The observable or measurable quantities of QM are represented by a linear operator L.</li><li>The possible readings of the measurements are eigenvalues $\lambda_i$. The state for which reading is <strong>unambiguously</strong> $\lambda_i$ is the corresponding eigenvector $\ket{\lambda_i}$.</li><li>Unambiguously distinguishable states are represented by orthogonal vectors. e.g. $\braket{u|d} = 0$.</li><li>If $\ket{A}$ is the state vector of the system, and the observable L is measured, the probability of observing $\lambda_i$ is given by $\braket{A|\lambda_i}\braket{\lambda_i|A}$.</li></ol><p>Since the readings (i.e., eigenvalues) are real and eigenvectors are orthogonal, the operator L must be hermitian.</p><p>P1 says that $\sigma_x, \sigma_y, \text{and} \sigma_z$ are identified with a specific linear operator in 2D space of states describing the states.
P2 says that the actual measurments can take discrete values. E.g., energy of atom will be one of the established energy levels of the atom.</p><h3 id=3-vector-operator-sigma>3-Vector Operator $\sigma$<a href=#3-vector-operator-sigma class=hanchor arialabel=Anchor>&#8983;</a></h3><ul><li>Just as a spin-measuring apparatus can only answer questions about a spin&rsquo;s orientation in a specific direction, a spin operator can only provide information about the spin component in a specific direction.</li><li>To physically measure spin in a different direction, we need to rotate the apparatus to point in the new direction. The same idea applies to the spin operatorâ€”if we want it to tell us about the spin component in a new direction, it too must be &ldquo;rotated&rdquo; but this kind of rotation is accomplished mathematically.</li></ul><h3 id=operator-matrices>Operator Matrices<a href=#operator-matrices class=hanchor arialabel=Anchor>&#8983;</a></h3><p>Using above four principles, and solving linear equations, we can derive them.</p><p>$$
\sigma_z = \begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix}, \sigma_x = \begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}, \sigma_y = \begin{bmatrix}
0 & -i \\
i & 0
\end{bmatrix} \text{.}
$$</p><p>These along with Identity matrix are called Pauli Matrices.</p><p>IMPORTANT: Applying the operator L to state $\ket{A}$ does not change the state to $L\ket{A}$. $L\ket{A}$ is actually a supuerposition and tells us the probabilities of basis states. When we actually measure using L, the system is changed to one of the eignestates unambiguously.</p><p>There is nothing special about these three operators. We can take any direction $\hat{n} = (n_x, n_y, n_z)$, orient the apparatus A along $\hat n$, activate A, and measure the component of the spin along $\hat n$. That means there has to be an operator that represents this operation. Indeed, this operator is given by $\sigma_n = \sigma \cdot \hat n$, where $\sigma = (\sigma_x, \sigma_y, \sigma_z)$.</p><p>$$
\sigma_n = \begin{bmatrix}
n_z & (n_x - i n_y) \\
(n_x + i n_y) & -n_z
\end{bmatrix}
$$</p><h3 id=spin-polarization-principle>Spin-Polarization Principle<a href=#spin-polarization-principle class=hanchor arialabel=Anchor>&#8983;</a></h3><p>For any state $\ket{A} = \alpha_u \ket{u} + \alpha_d \ket{d}$, there exists some direction $\hat n$ such that $\sigma \cdot \hat n = \ket{A}$.</p><p>States of the spins are characterized by a polarization vector, and along the polarization vector the component of the spin is predictably +1.</p><p>General spin state is $cos(\frac{\beta}{2}) \ket{u} + e^{i\phi} sin(\frac{\beta}{2}) \ket{d}$.</p><h3 id=time-development-operator>Time Development Operator<a href=#time-development-operator class=hanchor arialabel=Anchor>&#8983;</a></h3><p>Let the closed system be in state $\ket{\Psi(t)}$. Thus, state can be different at different times.</p><p>The time development operator $U(t)$ tells us how the system evolves with time.</p><p>$$\ket{\Psi(t)} = U(t) \ket{\Psi(0)}$$.</p><p>Thus, the time evolution of the state is deterministic.</p><p>Quantum mechanics assumes,</p><ol><li>that U(t) is a linear operator.</li><li>If two states are orthonormal, they remain orthonormal in the evolution. i.e., $\braket{\Psi(0)|\Phi(0)} =0 \implies \braket{\Psi(t)|\Phi(t)} = 0$.</li></ol><p>As a consequence of these two assumptions, it is easy to prove that $U(t)^{\dagger}U(t) = I$. Such operator is called unitary operator.</p><p>Principle 5: Time evolution of state vectors is unitary.</p><p>Unitarity also implies that as time evolves, the overlap (inner product) between two states remains the same.</p><p>Quantum Mechanics also assumes that time evolution is continuous.</p><p>Thus, for small time $\epsilon, U(\epsilon) = I - i \epsilon H$. Using the unitarity condition $U(t)^{\dagger}U(t) = I$, we can show that $H^{\dagger} = H$.</p><p>Thus, H is hermitian. i.e., it is an obeservable, and has complete set of orthonormal eigenvectors.</p><p>We will see that, the eigenvalues of H are the values that result from measuring the energy levels of the quantum system. Thus, it has resemblance to the hamiltonian from the classical mechanics.</p><p>Using the continuity assumption inside $\ket{\Psi(t)} = U(t) \ket{\Psi(0)}$, we can derive,</p><p>$$ \frac{d \ket{\Psi}}{dt} = -iH\ket{\Psi}$$.</p><p>This equation is known as Generalized Schrodinger equation or time-dependent schrodinger&rsquo;s equation. If we know the Hamiltonian of the system, we can know how the state of the undisturbed system evolves with the time.</p><h4 id=planks-constant>Plank&rsquo;s constant<a href=#planks-constant class=hanchor arialabel=Anchor>&#8983;</a></h4><p>h = 6.6 * 1e-34 kgm^2/s1.</p><p>$\hbar = \frac{h}{2\pi} = 1.054571726 \dots x 10^{-34} \frac{kgm^2}{s}$</p><p>In the gen. schrodinger&rsquo;s eqn, lhs has units of 1/time, whereas rhs has units of energy (kgm^2/s^2, because H is hamiltonian). This is wrong. However multiplying lhs by Plank&rsquo;s constant, units are proper. Thus, the correct Generalized schrodinger&rsquo;s equation is $$ \hbar \frac{d \ket{\Psi}}{dt} = -iH\ket{\Psi}$$.</p><h4 id=expectation>Expectation<a href=#expectation class=hanchor arialabel=Anchor>&#8983;</a></h4><p>If L is an observable, the expectation is defined as $\braket{L} = \sum_i P(\lambda_i) \lambda_i$.</p><p>When state of the system is $\ket{A} = \sum_i \alpha_i \lambda_i$, the expecation can be computed as $\braket{L} = \sum_i \alpha_i^\ast \alpha_i \lambda_i = \braket{A|L|A}$.</p><p>Try to apply this for state $\ket{r}$ and observable =$\sigma_z$. The arithmetic should result in value 0.</p><h4 id=effect-of-the-phase-factor>Effect of the phase factor.<a href=#effect-of-the-phase-factor class=hanchor arialabel=Anchor>&#8983;</a></h4><p>We can multiply the state vectors by a phase factor $e^{i\theta}$ for any $\theta$. This does no make any difference. Although, probability amplitude will change i.e., $\alpha_i \to e^{i\theta} \alpha_i$, the probability does not change, i.e., $\alpha_i^\ast \alpha_i$ remains unchanged. Similarly, the expectation of the observable does not change.</p><h4 id=change-in-the-expectation>Change in the expectation<a href=#change-in-the-expectation class=hanchor arialabel=Anchor>&#8983;</a></h4><p>$\frac{d \braket{\Psi(t)|L|\Psi(t)}}{dt} = \braket{\dot\Psi(t)|L|\Psi(t)} + \braket{\Psi(t)|L|\dot\Psi(t)}$. Using generalized schrodinger&rsquo;s equations,</p><p>$\frac{d \braket{\Psi(t)|L|\Psi(t)}}{dt} = \frac{i}{\hbar} \braket{\Psi(t)|HL - LH|\Psi(t)}$.</p><p>Linear operators don&rsquo;t commute, so HL != LH.</p><p><strong>Commutator</strong>: Given two operators L, and M, LM - ML is called the commutator of L with M, and is denoted by [L, M]. Note that [L, M] = -[M, L].</p><p>Thus, $\frac{d}{dt} \braket{L} = \frac{-i}{\hbar} \braket{[L, H]}$.</p><p>It is easy to prove that $i[L, H]$ is also Hermitian. Thus, a valid observable.</p><p>This has resemblance to classical mechanics. $\dot{F} = \{F, H\}$.</p><h3 id=conservation-in-quantum-mechanics>Conservation in quantum mechanics<a href=#conservation-in-quantum-mechanics class=hanchor arialabel=Anchor>&#8983;</a></h3><p>To say that an observable Q is conserved is to say that expected value $\braket{Q}$ does not change with time. A stronger condition is that any moment $\braket{Q^m}$ does not change with time.</p><p>$\braket{Q}$ does not change ammounts to $[Q, H] = 0$. That is Q commutes with H. Using the properties of commutator, it is easy to prove that $[Q^m, H] = 0$ for any $m \ge 1$.</p><p>It turns out that if $[Q, H] = 0$ then for <strong>any</strong> function of Q, $[f(Q), H] = 0$.</p><p>H is also conserved, as $[H, H] = 0$. H is defined to be the energy of the quantum system.</p><h3 id=solving-schrodingers-equation>Solving Schrodinger&rsquo;s equation<a href=#solving-schrodingers-equation class=hanchor arialabel=Anchor>&#8983;</a></h3><p>Time dependent Schrodinger&rsquo;s equation is $ \hbar \frac{d \ket{\Psi}}{dt} = -iH\ket{\Psi}$.</p><p>Since H represents enrgy, the oberservable values of energy are eigenvalues of H. Call them $\ket{E_j}$ and $E_j$.</p><p>$$
H \ket{E_j} = E_j \ket{E_j}
$$</p><p>These are call time-independent schrodinger&rsquo;s equations.</p><p>We can write $\ket{\Psi{(t)}} = \sum_j \alpha_j(t) \ket{E_j}$.</p><p>Thus, $\ket{\dot\Psi{(t)}} = \sum_j \dot\alpha_j(t) \ket{E_j}$.</p><p>Using time-dependent Schrodinger&rsquo;s equation, we can solve for $\dot\alpha_j(t)$. It has the form $\dot\alpha_j(t) = -\frac{i}{\hbar}E_j \alpha_j(t)$. Which has the solution $\alpha_j(t) = \alpha_j(0) e^{\frac{-i}{\hbar} E_j t}$.</p><p>But, $\alpha_j(0) = \braket{E_j|\Psi(0)}$.</p><p>So, $\Psi(t) = \sum_j \ket{E_j}\braket{E_j|\Psi(0)} e^{-\frac{i}{\hbar}E_j t}$.</p><h4 id=general-recipe>General Recipe<a href=#general-recipe class=hanchor arialabel=Anchor>&#8983;</a></h4><ol><li>Get the H somehow.</li><li>Find the $\ket{E_j}$ and $E_j$.</li><li>Prepare the system in initial state $\Psi(0)$.</li><li>Use above solution to find the state at any later time.</li><li>If you have some other operator L, you can &ldquo;predict&rdquo; the outcome of measuring future state using the eigenvectors of L. i.e., the probability the outcome of measuring L is $\lambda_i$ is precisely $|\braket{\lambda_i|\Psi(t)}|^2$.</li></ol><h2 id=complex-systems>Complex systems<a href=#complex-systems class=hanchor arialabel=Anchor>&#8983;</a></h2><p>There are systems for which there are more than two observables, and they all can be measured simultaneously. Single spin is <strong>not</strong> such system. Though there are three observables $\sigma_{\{x,y,z\}}$. Only one of them can be observed at a time.</p><p>Particle moving in three dimensions, all three dimensions x, y, and z can be measured simultaneously.</p><p>Other such system is a system of two particles. Let&rsquo;s denote these two observables as L and M.
If we measure both spins, we leave the system in a state which is simultaneously eigenvector of both L and M. Let the eigenvectors of L be denoted as $\lambda_i$ and same for M with $\mu_a$.</p><p>In order to have simultaneous eigenvectors $\ket{\lambda, \mu}$, L and M must commute with each other. i.e., $LM \ket{\lambda, \mu} = ML\ket{\lambda, \mu}$. i.e., $[L, M]\ket{\lambda, \mu} = 0$. Since this statement is true of any basis eigenvector $\ket{\lambda, \mu}$, we know that $[L, M] = 0$.</p><p>Thus, if we have complete basis of simultaneous eigenvectors, the observables commute. Converse is also true. If observables commute, they have a complete basis of simultaneous eigenvectors. This statement is true for any number of observables in the system, and not just two.</p><p>Suppose, we have a basis of states $\ket{a,b,c, \dots}$ for some complex system.</p><p>$$\ket{\Psi} = \sum_{a,b,c,\dots} \psi{(a,b,c, \dots)} \ket{a,b,c, \dots}$$</p><p>Where, $\psi{(a,b,c, \dots)} = \braket{a,b,c, \dots|\Psi}$. This $\psi$ is called a wave function.</p><p>$P(a,b,c, \dots) = \psi^\ast{(a,b,c, \dots)} \psi{(a,b,c, \dots)}$. This is the probability of observing the value $a,b,c,\dots$ of commuting observables $A,B,C,\dots$.</p><p>If a state is an eigenvector of operator A, which does not commute with B, then it will not be an eigenvector for B. Thus, if A and B don&rsquo;t commute, there will be uncertainity in either A or B if not both.</p><h3 id=quantifying-uncertainty>Quantifying Uncertainty<a href=#quantifying-uncertainty class=hanchor arialabel=Anchor>&#8983;</a></h3><p>The system is in state $\ket\Psi$. We want to observe some A. A can be thought as a random variable. The possible outcomes are eigenvalues of A, with predefined probabilities. Thus $\braket{A}$ is well defined, and is the expected value of A. We can also find the variance of A! This quantifies the uncertainty of A.</p><p>First define $\bar{A} = A - \braket{A}I$. This is also an Hermitian operator, so has real eigenvalues. If $a$ is an eigenvalue of A, $\bar{a} = a - \braket{A}$ is an eigenvalue of $\bar{A}$.</p><p>Thus the variance, denoted as $(\Delta A)^2 = \sum_a \bar{a}^2 P(a)$.</p><p>A little bit of algebra shows that this is equivalent to $\braket{\bar{A}^2} = \braket{\Psi|\bar{A}^2|\Psi}$.</p><h3 id=uncertainty-principle>Uncertainty Principle<a href=#uncertainty-principle class=hanchor arialabel=Anchor>&#8983;</a></h3><p>From triangle inequality $|x| + |y| \ge |x+y|$, one can derive $2|x||y| \ge |\braket{x|y} + \braket{y|x}|$. (Hint: Square both sides, and expand).</p><p>Putting, $x = A\ket\Psi$ and $y = iB\ket\Psi$, we get $\Delta{A} \Delta{B} \ge \frac{1}{2} | \braket{\Psi|[A, B]|\Psi}$. (Hint: Though formula is true in general, it is easy to derive by assuming that $\braket{A} = \braket{B} = 0$, in which case $(\Delta{A})^2 = \braket{A^2}$.</p><h2 id=composite-system>Composite System<a href=#composite-system class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Atom is made of nucleons and electrons, each is a quantum system of it&rsquo;s own. We want to study composite system.</p><p>For the moment we assume there are two systems, A and B (a.k.a. Alices&rsquo;s sytem and Bob&rsquo;s sytem, respectively).
A has states from staet space $S_A$, which has dimensionality of $N_A$. Same for system B.</p><p>The combined system has statespace $S = S_A \otimes S_B$, which has dimensionality $N_A N_B$. $\otimes$ is a tensor product. The states of S are denoted as $\ket{ab}$, where $a \in S_A$ and $b \in S_B$.</p><p>If an operator M acts on the states of the composite system, M has $N_A N_B$ rows and colums. The entry $M_{a&rsquo;b&rsquo;, ab}$ is given by $\braket{a&rsquo;b&rsquo;|M|ab}$.</p><p>If A, and B are 2x2 matrices, their product is defined as
$$\left[\begin{matrix}A_{11} B_{11} & A_{11} B_{12} & A_{12} B_{11} & A_{12} B_{12}\\ A_{11} B_{21} & A_{11} B_{22} & A_{12} B_{21} & A_{12} B_{22}\\ A_{21} B_{11} & A_{21} B_{12} & A_{22} B_{11} & A_{22} B_{12}\\ A_{21} B_{21} & A_{21} B_{22} & A_{22} B_{21} & A_{22} B_{22}\end{matrix}\right]
$$</p><p>The basis vectors $\ket{ab}$ are taken to be orthonormal. $\braket{ab|a&rsquo;b&rsquo;} = [a = a&rsquo;, b = b&rsquo;]$.</p><p>Thus, any state in the composite system is written as $\ket\Psi = \sum_{a,b} \psi(a, b) \ket{ab}$.</p><h3 id=two-spin-system>Two Spin System<a href=#two-spin-system class=hanchor arialabel=Anchor>&#8983;</a></h3><p>Let&rsquo;s have four state vectors $\ket{uu}, \ket{ud}, \ket{du}, \ket{dd}$.</p><h4 id=product-state>Product State<a href=#product-state class=hanchor arialabel=Anchor>&#8983;</a></h4><p>A product state is the result of completely independent preparations of two spins, each with its&rsquo; own apparatus to prepare a spin. If the state is not a product state than it is entangled.</p><p>Suppose spin A is prepared in $\alpha_u \ket{u} + \alpha_d \ket{d}$ and B is prepared in $\beta_u \ket{u} + \beta_d \ket{d}$ Where $\alpha^\ast_u \alpha_u + \alpha^\ast_d \alpha_d = \beta^\ast_u \beta_u + \beta^\ast_d \beta_d = 1$. In such case we need 4 real parameters to define the state (2 for each spin).</p><p>Thus, the combined product state is written as $\{\alpha_u \ket{u} + \alpha_d \ket{d} \} \otimes \{ \beta_u \ket{u} + \beta_d \ket{d} \}$. This can be expanded, to rewrite in the basis vectors of the combined system, as $\alpha_u \beta_u \ket{uu} + \alpha_u \beta_d \ket{ud} + \alpha_d \beta_u \ket{du} + \alpha_d \beta_d \ket{dd}$.</p><p>The main feature of a product state is that each subsystem behaves independently of the other.</p><h4 id=general-tensor-product-space>General Tensor product space.<a href=#general-tensor-product-space class=hanchor arialabel=Anchor>&#8983;</a></h4><p>The general space is written as $\psi_{uu} \ket{uu} + \psi_{ud} \ket{ud} + \psi_{du} \ket{du} + \psi_{dd} \ket{dd}$. Unlike product space, we need 6 real parameters (8(to define complex numbers) - 1 (normalization) - 1(phase factor)).</p><p>Thus, there has to be some states which can not be written as product state. Singlet, and triplets are such states.</p><p>Singlet: $\ket{sing} = \frac{1}{\sqrt{2}} (\ket{ud} - \ket{du})$.</p><p>Triplets:</p><ol><li>$T_1 = \frac{1}{\sqrt{2}} (\ket{ud} + \ket{du})$.</li><li>$T_2 = \frac{1}{\sqrt{2}} (\ket{uu} + \ket{dd})$.</li><li>$T_3 = \frac{1}{\sqrt{2}} (\ket{uu} - \ket{dd})$.</li></ol><p>It is easy to prove that these states can&rsquo;t be written as product states. (Hint: for product state you have to show that $\alpha_x \beta_y = c$, but this will be impossible as some other requirement $\alpha_x \beta_w = 0$ or $\alpha_z \beta_y = 0$ will be violated.)</p><p>These states are maximally entangled.</p><h3 id=observables-in-combined-system>Observables in combined system.<a href=#observables-in-combined-system class=hanchor arialabel=Anchor>&#8983;</a></h3><p>The observables in the case of single sping system, still apply in the combined system. They just modify their half.</p><p>E.g. if $\sigma_x$ is observable of spin A, $\sigma_x \ket{ud} = \ket{dd}$.</p><p>Note that $\sigma_x$ for single spin system and $\sigma_x$ for combined systems are different operators. This is easy to see in the matrix forms. The two matrices are different.</p><p>In the case of single spin system (or product state), there is a direction where the measurement is guaranteed to be +1. Thus, $\braket{\vec{\sigma} \cdot \vec{n}} = \braket{\Psi|\vec{\sigma} \cdot \vec{n}|\Psi} = 1$, for that particular $\vec{n}$. Thus we know that none of the $\braket{\sigma_x}, \braket{\sigma_y}, \braket{\sigma_z}$ can be zero(which would lead to contradiction).</p><p>On the other hand, if you compute $\braket{\sigma_x}$ for $\ket{sing}$, you will find it to be zero. In fact, for singlet state, $\braket{\sigma_x} = \braket{\sigma_y} = \braket{\sigma_z} = 0$. Since the components are zero on expectation, we are equaly likely to get +1 and -1 for any measurement. Thus, although the state is known, we don&rsquo;t know the outcome.</p><hr><p>The operators that act on the two separate factors commute with one another.</p><p>The operator $\tau_z \sigma_z$, is a product of two oprators. Little math tells us that $\ket{sing}$ is an eigenvector for $\tau_z \sigma_z$. $\tau_z \sigma_z \ket{sing} = -\ket{sing}$. This means that the product of observations is always -1. Thus both measurements always have opposite signs. In fact, $\ket{sing}$ is also an eigenvector for operators $\tau_x \sigma_x$ and $\tau_y \sigma_y$, with eigenvalue -1.</p><p>On the other hand if you try to compute $\braket{\tau_x \sigma_y}$, you will find that to be 0.</p><p>For the triplets:</p><table><thead><tr><th></th><th>$\ket{sing}$</th><th>$\ket{T_1}$</th><th>$\ket{T_2}$</th><th>$\ket{T_3}$</th></tr></thead><tbody><tr><td>$\braket{\sigma_z \tau_z}$</td><td>-1</td><td>-1</td><td>+1</td><td>+1</td></tr><tr><td>$\braket{\sigma_x \tau_x}$</td><td>-1</td><td>+1</td><td>+1</td><td>-1</td></tr><tr><td>$\braket{\sigma_y \tau_y}$</td><td>-1</td><td>+1</td><td>-1</td><td>+1</td></tr></tbody></table><p>Thus, singlet and triplets are eigenvalues for the product operators.</p><hr><p>$\vec{\sigma} \cdot \vec{\tau}$ is an observable that can not be measured by measuring two spins by individual apparatuses. How can we measure such thing? Some atoms have spins. When two of these atoms are close to each other (e.g., in a lattice), in some situations their Hamiltonian is proportional to $\vec{\sigma} \cdot \vec{\tau}$.</p><p>The four vectors above are eigenvectors for $\vec{\sigma} \cdot \vec{\tau}$. The singlet has eigenvalue -3 and each triplet has eigenvalue +1.</p><p>Comment: Generating singlet state is easy. Two spins prefer to be anti-aligned. Now bring these two spins together, they will radiate a photon and result in this state, as this state has low energy.</p><p>For product state it is easy to prove $\braket{AB} = \braket{A} \braket{B}$. Thus, we can see that $\braket{\sigma_w \tau_w}$ can not be product states. Because for them $\braket{\sigma_w} = 0$.</p><h3 id=outer-products>Outer Products<a href=#outer-products class=hanchor arialabel=Anchor>&#8983;</a></h3><p>$\ket{\Psi} \bra{\Phi}$ is an operator. Which can be applied to bras or kets.</p><p>$\ket{\Psi} \bra{\Phi} \ket{A} = \ket{\Psi} \braket{\Phi|A}$</p><p>Specificaly if $\ket\Psi$ is normalized, <strong>Projection Operator</strong> is defined as $\ket{\Psi}\bra{\Psi}$.</p><p>Properties of Projection Operators.</p><ul><li>They are Hermitian.</li><li>$\Psi$ is an eigenvector with eigenvalue +1.</li><li>All perpendicular vectors to $\Psi$ are eigenvectors with eigenvalue 0.</li><li>Square of projection operator is the projection operator itself. $(\ket\Psi \bra\Psi)^2 = \ket\Psi \bra\Psi$.</li><li>Trace($Tr(L) = \sum_i \braket{i|L|i})$ of projection operator is 1.</li><li>If we add all proj. operators for all basis vectors we get identity operator. $\sum_i \ket{i}\bra{i} = I$.</li><li>Expectation of any operator L, for state $\Psi$ is $Tr (\ket\Psi \bra\Psi L)$.</li></ul><h3 id=density-matrix>Density Matrix<a href=#density-matrix class=hanchor arialabel=Anchor>&#8983;</a></h3><p>Suppose we don&rsquo;t know the exact state of the system, but we know that it is either $\Psi$ of $\Phi$ with equal probability. Then, the expected value is $ \frac{\braket{\Psi|L|\Psi} + \braket{\Phi|L|\Phi}}{2}$. If we define a new operator $\rho = \frac{1}{2}\ket\Psi \bra\Psi + \frac{1}{2} \ket\Phi \bra\Phi$, the expected value can be computed as $Tr (\rho L)$.</p><p>The definition for $\rho$ is general, and holds for more than two states. On the other hand, if we knew exactly that the state is $\Psi$, the last property of the projection operator still applies. In any case, $\braket{L} = Tr (\rho L)$.</p><p>In a pure state, density matrix is just a projection operator. In a mixed state, it is a combination of multiple projection operators.</p><p>In the matrix form, for particular basis, $\rho_{aa&rsquo;} = \braket{a|\rho|a&rsquo;}$. In this basis, $\braket{L} = \sum_{a,a&rsquo;} \rho_{aa&rsquo;} L_{a&rsquo;a}$.</p><p>In classical system, two particles in a line for example, if we know state of the complete system (i.e., $x_1, x_2, p_1, p_2$), we know the state of the constituents (i.e., $x_1, p_1$ and $x_2, p_2$).
(Comment: This is classical pure state. But, sometimes we don&rsquo;t know the exact $x_1, x_2, p_1, p_2$, but have some distribution of $\rho(x_1, x_2, p_1, p_2)$. This is mixed state.)</p><p>This is not true in QM when there is entanglement. In such cases, even if the combined system can be in pure state $\psi(a,b)$, each of it&rsquo;s constituent states must be described by a mixed state (unlike classical setting).</p><p>Say we have the full knowledge of the combined system(A, B). That is we know $\psi(a, b)$ of the combined system. And we want to know what we can about A. Let&rsquo;s pick an operator L which only acts on A.</p><p>Here, $\braket{L} = \sum_{ab, a&rsquo;b&rsquo;} \psi^{\ast}{(a&rsquo;b&rsquo;)} L_{a&rsquo;b&rsquo;, ab} \psi{(ab)}$.</p><p>Using the rules of matrix construction(Kronecker Product), $L_{a&rsquo;b&rsquo;, ab} = L_{a&rsquo;a} \delta_{b&rsquo;b}$.</p><p>Thus, $$
\begin{align*}
\braket{L} &= \sum_{a, b, a&rsquo;} \psi^{\ast}{(a&rsquo;b)} L_{a&rsquo;a} \psi{(ab)} \\
&= \sum_{a, a&rsquo;} L_{a&rsquo;a} \sum_{b}\psi^{\ast}{(a&rsquo;b&rsquo;)} \psi{(ab)} \\
&= \sum_{a, a&rsquo;} \rho_{aa&rsquo;} L_{a&rsquo;a} \psi{(ab)} \\
\end{align*}
$$</p><p>Where, we defined $\rho_{aa&rsquo;} = \sum_{b}\psi^{\ast}{(a&rsquo;b)} \psi{(ab)}$.</p><p>Note the difference in the definition of the $\rho$s. If in the above equation, we had product state (i.e., non entangled state $\psi(a, b) = \psi(a) \phi(b)$), we will get $\rho_{aa&rsquo;} = \psi^{\ast}{(a&rsquo;)} \psi{(a)} \sum_{b}\phi^{\ast}{(b)} \phi{(b)} = \psi^{\ast}{(a&rsquo;)} \psi{(a)}$. Thus, we get our projection operator (i.e., pure state density matrix) back.</p><p>But in general, even if we had pure state for combined system ($\psi(a, b)$), we may not get pure state for constituent system.</p><p>For a single spin system, with pure state, the density matrix looks like below.</p><p>$$\left(\begin{matrix}
\alpha^\ast \alpha && \alpha \beta^\ast \\
\alpha^\ast \beta && \beta^\ast \beta
\end{matrix}\right)
$$</p><h4 id=properties-of-density-matrix>Properties of Density Matrix<a href=#properties-of-density-matrix class=hanchor arialabel=Anchor>&#8983;</a></h4><ol><li>Diagonal entries tell us the probability of the observation. That is, $P(a) = \rho_{aa}$. This is in line with traditional probability theory. The probability of observing eigenvalues for a, b is $P(a, b) = \psi^\ast(ab) \psi(ab)$. Marginalizing out b gives us $P(a) = \sum_b \psi^\ast(ab) \psi(ab)$. This is also easy to see in the density matrix for pure state shown above.</li><li>Density matrix is Hermitian.</li><li>Trace of density matrix is 1.</li><li>All Eigenvalues are between 0 and 1 inclusive. Since the trace is 1, if one eigenvalue is 1, the rest are 0.</li><li>For a <strong>pure</strong> state, $\rho^2 = \rho$ and $Tr(\rho^2) = 1$.</li><li>For a mixed or entangled state, $\rho^2 \ne \rho$ and $Tr(\rho^2) &lt; 1$.</li></ol><p>Proving 5 is easy. Simple algebra. But, I found proving 6 to be little bit difficult. So here is the proof.</p><details><summary>Proof for point 6</summary><p>We will first prove that $Tr(\rho^2) &lt; 1$.</p><p>Assume that our mixed state is $\sum_i p_i \ket{i} \bra{i}$.</p><p>$$
\begin{align*}
\rho^2 &= \sum_{i, j} p_i p_j \ket{i} \bra{i} \ket{j} \bra{j} \\
&= \sum_{i,j} p_i p_j \braket{i|j} \ket{i} \bra{j} \\
Tr(\rho^2) &= \sum_{i, j} p_i p_j \braket{i|j} Tr(\ket{i} \bra{j}) \text{ // linearity of trace} \\
&= \sum_{i, j} p_i p_j \braket{i|j} \sum_k (i_k j^\ast_k) \\
&= \sum_{i, j} p_i p_j \braket{i|j} \braket{j|i} \\
&= \sum_{i, j} p_i p_j |\braket{i|j}|^2 \\
&&lt; \sum_{i, j} p_i p_j \text{ //Cauchy Schwarz Inequalty} \\
&= \sum_i p_i \sum_j p_j \\
&= 1 \blacksquare
\end{align*}
$$</p><p>Now, $\rho^2 = \rho$ would imply that $Tr(\rho^2) = Tr(\rho) = 1$. Which contradicts what we just proved. So $\rho^2 \ne \rho$.</p><p>Some facts used in this proof are,</p><ul><li><a href=https://en.wikipedia.org/wiki/Trace_(linear_algebra)>Trace is a linear operator.</a></li><li><a href=https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality>Cauchy-Swartz Inequality</a>
$ |\braket{i|j}|^2 \le \braket{i|i} \braket{j|j}$ Moreover, equality holds only when both vectors are linearly dependent.</li></ul></details><details><summary>Examples of Density matrix (Exercise 7.8)</summary><ol><li>$\ket{\Psi_1} = \frac{1}{2}(\ket{uu} + \ket{ud} + \ket{du} + \ket{dd})$
Density matrix for system A</li></ol><p>$$
\left(\begin{matrix}
\frac{1}{2} && \frac{1}{2} \\
\frac{1}{2} && \frac{1}{2} \\
\end{matrix}\right)
$$</p><p>Density matrix for system B</p><p>$$
\left(\begin{matrix}
\frac{1}{2} && \frac{1}{2} \\
\frac{1}{2} && \frac{1}{2} \\
\end{matrix}\right)
$$</p><p>For both the systems $\rho^2 = \rho$. So is pure state.</p><ol start=2><li>$\ket{\Psi_2} = \frac{1}{\sqrt2}(\ket{uu} + \ket{dd}) $</li></ol><p>Density matrix for system A</p><p>$$
\left(\begin{matrix}
\frac{1}{2} && 0 \\
0 && \frac{1}{2} \\
\end{matrix}\right)
$$</p><p>Density matrix for system B</p><p>$$
\left(\begin{matrix}
\frac{1}{2} && 0 \\
0 && \frac{1}{2} \\
\end{matrix}\right)
$$</p><p>For both the systems $\rho^2 \ne \rho$. So is mixed state.</p><ol start=3><li>$\ket{\Psi_3} = \frac{1}{5}(3\ket{uu} + 4\ket{ud})$</li></ol><p>$$
\left(\begin{matrix}
1 && 0 \\
0 && 0 \\
\end{matrix}\right)
$$</p><p>Density matrix for system B</p><p>$$
\left(\begin{matrix}
\frac{9}{25} && \frac{12}{25} \\
\frac{12}{25} && \frac{16}{25} \\
\end{matrix}\right)
$$</p><p>For both the systems $\rho^2 = \rho$. So is pure state.</p></details><h3 id=test-for-entanglement>Test for entanglement<a href=#test-for-entanglement class=hanchor arialabel=Anchor>&#8983;</a></h3><p>We have a wave function $\psi(a, b)$, and we want to test if that state is entangled or not.</p><h4 id=correlation-test>Correlation Test<a href=#correlation-test class=hanchor arialabel=Anchor>&#8983;</a></h4><p>Say there are two observables A and B, for two systems. The correlation between them is defined as $C(A, B) = \braket{AB} - \braket{A} \braket{B}$.</p><p>If a system is any state for which $C(A, B) \ne 0$, the state is entangled.</p><h4 id=using-density-matrix>Using Density Matrix<a href=#using-density-matrix class=hanchor arialabel=Anchor>&#8983;</a></h4><p><strong>Theorem</strong>: For any product state, the density matrix of any constituent subsystem has exactly one non-zero eigenvalue (thus it has to be 1). Moreover, the eigenvector for this eigenvalue is the factor wave function for that subsystem.</p><p>E.g., if A&rsquo;s wave function is $\psi$, and B&rsquo;s wave function is $\phi$, such that $\psi(a, b) = \psi(a) \phi(b)$, $\psi$ is the eigenvector with eigenvalue 1 for A&rsquo;s density matrix.</p><p>Conversly, if there are at least two eigenvalues($\lambda_j > 0, \lambda_k > 0$ (and since the Trace = 1, they are less than 1), one can prove that $\rho^2 \ne \rho$. And since $\rho^2 = \rho$ is necessary and sufficient condition for product state, we know that the state must be entangled.</p><details><summary>Proof that at least two positive eigenvalues implies $\\rho^2 \\ne \\rho$.</summary>
Proof by contradiction.<p>Assume $\rho^2 = \rho$.</p><p>$$
\rho^2 - \rho = 0 = \sum_i (\lambda_i^2 - \lambda_i) \ket{\Psi_i} \bra{\Psi_i}
$$</p><p>Since, in particular $\lambda_k^2 - \lambda_k &lt; 0$ and $\Psi_i$ are eigenbasis,</p><p>$$
\begin{align*}
\braket{\Psi_k|0|\Psi_k} = 0 &= \sum_i (\lambda_i^2 - \lambda_i) \braket{\Psi_k|\Psi_i} \braket{\Psi_i|\Psi_k} \\
&= \lambda_k^2 - \lambda_k \\
&&lt; 0
\end{align*}
$$</p><p>Which is a contradiction.</p></details><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python></code></pre></div></div></div></div></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user"><span>Dhruv Patel</span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=https://dhruvpatel.dev/assets/main.js></script>
<script src=https://dhruvpatel.dev/assets/prism.js></script></div></body></html>