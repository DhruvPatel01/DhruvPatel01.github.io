<!doctype html><html lang=en><head><title>Can we do better than NumPy in special cases? :: Dhruv Patel</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Can we do better than NumPy by trading generality? Answer is yes, by using BLAS and C extension."><meta name=keywords content="numpy,python,blas,blis,openblas"><meta name=robots content="noodp"><link rel=canonical href=https://dhruvpatel.dev/posts/matrix_vector/><link rel=stylesheet href=https://dhruvpatel.dev/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://dhruvpatel.dev/css/code.min.4f0ccc8439f99bf7f7970298556b94011aabc1fcae743b6842fc3361a2da9ea3.css><link rel=stylesheet href=https://dhruvpatel.dev/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://dhruvpatel.dev/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://dhruvpatel.dev/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://dhruvpatel.dev/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://dhruvpatel.dev/css/main.min.15870410d15d02abd22fb5ef00996f65a00d04b3a7435e9f83831c7c2298de88.css><link rel=stylesheet href=https://dhruvpatel.dev/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css><link rel=stylesheet href=https://dhruvpatel.dev/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://dhruvpatel.dev/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css><link rel=stylesheet href=https://dhruvpatel.dev/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css><link rel=stylesheet href=https://dhruvpatel.dev/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css><link rel=stylesheet href=https://dhruvpatel.dev/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel="shortcut icon" href=https://dhruvpatel.dev/favicon.png><link rel=apple-touch-icon href=https://dhruvpatel.dev/apple-touch-icon.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Can we do better than NumPy in special cases?"><meta property="og:description" content="Can we do better than NumPy by trading generality? Answer is yes, by using BLAS and C extension."><meta property="og:url" content="https://dhruvpatel.dev/posts/matrix_vector/"><meta property="og:site_name" content="Dhruv Patel"><meta property="og:image" content="https://dhruvpatel.dev/"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2022-04-10 09:11:23 +0530 +0530"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css integrity=sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js integrity=sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4OPoNhVgjdGFwhvN02Ja crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Dhruv Patel</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/about/>About</a></li><li><a href=https://github.com/DhruvPatel01/>Github</a></li><li><a href=/notes>Notes</a></li><li><a href=/now>Now</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/about/>About</a></li><li><a href=https://github.com/DhruvPatel01/>Github</a></li><li><a href=/notes>Notes</a></li><li><a href=/now>Now</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://dhruvpatel.dev/posts/matrix_vector/>Can we do better than NumPy in special cases?</a></h1><div class=post-meta><time class=post-date>2022-04-10</time></div><span class=post-tags>#<a href=https://dhruvpatel.dev/tags/python/>python</a>&nbsp;
#<a href=https://dhruvpatel.dev/tags/hpc/>hpc</a>&nbsp;
#<a href=https://dhruvpatel.dev/tags/blas/>blas</a>&nbsp;</span><div class=post-content><div><h2 id=can-we>Can we?<a href=#can-we class=hanchor arialabel=Anchor>#</a></h2><p>Yes. This blog explains how I did. If you want to follow the whole code, you can download the source code from <a href=https://github.com/DhruvPatel01/notebooks/tree/main/High_Performance_Computing/mat_vec>GitHub repository.</a></p><h2 id=why-would-we>Why would we?<a href=#why-would-we class=hanchor arialabel=Anchor>#</a></h2><p>Good question. NumPy is great. It is fast. Beating it would be hard. Even if we beat it, will it be worth it? Probably not. You would quote Donald Knuth and say that premature optimization is the root of all evil. I am trying to justify this blog by asking What if this is not a premature optimization? I would have tried other optimizations, and now I would want to see if I can squeeze out anything else.</p><p>To quote Pavlo Andy, when money is involved, constants matter. Even if I can outperform NumPy by a measly 10%, it could be worth it at scale.</p><p>I will be focusing on a problem of matrix vector multiplication. This is not a toy problem. Many machine learning algorithms ultimately just boil down to computing cosine distance between candidate vectors with anchor vector, at least during inference. You can replace cosine with dot, if you store normalized vectors into your datastore. I will also assume that the number of rows in my matrix is &lt;1000. This is also a reasonable assumption. We might not want to wait to batch user requests if we don&rsquo;t want to sacrifice latencies.</p><h2 id=how-could-we>How could we?<a href=#how-could-we class=hanchor arialabel=Anchor>#</a></h2><p>So, here is what I am thinking. <code>np.dot</code> works, but it is general. It might need to check for continuity, data layout, etc. In production setting, I might know these variables a priory. What if I just skip these checks? Furthermore, I don&rsquo;t know what happens under the hood in NumPy. Sure, <code>np.show_config()</code> can tell me if NumPy was compiled with <code>BLAS</code> or not. But, does it use all the optimizations available when I actually make a call to <code>np.dot</code>?</p><p>I did try Numba. But, this is just a dot product. Nothing much complicated. Numba&rsquo;s performance was no good than NumPy.</p><p>Similarly, just lowering the computation to C is not going to beat it. Surely, NumPy does the same thing, but in a more sophisticated manner. I will have to use some <code>BLAS</code> implementation. I will report my numbers using <code>BLIS</code><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, though I have also tried OpenBLAS<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>, and the numbers were similar. I am picking BLIS over OpenBLAS, as I have some experience of using it in the course <a href=https://www.cs.utexas.edu/users/flame/laff/pfhp/>Programming for high performance</a> from the authors of BLIS, and I think for this simple matrix vector computation, both should be equally optimized. I am not using MKL as I am using AMD processor, and I have read reports that MKL does not perform well on AMD.</p><p>My NumPy installation showed OpenBLAS in <code>np.show_config()</code>. So this is a fair comparison.</p><h3 id=step-1-installing-blisor-openblas-or-something-else>Step 1: Installing BLIS(or OpenBLAS, or something else).<a href=#step-1-installing-blisor-openblas-or-something-else class=hanchor arialabel=Anchor>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/flame/blis.git
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cd blis
</span></span><span style=display:flex><span>./configure -t openmp -p ~/blis auto    
</span></span><span style=display:flex><span>make -j8
</span></span><span style=display:flex><span>make check -j8
</span></span><span style=display:flex><span>make install
</span></span></code></pre></div><p>This will install the header files and static library respectively into <code>~/blis/include/blis</code> and <code>~/blis/lib/libblis.a</code>. I will use these when I compile a wrapper around a BLAS <code>dgemv</code> call. <code>dgemv</code> stands for Generalized Matrix Vector multiplication. <code>d</code> prefix means that inputs are double precision floating points. For FP32 you would use <code>sgemv</code>.</p><details class=collapsable-code><summary title="Click to interact"><span class=collapsable-code__title>OpenBLAS Installation</span></summary><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/xianyi/OpenBLAS.git
</span></span><span style=display:flex><span>cd OpenBLAS
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>make PREFIX<span style=color:#f92672>=</span>/home/dhruv/OpenBLAS
</span></span><span style=display:flex><span>make PREFIX<span style=color:#f92672>=</span>/home/dhruv/OpenBLAS install</span></span></code></pre></div></details><h3 id=step-2-write-and-compile-a-wrapper-around-blas>Step 2: Write and compile a wrapper around BLAS.<a href=#step-2-write-and-compile-a-wrapper-around-blas class=hanchor arialabel=Anchor>#</a></h3><p>As you might have noticed, BLAS routines are also general (hence the g in gemv). The <code>bli_dgemv</code><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> computes $y := \beta y + \alpha * trans(A)* conjugate(x)$. Yup, we could conjugate a vector even if we are working with real numbers! I am not sure how it works here, I am not going to use transpose and conjugate features anyway. In my case, $\beta = 0, \alpha=1$.</p><p>The general signature of <code>bli_dgemv</code> is</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#66d9ef>void</span> <span style=color:#a6e22e>bli_dgemv</span>
</span></span><span style=display:flex><span>     (
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>trans_t</span> transa,
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>conj_t</span>  conjx,
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>dim_t</span>   m,
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>dim_t</span>   n,
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>double</span><span style=color:#f92672>*</span>  alpha,
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>double</span><span style=color:#f92672>*</span>  a, <span style=color:#66d9ef>inc_t</span> rsa, <span style=color:#66d9ef>inc_t</span> csa,
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>double</span><span style=color:#f92672>*</span>  x, <span style=color:#66d9ef>inc_t</span> incx,
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>double</span><span style=color:#f92672>*</span>  beta,
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>double</span><span style=color:#f92672>*</span>  y, <span style=color:#66d9ef>inc_t</span> incy
</span></span><span style=display:flex><span>     );
</span></span></code></pre></div><ul><li><code>transa</code> tells blis if we want to transpose A before the multiplication. I don&rsquo;t, so I will use <code>BLIS_NO_TRANSPOSE</code>.</li><li><code>conjx</code> tells blis if we want to conjugate x. I don&rsquo;t, so I will use <code>BLIS_NO_CONJUGATE</code>.</li><li>m is the number of rows of A.</li><li>n is the number of columns of A.</li><li>$\alpha, \beta$ were explained earlier.</li><li><code>double *a</code>, is a pointer to the matrix A. I will have stored the matrix in row major order, so <code>rsa</code>(row stride) will be <code>n</code> and <code>csa</code>(column stride) will be 1.</li><li><code>x</code> is a pointer to array where x, the vector we want to multiply A with, stays.</li><li><code>y</code> is a pointer to array where Ax will be saved.</li><li><code>incx</code> and <code>incy</code> will be 1 as I know I have continuous arrays <code>x</code> and <code>y</code>.</li></ul><details class=collapsable-code><summary title="Click to interact"><span class=collapsable-code__title>gemv_blis.c</span></summary><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&#34;blis.h&#34;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>void</span> <span style=color:#a6e22e>blis_gemv_raw</span>(<span style=color:#66d9ef>int</span> m, <span style=color:#66d9ef>int</span> n, <span style=color:#66d9ef>double</span> <span style=color:#f92672>*</span>c_matrix, <span style=color:#66d9ef>double</span> <span style=color:#f92672>*</span>x, <span style=color:#66d9ef>double</span> <span style=color:#f92672>*</span>y)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>double</span> one <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>double</span> zero <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>;
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>bli_dgemv</span>(BLIS_NO_TRANSPOSE, BLIS_NO_CONJUGATE, 
</span></span><span style=display:flex><span>              m, n, <span style=color:#f92672>&amp;</span>one, c_matrix, n, <span style=color:#ae81ff>1</span>, 
</span></span><span style=display:flex><span>              x, <span style=color:#ae81ff>1</span>, 
</span></span><span style=display:flex><span>              <span style=color:#f92672>&amp;</span>zero, 
</span></span><span style=display:flex><span>              y, <span style=color:#ae81ff>1</span>);
</span></span><span style=display:flex><span>}</span></span></code></pre></div></details><p>To compile <code>gemv_blis.c</code> into <code>gemv_blis.o</code>, I used the following command, adapted from the PfHP course I mentioned earlier.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>gcc -c -O3 -m64 -mavx2 -std<span style=color:#f92672>=</span>c99 -march<span style=color:#f92672>=</span>native <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>-fopenmp -D_POSIX_C_SOURCE<span style=color:#f92672>=</span>200809L -I/home/dhruv/blis/include/blis <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>gemv_blis.c
</span></span></code></pre></div><p>Note that though I am using <code>-fopenmp</code>, I will not use multithreading, as multithreading is not much useful when we work with small data (as mentioned in the introduction.) For this, I will set <code>OMP_NUM_THREADS=1</code>.</p><p>The installed NumPy also had detected <code>AVX2</code> support. So I will be doing a fair comparison. For those who do not know AVX2 or SIMD, SIMD stands for single instruction multiple data. SIMD is the reason why naive lowering to C would not have worked. When used properly, AVX2 instruction can execute floating points operations on 256 bits at a time. We can pack four fp64 numbers in 256 bits, so this would theoretically increase our performance by 4x. BLIS leverages AVX2.</p><p>I am not writing the details for OpenBLAS wrapper and process of compiling it. If you are interested, please see <code>Makefile</code> and <code>gemv_openblas.c</code> files in the GitHub repository accompanying this post<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</p><h3 id=step-3-writing-the-c-extension>Step 3: Writing the C extension.<a href=#step-3-writing-the-c-extension class=hanchor arialabel=Anchor>#</a></h3><p>We will need to call this wrapper function from Python. We can&rsquo;t do that directly, as the wrapper expects pointers to raw memory. One alternative is to use <code>ctypes</code>. I did try that, and converting NumPy arrays to appropriate pointers itself was taking more than the whole <code>np.dot</code>. Instead, I am opting for writing a C extension to Python. I won&rsquo;t be explaining the boilerplate code, please visit <a href=https://docs.python.org/3/extending/extending.html>this link</a> which explains the process.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style=background-color:#3c3d38><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span></span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code class=language-c data-lang=c><span style=display:flex><span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex;background-color:#3c3d38><span><span style=color:#66d9ef>void</span> <span style=color:#a6e22e>blis_gemv_raw</span>(<span style=color:#66d9ef>int</span> m, <span style=color:#66d9ef>int</span> n, <span style=color:#66d9ef>double</span> <span style=color:#f92672>*</span>c_matrix, <span style=color:#66d9ef>double</span> <span style=color:#f92672>*</span>x, <span style=color:#66d9ef>double</span> <span style=color:#f92672>*</span>y);
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>static</span> PyObject <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>blis_dgemv</span>(PyObject <span style=color:#f92672>*</span>self, PyObject <span style=color:#f92672>*</span>args)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>void</span>  <span style=color:#f92672>*</span>a;
</span></span><span style=display:flex><span>    Py_ssize_t asize;
</span></span><span style=display:flex><span>    Py_buffer in_buf, out_buf;
</span></span><span style=display:flex><span>    in_buf.buf <span style=color:#f92672>=</span> out_buf.buf <span style=color:#f92672>=</span> NULL;
</span></span><span style=display:flex><span>    in_buf.len <span style=color:#f92672>=</span> out_buf.len <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>PyArg_ParseTuple</span>(args, <span style=color:#e6db74>&#34;s#s*s*&#34;</span>, <span style=color:#f92672>&amp;</span>a, <span style=color:#f92672>&amp;</span>asize, <span style=color:#f92672>&amp;</span>in_buf, <span style=color:#f92672>&amp;</span>out_buf)) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (in_buf.buf) <span style=color:#a6e22e>PyBuffer_Release</span>(<span style=color:#f92672>&amp;</span>in_buf);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (out_buf.buf) <span style=color:#a6e22e>PyBuffer_Release</span>(<span style=color:#f92672>&amp;</span>out_buf);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> NULL;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> m, n;
</span></span><span style=display:flex><span>    m <span style=color:#f92672>=</span> out_buf.len<span style=color:#f92672>/</span><span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>double</span>);
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> in_buf.len<span style=color:#f92672>/</span><span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>double</span>);
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>blis_gemv_raw</span>(m, n, a, in_buf.buf, out_buf.buf);
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>	Py_RETURN_NONE;
</span></span><span style=display:flex><span>}
</span></span></code></pre></td></tr></table></div></div><p>In line 3, I declare the signature for <code>blis_gemv_raw</code> defined and now residing in <code>gemv_blis.o</code>. Next, I define a new function called <code>blis_gemv</code>. This function will be called from Python with three arguments, namely matrix A, vector x, and vector y. I&rsquo;m calling x as in_buf and y as out_buf. Once this parsing is successful, I compute m and n. m is computed using y and n is computed using x. Once I have all the arguments, I just call the wrapper around <code>bli_dgemv</code>.</p><h3 id=step-4-compiling-the-c-extension>Step 4: Compiling the C extension.<a href=#step-4-compiling-the-c-extension class=hanchor arialabel=Anchor>#</a></h3><p>Finally, we need to create a shared object file that can be imported from Python. This is easy. Following is the <code>setup.py</code> file.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> setuptools <span style=color:#f92672>import</span> setup, Extension
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>module1 <span style=color:#f92672>=</span> Extension(<span style=color:#e6db74>&#39;gemv&#39;</span>,
</span></span><span style=display:flex><span>                    sources<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;gemvmodule.c&#39;</span>],
</span></span><span style=display:flex><span>                    extra_objects<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;gemv_blis.o&#39;</span>, 
</span></span><span style=display:flex><span>                                   <span style=color:#e6db74>&#39;/home/dhruv/blis/lib/libblis.a&#39;</span>],
</span></span><span style=display:flex><span>                    libraries<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;m&#39;</span>, <span style=color:#e6db74>&#39;pthread&#39;</span>],
</span></span><span style=display:flex><span>                    extra_compile_args<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;-fopenmp&#39;</span>],
</span></span><span style=display:flex><span>                    extra_link_args<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;-fopenmp&#39;</span>, <span style=color:#e6db74>&#39;-m64&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>setup(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;gemv&#39;</span>, version<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;0.1&#39;</span>, ext_modules<span style=color:#f92672>=</span>[module1])
</span></span></code></pre></div><p>Generate the <code>gemv.so</code> (or something similar, depending upon your OS), using the following command.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python setup.py build_ext --inplace
</span></span></code></pre></div><p>Et, voila! Now we can just <code>import gemv</code> and call the <code>gemv.blis_dgemv</code> with three appropriately shaped ndarrays.</p><h2 id=benchmarking>Benchmarking<a href=#benchmarking class=hanchor arialabel=Anchor>#</a></h2><p>I ran both NumPy and BLIS based implementation on batches of [10, 50, 100, 500, 1000] for vector size of 128. Following are the results.</p><p><img src=/posts/matrix_vector/benchmark.png alt=benchmark_result.png></p><p>Following table shows percentage improvement over <code>np.dot</code>. This was computed as $\frac{\text{numpy\_time} - \text{my\_time}}{\text{numpy\_time}}$.</p><table><thead><tr><th>batch_size</th><th style=text-align:right>%improvement</th></tr></thead><tbody><tr><td>10</td><td style=text-align:right>25.75 %</td></tr><tr><td>50</td><td style=text-align:right>26.21 %</td></tr><tr><td>100</td><td style=text-align:right>24.07 %</td></tr><tr><td>500</td><td style=text-align:right>11.06 %</td></tr><tr><td>1000</td><td style=text-align:right>12.22 %</td></tr></tbody></table><h2 id=conclusion>Conclusion<a href=#conclusion class=hanchor arialabel=Anchor>#</a></h2><p>For all the batch sizes, our implementation performs better than NumPy. When the batch size is ≤ 100, the improvements are around 25%. As the batch size increases, the percentage improvement decreases. This is expected, as more time would be taken by the computation instead of the overhead. Still, 10% is good! You can run the benchmark on your computer by running <code>benchmark.py</code> from the GitHub repository.</p><p>However, call me greedy if you want, but maybe there is still a scope for improvement? gemv is a general implementation. Maybe just for the pure dot product, we can strip some code out? Worth experimenting. I&rsquo;ll write a new blog if I do that. Meanwhile, if you have any suggestions, or comments, please drop me an email at <code>hello@dxxxxxxxxl.dev</code>.</p><h2 id=references>References<a href=#references class=hanchor arialabel=Anchor>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://github.com/flame/blis/>https://github.com/flame/blis/</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://www.openblas.net/>https://www.openblas.net/</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://github.com/flame/blis/blob/master/docs/BLISTypedAPI.md#gemv>https://github.com/flame/blis/blob/master/docs/BLISTypedAPI.md#gemv</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p><a href=https://github.com/DhruvPatel01/notebooks/tree/main/High_Performance_Computing/mat_vec>https://github.com/DhruvPatel01/notebooks/tree/main/High_Performance_Computing/mat_vec</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://dhruvpatel.dev/posts/gemma3_tokenizer/ class="button inline prev">&lt; [<span class=button__text>(Re)building Gemma tokenizer in Python</span>]
</a>::
<a href=https://dhruvpatel.dev/posts/aoc/21/day7/ class="button inline next">[<span class=button__text>Why the solution to part 2(AoC21-Day7) works?</span>] ></a></div></div></article></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user"><span>Dhruv Patel</span>
<span>:: <a href=https://github.com/panr/hugo-theme-terminal target=_blank>Theme</a> made by <a href=https://github.com/panr target=_blank>panr</a></span></div></div></footer><script type=text/javascript src=/bundle.min.js></script></div></body></html>