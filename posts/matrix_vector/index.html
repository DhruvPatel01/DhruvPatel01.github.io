<!doctype html><html lang=en><head><title>Can we do better than NumPy in special cases? :: Dhruv Patel</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Can we do better than NumPy by trading generality? Answer is yes, by using BLAS and C extension."><meta name=keywords content="numpy,python,blas,blis,openblas"><meta name=robots content="noodp"><link rel=canonical href=https://DhruvPatel01.github.io/posts/matrix_vector/><link rel=stylesheet href=https://DhruvPatel01.github.io/assets/style.css><link rel=apple-touch-icon href=https://DhruvPatel01.github.io/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=https://DhruvPatel01.github.io/img/favicon/orange.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Can we do better than NumPy in special cases?"><meta property="og:description" content="Can we do better than NumPy by trading generality? Answer is yes, by using BLAS and C extension."><meta property="og:url" content="https://DhruvPatel01.github.io/posts/matrix_vector/"><meta property="og:site_name" content="Dhruv Patel"><meta property="og:image" content="https://DhruvPatel01.github.io"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2022-04-10 09:11:23 +0530 +0530"></head><body class=orange><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Dhruv Patel</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about/>About</a></li><li><a href=https://github.com/DhruvPatel01/>Github</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about/>About</a></li><li><a href=https://github.com/DhruvPatel01/>Github</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=https://DhruvPatel01.github.io/posts/matrix_vector/>Can we do better than NumPy in special cases?</a></h1><div class=post-meta><span class=post-date>2022-04-10</span></div><span class=post-tags>#<a href=https://DhruvPatel01.github.io/tags/python/>python</a>&nbsp;
#<a href=https://DhruvPatel01.github.io/tags/hpc/>hpc</a>&nbsp;
#<a href=https://DhruvPatel01.github.io/tags/blas/>blas</a>&nbsp;</span><div class=post-content><div><h2 id=can-we>Can we?<a href=#can-we class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Yes. This blog explains how I did. If you want to follow the whole code, you can download the source code from <a href=https://github.com/DhruvPatel01/notebooks/tree/main/High_Performance_Computing/mat_vec>GitHub repository.</a></p><h2 id=why-would-we>Why would we?<a href=#why-would-we class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Good question. NumPy is great. It is fast. Beating it would be hard. Even if we beat it, will it be worth it? Probably not. You would quote Donald Knuth and say that premature optimization is the root of all evil. I am trying to justify this blog by asking What if this is not a premature optimization? I would have tried other optimizations, and now I would want to see if I can squeeze out anything else.</p><p>To quote Pavlo Andy, when money is involved, constants matter. Even if I can outperform NumPy by a measly 10%, it could be worth it at scale.</p><p>I will be focusing on a problem of matrix vector multiplication. This is not a toy problem. Many machine learning algorithms ultimately just boil down to computing cosine distance between candidate vectors with anchor vector, at least during inference. You can replace cosine with dot, if you store normalized vectors into your datastore. I will also assume that the number of rows in my matrix is &lt;1000. This is also a reasonable assumption. We might not want to wait to batch user requests if we don&rsquo;t want to sacrifice latencies.</p><h2 id=how-could-we>How could we?<a href=#how-could-we class=hanchor arialabel=Anchor>&#8983;</a></h2><p>So, here is what I am thinking. <code>np.dot</code> works, but it is general. It might need to check for continuity, data layout, etc. In production setting, I might know these variables a priory. What if I just skip these checks? Furthermore, I don&rsquo;t know what happens under the hood in NumPy. Sure, <code>np.show_config()</code> can tell me if NumPy was compiled with <code>BLAS</code> or not. But, does it use all the optimizations available when I actually make a call to <code>np.dot</code>?</p><p>I did try Numba. But, this is just a dot product. Nothing much complicated. Numba&rsquo;s performance was no good than NumPy.</p><p>Similarly, just lowering the computation to C is not going to beat it. Surely, NumPy does the same thing, but in a more sophisticated manner. I will have to use some <code>BLAS</code> implementation. I will report my numbers using <code>BLIS</code><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, though I have also tried OpenBLAS<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>, and the numbers were similar. I am picking BLIS over OpenBLAS, as I have some experience of using it in the course <a href=https://www.cs.utexas.edu/users/flame/laff/pfhp/>Programming for high performance</a> from the authors of BLIS, and I think for this simple matrix vector computation, both should be equally optimized. I am not using MKL as I am using AMD processor, and I have read reports that MKL does not perform well on AMD.</p><p>My NumPy installation showed OpenBLAS in <code>np.show_config()</code>. So this is a fair comparison.</p><h3 id=step-1-installing-blisor-openblas-or-something-else>Step 1: Installing BLIS(or OpenBLAS, or something else).<a href=#step-1-installing-blisor-openblas-or-something-else class=hanchor arialabel=Anchor>&#8983;</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/flame/blis.git
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cd blis
</span></span><span style=display:flex><span>./configure -t openmp -p ~/blis auto    
</span></span><span style=display:flex><span>make -j8
</span></span><span style=display:flex><span>make check -j8
</span></span><span style=display:flex><span>make install
</span></span></code></pre></div><p>This will install the header files and static library respectively into <code>~/blis/include/blis</code> and <code>~/blis/lib/libblis.a</code>. I will use these when I compile a wrapper around a BLAS <code>dgemv</code> call. <code>dgemv</code> stands for Generalized Matrix Vector multiplication. <code>d</code> prefix means that inputs are double precision floating points. For FP32 you would use <code>sgemv</code>.</p><div class=collapsable-code><input id=278416593 type=checkbox checked>
<label for=278416593><span class=collapsable-code__language>bash</span>
<span class=collapsable-code__title>OpenBLAS Installation</span>
<span class=collapsable-code__toggle data-label-expand=△ data-label-collapse=▽></span></label><pre class=language-bash><code>
git clone https://github.com/xianyi/OpenBLAS.git
cd OpenBLAS

make PREFIX=/home/dhruv/OpenBLAS
make PREFIX=/home/dhruv/OpenBLAS install

</code></pre></div><h3 id=step-2-write-and-compile-a-wrapper-around-blas>Step 2: Write and compile a wrapper around BLAS.<a href=#step-2-write-and-compile-a-wrapper-around-blas class=hanchor arialabel=Anchor>&#8983;</a></h3><p>As you might have noticed, BLAS routines are also general (hence the g in gemv). The <code>bli_dgemv</code><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> computes $y := \beta y + \alpha * trans(A)* conjugate(x)$. Yup, we could conjugate a vector even if we are working with real numbers! I am not sure how it works here, I am not going to use transpose and conjugate features anyway. In my case, $\beta = 0, \alpha=1$.</p><p>The general signature of <code>bli_dgemv</code> is</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#66d9ef>void</span> <span style=color:#a6e22e>bli_dgemv</span>
</span></span><span style=display:flex><span>     (
</span></span><span style=display:flex><span>       trans_t transa,
</span></span><span style=display:flex><span>       conj_t  conjx,
</span></span><span style=display:flex><span>       dim_t   m,
</span></span><span style=display:flex><span>       dim_t   n,
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>double</span><span style=color:#f92672>*</span>  alpha,
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>double</span><span style=color:#f92672>*</span>  a, inc_t rsa, inc_t csa,
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>double</span><span style=color:#f92672>*</span>  x, inc_t incx,
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>double</span><span style=color:#f92672>*</span>  beta,
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>double</span><span style=color:#f92672>*</span>  y, inc_t incy
</span></span><span style=display:flex><span>     );
</span></span></code></pre></div><ul><li><code>transa</code> tells blis if we want to transpose A before the multiplication. I don&rsquo;t, so I will use <code>BLIS_NO_TRANSPOSE</code>.</li><li><code>conjx</code> tells blis if we want to conjugate x. I don&rsquo;t, so I will use <code>BLIS_NO_CONJUGATE</code>.</li><li>m is the number of rows of A.</li><li>n is the number of columns of A.</li><li>$\alpha, \beta$ were explained earlier.</li><li><code>double *a</code>, is a pointer to the matrix A. I will have stored the matrix in row major order, so <code>rsa</code>(row stride) will be <code>n</code> and <code>csa</code>(column stride) will be 1.</li><li><code>x</code> is a pointer to array where x, the vector we want to multiply A with, stays.</li><li><code>y</code> is a pointer to array where Ax will be saved.</li><li><code>incx</code> and <code>incy</code> will be 1 as I know I have continuous arrays <code>x</code> and <code>y</code>.</li></ul><div class=collapsable-code><input id=832475196 type=checkbox>
<label for=832475196><span class=collapsable-code__language>c</span>
<span class=collapsable-code__title>gemv_blis.c</span>
<span class=collapsable-code__toggle data-label-expand=△ data-label-collapse=▽></span></label><pre class=language-c><code>
#include &#34;blis.h&#34;

void blis_gemv_raw(int m, int n, double *c_matrix, double *x, double *y)
{
    double one = 1.0;
    double zero = 0.0;
    bli_dgemv(BLIS_NO_TRANSPOSE, BLIS_NO_CONJUGATE, 
              m, n, &amp;one, c_matrix, n, 1, 
              x, 1, 
              &amp;zero, 
              y, 1);
}
</code></pre></div><p>To compile <code>gemv_blis.c</code> into <code>gemv_blis.o</code>, I used the following command, adapted from the PfHP course I mentioned earlier.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>gcc -c -O3 -m64 -mavx2 -std<span style=color:#f92672>=</span>c99 -march<span style=color:#f92672>=</span>native <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>-fopenmp -D_POSIX_C_SOURCE<span style=color:#f92672>=</span>200809L -I/home/dhruv/blis/include/blis <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>gemv_blis.c
</span></span></code></pre></div><p>Note that though I am using <code>-fopenmp</code>, I will not use multithreading, as multithreading is not much useful when we work with small data (as mentioned in the introduction.) For this, I will set <code>OMP_NUM_THREADS=1</code>.</p><p>The installed NumPy also had detected <code>AVX2</code> support. So I will be doing a fair comparison. For those who do not know AVX2 or SIMD, SIMD stands for single instruction multiple data. SIMD is the reason why naive lowering to C would not have worked. When used properly, AVX2 instruction can execute floating points operations on 256 bits at a time. We can pack four fp64 numbers in 256 bits, so this would theoretically increase our performance by 4x. BLIS leverages AVX2.</p><p>I am not writing the details for OpenBLAS wrapper and process of compiling it. If you are interested, please see <code>Makefile</code> and <code>gemv_openblas.c</code> files in the GitHub repository accompanying this post<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</p><h3 id=step-3-writing-the-c-extension>Step 3: Writing the C extension.<a href=#step-3-writing-the-c-extension class=hanchor arialabel=Anchor>&#8983;</a></h3><p>We will need to call this wrapper function from Python. We can&rsquo;t do that directly, as the wrapper expects pointers to raw memory. One alternative is to use <code>ctypes</code>. I did try that, and converting NumPy arrays to appropriate pointers itself was taking more than the whole <code>np.dot</code>. Instead, I am opting for writing a C extension to Python. I won&rsquo;t be explaining the boilerplate code, please visit <a href=https://docs.python.org/3/extending/extending.html>this link</a> which explains the process.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style=background-color:#3c3d38><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span></span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code class=language-c data-lang=c><span style=display:flex><span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex;background-color:#3c3d38><span><span style=color:#66d9ef>void</span> blis_gemv_raw(<span style=color:#66d9ef>int</span> m, <span style=color:#66d9ef>int</span> n, <span style=color:#66d9ef>double</span> <span style=color:#f92672>*</span>c_matrix, <span style=color:#66d9ef>double</span> <span style=color:#f92672>*</span>x, <span style=color:#66d9ef>double</span> <span style=color:#f92672>*</span>y);
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>static</span> PyObject <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span>blis_dgemv(PyObject <span style=color:#f92672>*</span>self, PyObject <span style=color:#f92672>*</span>args)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>void</span>  <span style=color:#f92672>*</span>a;
</span></span><span style=display:flex><span>    Py_ssize_t asize;
</span></span><span style=display:flex><span>    Py_buffer in_buf, out_buf;
</span></span><span style=display:flex><span>    in_buf.buf <span style=color:#f92672>=</span> out_buf.buf <span style=color:#f92672>=</span> NULL;
</span></span><span style=display:flex><span>    in_buf.len <span style=color:#f92672>=</span> out_buf.len <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>PyArg_ParseTuple(args, <span style=color:#e6db74>&#34;s#s*s*&#34;</span>, <span style=color:#f92672>&amp;</span>a, <span style=color:#f92672>&amp;</span>asize, <span style=color:#f92672>&amp;</span>in_buf, <span style=color:#f92672>&amp;</span>out_buf)) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (in_buf.buf) PyBuffer_Release(<span style=color:#f92672>&amp;</span>in_buf);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (out_buf.buf) PyBuffer_Release(<span style=color:#f92672>&amp;</span>out_buf);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> NULL;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> m, n;
</span></span><span style=display:flex><span>    m <span style=color:#f92672>=</span> out_buf.len<span style=color:#f92672>/</span><span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>double</span>);
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> in_buf.len<span style=color:#f92672>/</span><span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>double</span>);
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    blis_gemv_raw(m, n, a, in_buf.buf, out_buf.buf);
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>	Py_RETURN_NONE;
</span></span><span style=display:flex><span>}
</span></span></code></pre></td></tr></table></div></div><p>In line 3, I declare the signature for <code>blis_gemv_raw</code> defined and now residing in <code>gemv_blis.o</code>. Next, I define a new function called <code>blis_gemv</code>. This function will be called from Python with three arguments, namely matrix A, vector x, and vector y. I&rsquo;m calling x as in_buf and y as out_buf. Once this parsing is successful, I compute m and n. m is computed using y and n is computed using x. Once I have all the arguments, I just call the wrapper around <code>bli_dgemv</code>.</p><h3 id=step-4-compiling-the-c-extension>Step 4: Compiling the C extension.<a href=#step-4-compiling-the-c-extension class=hanchor arialabel=Anchor>&#8983;</a></h3><p>Finally, we need to create a shared object file that can be imported from Python. This is easy. Following is the <code>setup.py</code> file.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> setuptools <span style=color:#f92672>import</span> setup, Extension
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>module1 <span style=color:#f92672>=</span> Extension(<span style=color:#e6db74>&#39;gemv&#39;</span>,
</span></span><span style=display:flex><span>                    sources<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;gemvmodule.c&#39;</span>],
</span></span><span style=display:flex><span>                    extra_objects<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;gemv_blis.o&#39;</span>, 
</span></span><span style=display:flex><span>                                   <span style=color:#e6db74>&#39;/home/dhruv/blis/lib/libblis.a&#39;</span>],
</span></span><span style=display:flex><span>                    libraries<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;m&#39;</span>, <span style=color:#e6db74>&#39;pthread&#39;</span>],
</span></span><span style=display:flex><span>                    extra_compile_args<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;-fopenmp&#39;</span>],
</span></span><span style=display:flex><span>                    extra_link_args<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;-fopenmp&#39;</span>, <span style=color:#e6db74>&#39;-m64&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>setup(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;gemv&#39;</span>, version<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;0.1&#39;</span>, ext_modules<span style=color:#f92672>=</span>[module1])
</span></span></code></pre></div><p>Generate the <code>gemv.so</code> (or something similar, depending upon your OS), using the following command.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python setup.py build_ext --inplace
</span></span></code></pre></div><p>Et, voila! Now we can just <code>import gemv</code> and call the <code>gemv.blis_dgemv</code> with three appropriately shaped ndarrays.</p><h2 id=benchmarking>Benchmarking<a href=#benchmarking class=hanchor arialabel=Anchor>&#8983;</a></h2><p>I ran both NumPy and BLIS based implementation on batches of [10, 50, 100, 500, 1000] for vector size of 128. Following are the results.</p><p><img src=./benchmark.png alt=benchmark_result.png></p><p>Following table shows percentage improvement over <code>np.dot</code>. This was computed as $\frac{\text{numpy_time} - \text{numpy_time}}{\text{numpy_time}}$.</p><table><thead><tr><th>batch_size</th><th style=text-align:right>%improvement</th></tr></thead><tbody><tr><td>10</td><td style=text-align:right>25.75 %</td></tr><tr><td>50</td><td style=text-align:right>26.21 %</td></tr><tr><td>100</td><td style=text-align:right>24.07 %</td></tr><tr><td>500</td><td style=text-align:right>11.06 %</td></tr><tr><td>1000</td><td style=text-align:right>12.22 %</td></tr></tbody></table><h2 id=conclusion>Conclusion<a href=#conclusion class=hanchor arialabel=Anchor>&#8983;</a></h2><p>For all the batch sizes, our implementation performs better than NumPy. When the batch size is ≤ 100, the improvements are around 25%. As the batch size increases, the percentage improvement decreases. This is expected, as more time would be taken by the computation instead of the overhead. Still, 10% is good! You can run the benchmark on your computer by running <code>benchmark.py</code> from the GitHub repository.</p><p>However, call me greedy if you want, but maybe there is still a scope for improvement? gemv is a general implementation. Maybe just for the pure dot product, we can strip some code out? Worth experimenting. I&rsquo;ll write a new blog if I do that. Meanwhile, if you have any suggestions, or comments, please drop me an email at <code>hello@dxxxxxxxxl.dev</code>.</p><h2 id=references>References<a href=#references class=hanchor arialabel=Anchor>&#8983;</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://github.com/flame/blis/>https://github.com/flame/blis/</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://www.openblas.net/>https://www.openblas.net/</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://github.com/flame/blis/blob/master/docs/BLISTypedAPI.md#gemv>https://github.com/flame/blis/blob/master/docs/BLISTypedAPI.md#gemv</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p><a href=https://github.com/DhruvPatel01/notebooks/tree/main/High_Performance_Computing/mat_vec>https://github.com/DhruvPatel01/notebooks/tree/main/High_Performance_Computing/mat_vec</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button next"><a href=https://DhruvPatel01.github.io/posts/aoc/21/day7/><span class=button__text>Why the solution to part 2(AoC21-Day7) works?</span>
<span class=button__icon>→</span></a></span></div></div></div></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user"><span>Dhruv Patel</span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=https://DhruvPatel01.github.io/assets/main.js></script>
<script src=https://DhruvPatel01.github.io/assets/prism.js></script>
<script>window.MathJax={loader:{load:["[tex]/ams"]},tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,packages:{"[+]":["ams"]},ams:{multlineWidth:"100%",multlineIndent:"1em"}},svg:{fontCache:"global"}}</script></script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></div></body></html>