<!doctype html><html lang=en><head><title>(Re)building Gemma tokenizer in Python :: Dhruv Patel</title>
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content=" [!NOTE] This is not a tutorial type blog. Think of this as my notes as I was going through the building tokenizer phase.
First thing first, let&rsquo;s download the model files. I mean tokenizer files. But, they end with .model. These are serialized using ProtoBuf. You can find the specification of the file here. You can download the model file from here.
Gemma-3 tokenizer is different than Gemma-2. Algorithm remains same, vocabulary size is also more or less similar (256K vs ~262K). Within gemma3 variants, the same tokenizers are used.
"><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://dhruvpatel.dev/posts/gemma3_tokenizer/><link rel=stylesheet href=https://dhruvpatel.dev/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://dhruvpatel.dev/css/code.min.4f0ccc8439f99bf7f7970298556b94011aabc1fcae743b6842fc3361a2da9ea3.css><link rel=stylesheet href=https://dhruvpatel.dev/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://dhruvpatel.dev/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://dhruvpatel.dev/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://dhruvpatel.dev/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://dhruvpatel.dev/css/main.min.15870410d15d02abd22fb5ef00996f65a00d04b3a7435e9f83831c7c2298de88.css><link rel=stylesheet href=https://dhruvpatel.dev/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css><link rel=stylesheet href=https://dhruvpatel.dev/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://dhruvpatel.dev/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css><link rel=stylesheet href=https://dhruvpatel.dev/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css><link rel=stylesheet href=https://dhruvpatel.dev/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css><link rel=stylesheet href=https://dhruvpatel.dev/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel="shortcut icon" href=https://dhruvpatel.dev/favicon.png><link rel=apple-touch-icon href=https://dhruvpatel.dev/apple-touch-icon.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="(Re)building Gemma tokenizer in Python"><meta property="og:description" content=" [!NOTE] This is not a tutorial type blog. Think of this as my notes as I was going through the building tokenizer phase.
First thing first, let&rsquo;s download the model files. I mean tokenizer files. But, they end with .model. These are serialized using ProtoBuf. You can find the specification of the file here. You can download the model file from here.
Gemma-3 tokenizer is different than Gemma-2. Algorithm remains same, vocabulary size is also more or less similar (256K vs ~262K). Within gemma3 variants, the same tokenizers are used.
"><meta property="og:url" content="https://dhruvpatel.dev/posts/gemma3_tokenizer/"><meta property="og:site_name" content="Dhruv Patel"><meta property="og:image" content="https://dhruvpatel.dev/og-image.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2025-04-21 09:05:38 +0000 UTC"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css integrity=sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js integrity=sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4OPoNhVgjdGFwhvN02Ja crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Dhruv Patel</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/about/>About</a></li><li><a href=https://github.com/DhruvPatel01/>Github</a></li><li><a href=/notes>Notes</a></li><li><a href=/now>Now</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/about/>About</a></li><li><a href=https://github.com/DhruvPatel01/>Github</a></li><li><a href=/notes>Notes</a></li><li><a href=/now>Now</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://dhruvpatel.dev/posts/gemma3_tokenizer/>(Re)building Gemma tokenizer in Python</a></h1><div class=post-meta><time class=post-date>2025-04-21</time></div><div class=post-content><div><blockquote><p>[!NOTE] This is not a tutorial type blog. Think of this as my notes as I was going through the building tokenizer phase.</p></blockquote><p>First thing first, let&rsquo;s download the model files. I mean tokenizer files. But, they end with <code>.model</code>. These are serialized using ProtoBuf. You can find the specification of the file <a href=https://github.com/google/sentencepiece/blob/master/src/sentencepiece_model.proto>here</a>. You can download the model file from <a href=https://huggingface.co/google/gemma-3-27b-it/blob/main/tokenizer.model>here</a>.</p><p>Gemma-3 tokenizer is different than Gemma-2. Algorithm remains same, vocabulary size is also more or less similar (256K vs ~262K). Within gemma3 variants, the same tokenizers are used.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>md5sum <span style=color:#f92672>../</span>models<span style=color:#f92672>/</span>gemma<span style=color:#f92672>*.</span>model
</span></span></code></pre></div><p>f9e2445870ec741aa6346bbd75531bb4 ../models/gemma2.model
00d2276cbec4474f6cf3df98fbc18cbb ../models/gemma3-4b-it.model
00d2276cbec4474f6cf3df98fbc18cbb ../models/gemma3-27b-it.model
00d2276cbec4474f6cf3df98fbc18cbb ../models/gemma3.model</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> sentencepiece <span style=color:#66d9ef>as</span> sp
</span></span><span style=display:flex><span>g2 <span style=color:#f92672>=</span> sp<span style=color:#f92672>.</span>SentencePieceProcessor(model_file<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;../models/gemma2.model&#34;</span>)
</span></span><span style=display:flex><span>print(g2<span style=color:#f92672>.</span>vocab_size())
</span></span></code></pre></div><p>256000</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>g3 <span style=color:#f92672>=</span> sp<span style=color:#f92672>.</span>SentencePieceProcessor(model_file<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;../models/gemma3-4b-it.model&#34;</span>)
</span></span><span style=display:flex><span>print(g3<span style=color:#f92672>.</span>vocab_size())
</span></span></code></pre></div><p>262144</p><p>Let&rsquo;s have a list of toy strings to keep track how our tokenizer is doing as we progress.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>txt_emojis1 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;🫩 🥲 🩵&#34;</span>
</span></span><span style=display:flex><span>txt_emojis2 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;🗽⃢⃢🗿&#34;</span>
</span></span><span style=display:flex><span>txt_life <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;🤔➡️🌱🌎✨➡️🚶‍♂️🚶‍♀️❓➡️🌅🌄🌌➡️💭💡🧠➡️🤝❤️🤗➡️📚📜🏛️&#34;</span>
</span></span><span style=display:flex><span>txt_gibberish <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Hello&#39;ve world23 HOW&#39;s HOW&#39;S how&#39;s are yous?&#34;</span>
</span></span><span style=display:flex><span>txts <span style=color:#f92672>=</span> [txt_emojis1, txt_emojis2, txt_life, txt_gibberish]
</span></span></code></pre></div><h2 id=lets-play-with-the-binary-file>Let&rsquo;s play with the binary file.<a href=#lets-play-with-the-binary-file class=hanchor arialabel=Anchor>#</a></h2><p>You will need proto file I was talking about earlier. And also, you should install <code>protoc</code> <a href=https://protobuf.dev/installation/>https://protobuf.dev/installation/</a>. That will generate a file which you can use to read model files in Python.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>protoc <span style=color:#f92672>--</span>proto_path<span style=color:#f92672>=./</span> <span style=color:#f92672>--</span>python_out<span style=color:#f92672>=./</span> <span style=color:#f92672>./</span>sentencepiece_model<span style=color:#f92672>.</span>proto
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> sentencepiece_model_pb2 <span style=color:#66d9ef>as</span> model_pb
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>g3_pb <span style=color:#f92672>=</span> model_pb<span style=color:#f92672>.</span>ModelProto()
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;../models/gemma3-4b-it.model&#34;</span>, <span style=color:#e6db74>&#34;rb&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>    g3_pb<span style=color:#f92672>.</span>ParseFromString(f<span style=color:#f92672>.</span>read())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>g2_pb <span style=color:#f92672>=</span> model_pb<span style=color:#f92672>.</span>ModelProto()
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;../models/gemma2.model&#34;</span>, <span style=color:#e6db74>&#34;rb&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>    g2_pb<span style=color:#f92672>.</span>ParseFromString(f<span style=color:#f92672>.</span>read())
</span></span></code></pre></div><h2 id=control-tokens>Control Tokens<a href=#control-tokens class=hanchor arialabel=Anchor>#</a></h2><p>Let&rsquo;s see what control tokens we have.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> i, p <span style=color:#f92672>in</span> enumerate(g3_pb<span style=color:#f92672>.</span>pieces):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> p<span style=color:#f92672>.</span>type <span style=color:#f92672>==</span> <span style=color:#ae81ff>3</span>:
</span></span><span style=display:flex><span>        print(i, p<span style=color:#f92672>.</span>piece)
</span></span></code></pre></div><p>0 <pad>1 <eos>2 <bos></p><p>BTW, when you tokenize, you have to add bos yourself. <code>&lt;bos></code> is not tokenized. They talk about this in <a href=https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf>the technical report</a>. <code>&lt;bos></code> is not tokenized, because it is control token. Other special tokens, like <code>&lt;start_of_turn></code> automatically get tokenized correctly.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>g3<span style=color:#f92672>.</span>encode_as_pieces(<span style=color:#e6db74>&#34;&lt;bos&gt;Heyllo&#34;</span>, add_bos<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, add_eos<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>[&#39;&lt;bos&gt;&#39;, &#39;&lt;&#39;, &#39;bos&#39;, &#39;&gt;&#39;, &#39;Hey&#39;, &#39;llo&#39;, &#39;&lt;eos&gt;&#39;]
</span></span></code></pre></div><p>You see, <code>&lt;bos></code> got splitted into <code>&lt;</code>, <code>bos</code>, <code>></code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>g3<span style=color:#f92672>.</span>encode_as_pieces(<span style=color:#e6db74>&#34;&#34;&#34;&lt;start_of_turn&gt;user
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Who are you?&lt;end_of_turn&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;start_of_turn&gt;model&#34;&#34;&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>[&#39;&lt;start_of_turn&gt;&#39;,
</span></span><span style=display:flex><span> &#39;user&#39;,
</span></span><span style=display:flex><span> &#39;\n&#39;,
</span></span><span style=display:flex><span> &#39;Who&#39;,
</span></span><span style=display:flex><span> &#39;▁are&#39;,
</span></span><span style=display:flex><span> &#39;▁you&#39;,
</span></span><span style=display:flex><span> &#39;?&#39;,
</span></span><span style=display:flex><span> &#39;&lt;end_of_turn&gt;&#39;,
</span></span><span style=display:flex><span> &#39;\n&#39;,
</span></span><span style=display:flex><span> &#39;&lt;start_of_turn&gt;&#39;,
</span></span><span style=display:flex><span> &#39;model&#39;]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>piece_to_id <span style=color:#f92672>=</span> {p<span style=color:#f92672>.</span>piece: i <span style=color:#66d9ef>for</span> i,p <span style=color:#f92672>in</span> enumerate(g3_pb<span style=color:#f92672>.</span>pieces)}
</span></span><span style=display:flex><span>print(piece_to_id[<span style=color:#e6db74>&#34;&lt;start_of_turn&gt;&#34;</span>])
</span></span></code></pre></div><p>105</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>g3_pb<span style=color:#f92672>.</span>pieces[<span style=color:#ae81ff>105</span>]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>piece: &#34;&lt;start_of_turn&gt;&#34;
</span></span><span style=display:flex><span>score: 0
</span></span><span style=display:flex><span>type: USER_DEFINED
</span></span></code></pre></div><p>Why then did <code>&lt;start_of_turn></code> got tokenized properly? Because, it is something we call user defined token. Like when you pretrain the model, your corpus will probably not have such tokens. Or even if they have, they may not be frequent enough to get BPEd.</p><p>By the looks of it, Gemma3 has lot more user defined tokens than Gemma2. I am not sure why is this the case though. Though, I am curious, what do you gain by making it a control token instead of user defined token like <code>&lt;start_of_turn></code>.</p><p>Let&rsquo;s look at the protocol specification to understand this <code>type</code> better.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> collections <span style=color:#f92672>import</span> Counter
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Just copying some fields from proto specification of the model.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>NORMAL <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>;        <span style=color:#75715e># normal symbol</span>
</span></span><span style=display:flex><span>UNKNOWN <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>;       <span style=color:#75715e># unknown symbol. only &lt;unk&gt; for now.</span>
</span></span><span style=display:flex><span>CONTROL <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>;       <span style=color:#75715e># control symbols. &lt;/s&gt;, &lt;s&gt;, &lt;2ja&gt; etc.</span>
</span></span><span style=display:flex><span>USER_DEFINED <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>;  <span style=color:#75715e># user defined symbols.</span>
</span></span><span style=display:flex><span>                   <span style=color:#75715e># Typical usage of USER_DEFINED symbol</span>
</span></span><span style=display:flex><span>                   <span style=color:#75715e># is placeholder.</span>
</span></span><span style=display:flex><span>BYTE <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>;          <span style=color:#75715e># byte symbols. Used when `byte_fallback` is true.</span>
</span></span><span style=display:flex><span>UNUSED <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>;        <span style=color:#75715e># this piece is not used.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Counter([<span style=color:#66d9ef>None</span>, <span style=color:#e6db74>&#34;Normal&#34;</span>, <span style=color:#e6db74>&#34;Unknown&#34;</span>, <span style=color:#e6db74>&#34;Control&#34;</span>, <span style=color:#e6db74>&#34;User Defined&#34;</span>, <span style=color:#e6db74>&#34;Unused&#34;</span>, <span style=color:#e6db74>&#34;Byte&#34;</span>][p<span style=color:#f92672>.</span>type] <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> g3_pb<span style=color:#f92672>.</span>pieces)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Counter({&#39;Normal&#39;: 255474,
</span></span><span style=display:flex><span>         &#39;User Defined&#39;: 6410,
</span></span><span style=display:flex><span>         &#39;Byte&#39;: 256,
</span></span><span style=display:flex><span>         &#39;Control&#39;: 3,
</span></span><span style=display:flex><span>         &#39;Unknown&#39;: 1})
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Counter([<span style=color:#66d9ef>None</span>, <span style=color:#e6db74>&#34;Normal&#34;</span>, <span style=color:#e6db74>&#34;Unknown&#34;</span>, <span style=color:#e6db74>&#34;Control&#34;</span>, <span style=color:#e6db74>&#34;User Defined&#34;</span>, <span style=color:#e6db74>&#34;Unused&#34;</span>, <span style=color:#e6db74>&#34;Byte&#34;</span>][p<span style=color:#f92672>.</span>type] <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> g2_pb<span style=color:#f92672>.</span>pieces)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Counter({&#39;Normal&#39;: 255495,
</span></span><span style=display:flex><span>         &#39;Byte&#39;: 256,
</span></span><span style=display:flex><span>         &#39;User Defined&#39;: 245,
</span></span><span style=display:flex><span>         &#39;Control&#39;: 3,
</span></span><span style=display:flex><span>         &#39;Unknown&#39;: 1})
</span></span></code></pre></div><p>They have lots of &ldquo;unused&rdquo; tokens than Gemma2. I wonder why they added &ldquo;unused&rdquo; tokens, thousands of them, if they were unusable. Never the less, since it is user defined, it is treated specially, even if it was not in the training corpus.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>g3<span style=color:#f92672>.</span>encode_as_pieces(<span style=color:#e6db74>&#34;&lt;unused10&gt; blahblahblah&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>[&#39;&lt;unused10&gt;&#39;, &#39;▁blah&#39;, &#39;blah&#39;, &#39;blah&#39;]
</span></span></code></pre></div><p>Now enough of playing with their tokenizer.</p><h2 id=lets-build-tokenizer>Let&rsquo;s build tokenizer<a href=#lets-build-tokenizer class=hanchor arialabel=Anchor>#</a></h2><p>I&rsquo;ll assume you know little bit of BPE. If you don&rsquo;t, YouTube &ldquo;Andrej Karpathy Tokenizer&rdquo;. You won&rsquo;t be disappointed.</p><p>Speaking of assumptions, SentencePiece assumes that you don&rsquo;t use &ldquo;▁&rdquo; in your words. What SentencePiece does is it replaces spaces by this &ldquo;▁&rdquo;. Then it runs BPE (not actually Byte Pair, but unicode pairs, will come there, hold on.)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>g3<span style=color:#f92672>.</span>decode(g3<span style=color:#f92672>.</span>encode_as_pieces(<span style=color:#e6db74>&#34;He▁llo▁ World&#34;</span>)) <span style=color:#75715e># Yikes</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&#39;He llo  World&#39;
</span></span></code></pre></div><p>Anyway, after replacing space with &lsquo;▁&rsquo; (U+2581), SentencePiece finds all the tokens that are user defined. Thus <code>"&lt;start_of_turn>Hello▁World!▁🩵"</code> becomes <code>["&lt;start_of_turn>", "H", "e", "l", "l", "o", "▁", "W", "o", "r", "l", "d", "!", "🩵"]</code>. You see, unlike OpenAI&rsquo;s tiktokenizer, SentencePiece does not split &ldquo;🩵&rdquo; into <code>0xF0 0x9F 0xA9 0xB5</code>. Also, user defined tokens are frozen. They aren&rsquo;t allowed to form pairs with their neighbours. :(</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>user_defined_pieces <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i, p <span style=color:#f92672>in</span> enumerate(g3_pb<span style=color:#f92672>.</span>pieces):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> p<span style=color:#f92672>.</span>type <span style=color:#f92672>==</span> USER_DEFINED:
</span></span><span style=display:flex><span>        user_defined_pieces[p<span style=color:#f92672>.</span>piece] <span style=color:#f92672>=</span> i
</span></span></code></pre></div><p>So this is what their preprocessing code looks like. (Not really, mine is crappy and inefficient. But it&rsquo;s a start.)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>process</span>(txt):
</span></span><span style=display:flex><span>    txt <span style=color:#f92672>=</span> txt<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34; &#34;</span>, <span style=color:#e6db74>&#34;▁&#34;</span>)
</span></span><span style=display:flex><span>    new_txt <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> txt:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> user_piece <span style=color:#f92672>in</span> user_defined_pieces:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> txt<span style=color:#f92672>.</span>startswith(user_piece):
</span></span><span style=display:flex><span>                new_txt<span style=color:#f92672>.</span>append(user_piece)
</span></span><span style=display:flex><span>                txt <span style=color:#f92672>=</span> txt<span style=color:#f92672>.</span>removeprefix(user_piece)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            new_txt<span style=color:#f92672>.</span>append(txt[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>            txt <span style=color:#f92672>=</span> txt[<span style=color:#ae81ff>1</span>:]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> new_txt
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>txt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&lt;start_of_turn&gt;Hello World! &lt;unused0&gt; 🩵&lt;unused1&gt; - ધ્રુવ&#34;</span>
</span></span><span style=display:flex><span>process(txt)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>[&#39;&lt;start_of_turn&gt;&#39;,
</span></span><span style=display:flex><span> &#39;H&#39;,
</span></span><span style=display:flex><span> &#39;e&#39;,
</span></span><span style=display:flex><span> &#39;l&#39;,
</span></span><span style=display:flex><span> &#39;l&#39;,
</span></span><span style=display:flex><span> &#39;o&#39;,
</span></span><span style=display:flex><span> &#39;▁&#39;,
</span></span><span style=display:flex><span> &#39;W&#39;,
</span></span><span style=display:flex><span> &#39;o&#39;,
</span></span><span style=display:flex><span> &#39;r&#39;,
</span></span><span style=display:flex><span> &#39;l&#39;,
</span></span><span style=display:flex><span> &#39;d&#39;,
</span></span><span style=display:flex><span> &#39;!&#39;,
</span></span><span style=display:flex><span> &#39;▁&#39;,
</span></span><span style=display:flex><span> &#39;🩵&#39;,
</span></span><span style=display:flex><span> &#39;▁&#39;,
</span></span><span style=display:flex><span> &#39;-&#39;,
</span></span><span style=display:flex><span> &#39;▁&#39;,
</span></span><span style=display:flex><span> &#39;ધ&#39;,
</span></span><span style=display:flex><span> &#39;્&#39;,
</span></span><span style=display:flex><span> &#39;ર&#39;,
</span></span><span style=display:flex><span> &#39;ુ&#39;,
</span></span><span style=display:flex><span> &#39;વ&#39;]
</span></span></code></pre></div><p>Alright, next, we find the pairs which occur most frequently in the training corpus. In the protobuf, there is a field called score. The more the score, more important that piece is than others.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>piece_scores <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    p<span style=color:#f92672>.</span>piece: p<span style=color:#f92672>.</span>score <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> g3_pb<span style=color:#f92672>.</span>pieces
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_candidate_scores</span>(txt):
</span></span><span style=display:flex><span>    scores <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, len(txt)):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> txt[i] <span style=color:#f92672>in</span> user_defined_pieces <span style=color:#f92672>or</span> txt[i<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>] <span style=color:#f92672>in</span> user_defined_pieces:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        candidate <span style=color:#f92672>=</span> txt[i<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>] <span style=color:#f92672>+</span> txt[i]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> candidate <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> piece_scores: <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        scores[candidate] <span style=color:#f92672>=</span> piece_scores[candidate]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> scores
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>get_candidate_scores(process(<span style=color:#e6db74>&#34;&lt;start_of_turn&gt;Hello&#34;</span>))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>{&#39;He&#39;: -1715.0, &#39;el&#39;: -41.0, &#39;ll&#39;: -365.0, &#39;lo&#39;: -350.0}
</span></span></code></pre></div><p>Thus, we will merge <code>el</code>. Then we will repeat the process for <code>[&lt;start_of_turn> H el l o]</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>merge</span>(txt, to_merge):
</span></span><span style=display:flex><span>    new_txt <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    merged <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> i <span style=color:#f92672>&lt;</span> len(txt):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> txt[i] <span style=color:#f92672>in</span> user_defined_pieces:
</span></span><span style=display:flex><span>            new_txt<span style=color:#f92672>.</span>append(txt[i])
</span></span><span style=display:flex><span>            i <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> i <span style=color:#f92672>&lt;</span> len(txt)<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> txt[i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>] <span style=color:#f92672>in</span> user_defined_pieces:
</span></span><span style=display:flex><span>                new_txt<span style=color:#f92672>.</span>extend(txt[i:i<span style=color:#f92672>+</span><span style=color:#ae81ff>2</span>])
</span></span><span style=display:flex><span>                i <span style=color:#f92672>+=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                candidate <span style=color:#f92672>=</span> txt[i] <span style=color:#f92672>+</span> txt[i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> candidate <span style=color:#f92672>==</span> to_merge:
</span></span><span style=display:flex><span>                    merged <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>                    new_txt<span style=color:#f92672>.</span>append(to_merge)
</span></span><span style=display:flex><span>                    i <span style=color:#f92672>+=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                    new_txt<span style=color:#f92672>.</span>append(txt[i])
</span></span><span style=display:flex><span>                    i <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            new_txt<span style=color:#f92672>.</span>append(txt[i])
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> new_txt, merged
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>candidates <span style=color:#f92672>=</span> get_candidate_scores(process(<span style=color:#e6db74>&#34;Hello&#34;</span>))
</span></span><span style=display:flex><span>nxt <span style=color:#f92672>=</span> max(candidates, key<span style=color:#f92672>=</span>candidates<span style=color:#f92672>.</span>get)
</span></span><span style=display:flex><span>print(nxt)
</span></span></code></pre></div><p>el</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tokenize</span>(txt):
</span></span><span style=display:flex><span>    txt <span style=color:#f92672>=</span> process(txt)
</span></span><span style=display:flex><span>    merged <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> merged:
</span></span><span style=display:flex><span>        candidates <span style=color:#f92672>=</span> get_candidate_scores(txt)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> candidates: <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        to_merge <span style=color:#f92672>=</span> max(candidates, key<span style=color:#f92672>=</span>candidates<span style=color:#f92672>.</span>get)
</span></span><span style=display:flex><span>        txt, merged <span style=color:#f92672>=</span> merge(txt, to_merge)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> txt
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>txt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&lt;start_of_turn&gt; Hello World! How are you doing! - ધ્રુવ&#34;</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenize(txt) <span style=color:#f92672>==</span> g3<span style=color:#f92672>.</span>encode_as_pieces(txt)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>True
</span></span></code></pre></div><p>Yay!</p><p>or Nay?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenize(<span style=color:#e6db74>&#34;🫩&#34;</span>), g3<span style=color:#f92672>.</span>encode_as_pieces(<span style=color:#e6db74>&#34;🫩&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>([&#39;\U0001fae9&#39;], [&#39;&lt;0xF0&gt;&#39;, &#39;&lt;0x9F&gt;&#39;, &#39;&lt;0xAB&gt;&#39;, &#39;&lt;0xA9&gt;&#39;])
</span></span></code></pre></div><p>It&rsquo;s nay. Because, we forgot about byte fallback. You see, &ldquo;🫩&rdquo; was added to Unicode in December 2024. So, it is not widely used emoji. So, there is no integer id for that piece, as it is not a piece. In cases like this, byte fallback is used.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>post_process</span>(txt):
</span></span><span style=display:flex><span>    pieces <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> txt:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> p <span style=color:#f92672>in</span> user_defined_pieces <span style=color:#f92672>or</span> p <span style=color:#f92672>in</span> piece_scores:
</span></span><span style=display:flex><span>            pieces<span style=color:#f92672>.</span>append(p)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            pieces<span style=color:#f92672>.</span>extend(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;&lt;0x</span><span style=color:#e6db74>{</span>b<span style=color:#e6db74>:</span><span style=color:#e6db74>X</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&gt;&#34;</span> <span style=color:#66d9ef>for</span> b <span style=color:#f92672>in</span> p<span style=color:#f92672>.</span>encode())
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pieces
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tokenize_to_ids</span>(txt):
</span></span><span style=display:flex><span>    pieces <span style=color:#f92672>=</span> post_process(tokenize(txt))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [piece_to_id[p] <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> pieces]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> txt <span style=color:#f92672>in</span> txts:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> tokenize_to_ids(txt) <span style=color:#f92672>==</span> g3<span style=color:#f92672>.</span>tokenize(txt)
</span></span></code></pre></div><h1 id=efficient-implementation-ideas>Efficient Implementation Ideas<a href=#efficient-implementation-ideas class=hanchor arialabel=Anchor>#</a></h1><p>There are many things you can do to improve the performance. Our preprocess function is very very suboptimal. It&rsquo;s quadratic. For each user defined token, we check if our string has that prefix. If it is, we remove the prefix. A better data structure would be Trie. But, I won&rsquo;t use trie. I found state machine (which can be thought of compact trie) to be easier to write.</p><p>Let&rsquo;s do some rudimentary benchmarking.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%%</span>timeit
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenize(<span style=color:#e6db74>&#34;&#34;&#34;The given statement (which doesn&#39;t require quote marks) is run via the LineProfiler. Profiling is enabled for the functions specified by the -f options. The statistics will be shown side-by-side with the code through the pager once the statement has completed.&#34;&#34;&#34;</span>)
</span></span></code></pre></div><p>46.1 ms ± 1.26 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)</p><p>Got it. Let&rsquo;s check how much time is spent in preprocessing.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%</span>load_ext line_pr   ofiler
</span></span><span style=display:flex><span><span style=color:#f92672>%</span>lprun <span style=color:#f92672>-</span>f tokenize  tokenize(<span style=color:#e6db74>&#34;&#34;&#34;The given statement (which doesn&#39;t require quote marks) is run via the LineProfiler. Profiling is enabled for the functions specified by the -f options. The statistics will be shown side-by-side with the code through the pager once the statement has completed.&#34;&#34;&#34;</span>)
</span></span></code></pre></div><p>Timer unit: 1e-09 s</p><p>Total time: 0.148539 s
File: /tmp/ipykernel_22214/2407760563.py
Function: tokenize at line 1</p><h1 id=line-------hits---------time--per-hit----time--line-contents>Line # Hits Time Per Hit % Time Line Contents<a href=#line-------hits---------time--per-hit----time--line-contents class=hanchor arialabel=Anchor>#</a></h1><pre><code> 1                                           def tokenize(txt):
 2         1  131657030.0    1e+08     88.6      txt = process(txt)
 3         1        160.0    160.0      0.0      merged = True
 4       146      19612.0    134.3      0.0      while merged:
 5       146    7475920.0  51204.9      5.0          candidates = get_candidate_scores(txt)
 6       146      22430.0    153.6      0.0          if not candidates: break
 7       145     351242.0   2422.4      0.2          to_merge = max(candidates, key=candidates.get)
 8       145    9012017.0  62151.8      6.1          txt, merged = merge(txt, to_merge)
 9         1        779.0    779.0      0.0      return txt
</code></pre><p>Whoa. That is whopping 90%. Well I&rsquo;m not suprised. Let&rsquo;s try to optimize this using State Machine. The idea is, you start in start state. Then you read byte by byte (you can read codepoint by codepoint as well, they are the same). Each time you read a byte, you transition from one state to another. Each state is either valid or invalid. You always keep track of a last valid state.</p><p>For example, say you have strings &ldquo;abcd&rdquo; and &ldquo;ab&rdquo;. A state machine could look like below. <code>[a -> 1]</code> means if you read a, goto 1.</p><pre tabindex=0><code>state 0: start state [a -&gt; 1]
state 1: a (invalid) [b -&gt; 2]
state 2: ab (valid)  [c -&gt; 3]
state 3: abc (invalid) [d -&gt; 4]
state 4: abcd (valid)
</code></pre><p>if your string is &ldquo;babd&rdquo;, you would do something like below. Here <code>[0]</code> means you are in this state.</p><ul><li><code>[0]</code>. Read b. No transition. emit b is not a valid prefix.</li><li><code>[0]</code>. Read a. Goto 1.</li><li><code>[1]</code>. Read b. Goto 2.</li><li><code>[2]</code>. Read d. No valid transition. Since the last valid state was 2, we emit &ldquo;ab&rdquo;, and start from the next byte after &ldquo;ab&rdquo;.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>StateMachine</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, strings):
</span></span><span style=display:flex><span>        is_valid <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>        states <span style=color:#f92672>=</span> [[<span style=color:#66d9ef>None</span>]<span style=color:#f92672>*</span><span style=color:#ae81ff>256</span>]  <span style=color:#75715e># 0 is the initial state. None entry means no transition.</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> string <span style=color:#f92672>in</span> strings:
</span></span><span style=display:flex><span>            state <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> string<span style=color:#f92672>.</span>encode():
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> states[state][c] <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>: 
</span></span><span style=display:flex><span>                    <span style=color:#75715e># There is no state created for this prefix. Create a new state with empty transistions.</span>
</span></span><span style=display:flex><span>                    states<span style=color:#f92672>.</span>append([<span style=color:#66d9ef>None</span>]<span style=color:#f92672>*</span><span style=color:#ae81ff>256</span>)
</span></span><span style=display:flex><span>                    states[state][c] <span style=color:#f92672>=</span> len(states)<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>                state <span style=color:#f92672>=</span> states[state][c]
</span></span><span style=display:flex><span>            is_valid[state] <span style=color:#f92672>=</span> string <span style=color:#75715e># mark the final state as valid.</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>states <span style=color:#f92672>=</span> states
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>is_valid <span style=color:#f92672>=</span> is_valid
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>find_longest_prefix</span>(self, string, i):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;Finds the longest prefix of the string that is a valid prefix of any of the strings used to create the state machine.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        state <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        last_valid <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        last_i <span style=color:#f92672>=</span> i <span style=color:#75715e># keeps track of the last index where a valid prefix was found.</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> i <span style=color:#f92672>&lt;</span> len(string) <span style=color:#f92672>and</span> (state <span style=color:#f92672>:=</span> self<span style=color:#f92672>.</span>states[state][string[i]]):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> state <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>is_valid:
</span></span><span style=display:flex><span>                last_valid <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>is_valid[state]
</span></span><span style=display:flex><span>                last_i <span style=color:#f92672>=</span> i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>            i <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> last_valid, last_i
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sm <span style=color:#f92672>=</span> StateMachine([p<span style=color:#f92672>.</span>piece <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> g3_pb<span style=color:#f92672>.</span>pieces <span style=color:#66d9ef>if</span> p<span style=color:#f92672>.</span>type <span style=color:#f92672>==</span> <span style=color:#ae81ff>4</span>])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>len(sm<span style=color:#f92672>.</span>states)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>12951
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># using the first byte of the utf-8 coded character to determine the length of the character.</span>
</span></span><span style=display:flex><span>char_length <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>]<span style=color:#f92672>*</span><span style=color:#ae81ff>12</span> <span style=color:#f92672>+</span> [<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># I am overwriting the process function for &#34;educational purposes&#34; </span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>process</span>(input_str):
</span></span><span style=display:flex><span>    input_str <span style=color:#f92672>=</span> input_str<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34; &#34;</span>, <span style=color:#e6db74>&#34;▁&#34;</span>)<span style=color:#f92672>.</span>encode()
</span></span><span style=display:flex><span>    new_txt <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> i <span style=color:#f92672>&lt;</span> len(input_str):
</span></span><span style=display:flex><span>        found, i <span style=color:#f92672>=</span> sm<span style=color:#f92672>.</span>find_longest_prefix(input_str, i)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> found:
</span></span><span style=display:flex><span>            new_txt<span style=color:#f92672>.</span>append(found)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            n <span style=color:#f92672>=</span> char_length[(input_str[i] <span style=color:#f92672>&gt;&gt;</span> <span style=color:#ae81ff>4</span>)]
</span></span><span style=display:flex><span>            new_txt<span style=color:#f92672>.</span>append(input_str[i:i<span style=color:#f92672>+</span>n]<span style=color:#f92672>.</span>decode())
</span></span><span style=display:flex><span>            i <span style=color:#f92672>+=</span> n
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> new_txt
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> txt <span style=color:#f92672>in</span> txts:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> tokenize_to_ids(txt) <span style=color:#f92672>==</span> g3<span style=color:#f92672>.</span>tokenize(txt)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%%</span>timeit
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenize(<span style=color:#e6db74>&#34;&#34;&#34;The given statement (which doesn&#39;t require quote marks) is run via the LineProfiler. Profiling is enabled for the functions specified by the -f options. The statistics will be shown side-by-side with the code through the pager once the statement has completed.&#34;&#34;&#34;</span>)
</span></span></code></pre></div><p>8.06 ms ± 220 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%</span>lprun <span style=color:#f92672>-</span>f tokenize tokenize(<span style=color:#e6db74>&#34;&#34;&#34;The given statement (which doesn&#39;t require quote marks) is run via the LineProfiler. Profiling is enabled for the functions specified by the -f options. The statistics will be shown side-by-side with the code through the pager once the statement has completed.&#34;&#34;&#34;</span>)
</span></span></code></pre></div><p>Timer unit: 1e-09 s</p><p>Total time: 0.018039 s
File: /tmp/ipykernel_22214/2407760563.py
Function: tokenize at line 1</p><h1 id=line-------hits---------time--per-hit----time--line-contents-1>Line # Hits Time Per Hit % Time Line Contents<a href=#line-------hits---------time--per-hit----time--line-contents-1 class=hanchor arialabel=Anchor>#</a></h1><pre><code> 1                                           def tokenize(txt):
 2         1     634474.0 634474.0      3.5      txt = process(txt)
 3         1        354.0    354.0      0.0      merged = True
 4       146      18910.0    129.5      0.1      while merged:
 5       146    7488849.0  51293.5     41.5          candidates = get_candidate_scores(txt)
 6       146      21575.0    147.8      0.1          if not candidates: break
 7       145     339697.0   2342.7      1.9          to_merge = max(candidates, key=candidates.get)
 8       145    9534679.0  65756.4     52.9          txt, merged = merge(txt, to_merge)
 9         1        474.0    474.0      0.0      return txt
</code></pre><p>So, that is a significant speedup. Just ~25 lines of code reduced the speed from 40ms to ~10ms. We see that process function only takes around 2% of the whole thing. Next bottleneck is the <code>get_candidate_scores</code>.</p><p>Well, you don&rsquo;t need to recompute scores of all the pairs in every loop. After all, all but one pair remains the same.</p><p>e.g., &ldquo;a b c d e f g&rdquo; => &ldquo;a b cd e f g&rdquo;. We would only need to recompute &ldquo;bcd&rdquo; score and &ldquo;cde&rdquo; score after the merging. Other pairs (like ab) don&rsquo;t need to be recomputed.</p><p>We can instead keep a priority queue. I won&rsquo;t do this here, but it could be interesting exercise for interested reader. You can also look at the original sentencepiece implementation to see how they have done it.</p><blockquote><p>[!NOTE] I think there are different schools of thoughts on wether to use heap or not. Some folks believe that for short strings just iterating might be faster than using heap. Maybe topic for the next blog?</p></blockquote></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://dhruvpatel.dev/posts/matrix_vector/ class="button inline next">[<span class=button__text>Can we do better than NumPy in special cases?</span>] ></a></div></div></article></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user"><span>Dhruv Patel</span>
<span>:: <a href=https://github.com/panr/hugo-theme-terminal target=_blank>Theme</a> made by <a href=https://github.com/panr target=_blank>panr</a></span></div></div></footer><script type=text/javascript src=/bundle.min.js></script></div></body></html>