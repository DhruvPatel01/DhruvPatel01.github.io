<!doctype html><html lang=en><head><title>Why the solution to part 2(AoC21-Day7) works? :: Dhruv Patel</title>
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Day 7(part2) of this year's content brought an interesting problem. Many peoples' solution just worked, but many (including me) had no idea why it worked. This is my attempt to solve that mystery."><meta name=keywords content=","><meta name=robots content="noodp"><link rel=canonical href=https://dhruvpatel.dev/posts/aoc/21/day7/><link rel=stylesheet href=https://dhruvpatel.dev/assets/style.css><link rel=apple-touch-icon href=https://dhruvpatel.dev/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=https://dhruvpatel.dev/img/favicon/orange.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Why the solution to part 2(AoC21-Day7) works?"><meta property="og:description" content="Day 7(part2) of this year's content brought an interesting problem. Many peoples' solution just worked, but many (including me) had no idea why it worked. This is my attempt to solve that mystery."><meta property="og:url" content="https://dhruvpatel.dev/posts/aoc/21/day7/"><meta property="og:site_name" content="Dhruv Patel"><meta property="og:image" content="https://dhruvpatel.dev"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2021-12-07 16:28:35 +0530 +0530"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css integrity=sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js integrity=sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4OPoNhVgjdGFwhvN02Ja crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body class=orange><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Dhruv Patel</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about/>About</a></li><li><a href=https://github.com/DhruvPatel01/>Github</a></li><li><a href=/notes>Notes</a></li><li><a href=/now>Now</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about/>About</a></li><li><a href=https://github.com/DhruvPatel01/>Github</a></li><li><a href=/notes>Notes</a></li><li><a href=/now>Now</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=https://dhruvpatel.dev/posts/aoc/21/day7/>Why the solution to part 2(AoC21-Day7) works?</a></h1><div class=post-meta><span class=post-date>2021-12-07</span></div><span class=post-tags>#<a href=https://dhruvpatel.dev/tags/aoc/>aoc</a>&nbsp;
#<a href=https://dhruvpatel.dev/tags/optimization/>optimization</a>&nbsp;</span><div class=post-content><div><h2 id=problem>Problem<a href=#problem class=hanchor arialabel=Anchor>&#8983;</a></h2><p>We are given an array of integers, $x_1, x_2, \dots, x_N.$ We want to find an $x$ (also an integer) that minimizes some cost function.</p><p>$$
argmin_x \sum_{i=1}^N f(x_i, x)
$$</p><p>Part 1 has a simple cost function, L1 distance. i.e. $f(x_i, x) = | x_i - x|$. Even with non-differentiability, there is a closed form solution to this function, a median. The proofs are there on the Internet, or the book Probability and Computing[1].</p><p>Part 2 has a non-trivial cost function. If L1 between $x_i$ and $x$ is $d$, the cost function $f$ is defined as $f(x_i, x) = \sum_{i=1}^d i = \frac{d(d+1)}{2} \propto d^2+d.$ This cost function is deceptively similar to L2 (or L1)!</p><p><strong>Note:</strong> The code snippets and values in this blog are from input generated for me. I think Advent of Code generates different inputs for different people. So, you might not be able to replicate exact numbers here, but direction should be same.</p><h2 id=first-approach-to-the-solution>First approach to the solution.<a href=#first-approach-to-the-solution class=hanchor arialabel=Anchor>&#8983;</a></h2><p>At first, I thought, this is some kind of combination of L1 and L2. Both L1 and L2 cost functions have a closed form solution (median and mean, respectively). Surely, this one has one too. After all, we are at day 7 only! I pulled pen and paper out, and started differentiating.</p><p>$$
\begin{aligned}
L(X, x) &= \sum_{i=1}^N f(x_i, x) \\
&= \sum_{i=1}^N (x_i - x)^2 + |x_i - x| \\
&= \sum_{i=1}^N (x_i^2 + x^2 - 2x_ix) + | x_i - x| \\
\end{aligned}
$$</p><p>$$
\begin{aligned}
\frac{d L(X, x)}{dx} &= \sum_{i=1}^N (2x - 2x_i) + sign(| x_i - x|) \\
&= 2Nx - 2\sum_{i=1}^N x_i + \sum_{i=1}^N sign(| x_i - x|)
= 0\end{aligned}
$$</p><p>I got stuck here. How do I write above thing in the form $x = $ something?</p><h2 id=second-approach>Second approach<a href=#second-approach class=hanchor arialabel=Anchor>&#8983;</a></h2><p>After banging my head for some time, I changed the approach. It is obvious that the loss function is convex. It is a sum of quadratics and linear terms, each of them is convex. Sum of convex functions is convex. So a unique global minimum exists. But how do I find it in a closed form? Not all convex problems have closed form solutions, e.g. Logistic Regression.</p><p>Also, the solution $x$ has to lie between min(x) and max(x). Obviously! Maybe I can find answer visually?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cost</span>(X, x):
</span></span><span style=display:flex><span>    d <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>abs(X<span style=color:#f92672>-</span>x)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> d<span style=color:#f92672>*</span>(d<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>/</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>s <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(X<span style=color:#f92672>.</span>min(), X<span style=color:#f92672>.</span>max(), <span style=color:#ae81ff>10000</span>)
</span></span><span style=display:flex><span>costs <span style=color:#f92672>=</span> [cost(X, x)<span style=color:#f92672>.</span>sum() <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> s]
</span></span><span style=display:flex><span>argmin <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argmin(costs)
</span></span></code></pre></div><p>Following is the output plotted.</p><p><img src=images/day7_plot.png alt=plot></p><p><code>s[argmin]</code> gave me <code>482.555</code>. Since required answer is integer and function is convex, I computed the cost of 482 and 483, and <code>cost(X, 482) &lt; cost(X, 483)</code>. And, that indeed, was the correct answer.</p><h2 id=ok-so-what>Ok, so what?<a href=#ok-so-what class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Nothing exceptional has happened so far. I could have just used binary search, or could have called heavy guns like PyTorch or JAX to do the optimization.</p><p>The thing that surprised me was the statement <code>print(X.mean())</code>. This printed <code>482.59</code>. The mean is very close to the answer I got using Grid Search! How can I be sure if the mean is not the optimizer? But how can that be? Mean is the optimizer of the L2 loss function. Is this just a coincidence? Many people indeed came up with the solution by just computing the mean instead of doing a grid search. And it works!</p><p>To explain why this would work, let&rsquo;s revisit the derivation. This time I will write it differently, so that I don&rsquo;t need to worry about undefined differentiation at some points.</p><p>$$
\begin{aligned}
L(X, x) &=
\sum_{i \; | \; x_i \, \le \, x} f(x_i, x) + \sum_{i \; | \; x_i \,> \,x} f(x_i, x) \\
&= \sum_{i \; | \; x_i \, \le \, x} (x_i - x)^2 + x-x_i +
\sum_{i \; | \; x_i \,> \,x} (x_i - x)^2 + x_i - x \\
&= \sum_{i=1}^N (x_i^2 + x^2 - 2x_ix) +
\sum_{i \; | \; x_i \, \le \, x} x-x_i +
\sum_{i \; | \; x_i \,> \,x} x_i - x \\
\end{aligned}
$$</p><p>Now, this function is differentiable at all points.</p><p>$$
\begin{align*}
\frac{d L(X, x)}{dx} &= \sum_{i=1}^N (2x - 2x_i) +
\sum_{i \; | \; x_i \, \le \, x} 1 +
\sum_{i \; | \; x_i \, > \, x} -1 \\
&= 2Nx - 2\sum_{i=1}^N x_i + N_{\le} - N_{>} = 0
\end{align*}
$$</p><p>Here, I have defined $N_{\le}$ to be the number of elements less than or equal to $x$, and similarly $N_>$</p><p>$$
\begin{align*}
x &= \frac{2\sum_{i=1}^N x_i + N_> - N_\le}{2N} \\
&= \bar{x} + \frac{1}{2} \frac{N_> - N_\le}{N}
\end{align*}
$$</p><p>We do not know the values of $N_>$ and $N_\le$. But, we can get bounds by doing worse case analysis.</p><p>In one case, $N_> = 0, N_\le = N$, which gives us $x \ge \bar{x} - \frac{1}{2}$. On other extreme, $N_> = N, N_\le = 0$, which gives us $x \le \bar{x} + \frac{1}{2}$.</p><p>So, $\bar{x} - \frac{1}{2} \le x \le \bar{x} + \frac{1}{2}$. This is the reason why looking at integers near to $\bar{x}$ as candidate solutions, was the correct thing to do.</p><h2 id=references>References<a href=#references class=hanchor arialabel=Anchor>&#8983;</a></h2><ol><li>Mitzenmacher, Michael, and Eli Upfal. Probability and computing: Randomization and probabilistic techniques in algorithms and data analysis. Cambridge university press, 2017. (Chapter 3)</li></ol></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=https://dhruvpatel.dev/posts/matrix_vector/><span class=button__icon>←</span>
<span class=button__text>Can we do better than NumPy in special cases?</span>
</a></span><span class="button next"><a href=https://dhruvpatel.dev/posts/why_blelloch_scan_works/><span class=button__text>Why Blelloch Scan Works</span>
<span class=button__icon>→</span></a></span></div></div></div></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user"><span>Dhruv Patel</span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=https://dhruvpatel.dev/assets/main.js></script><script src=https://dhruvpatel.dev/assets/prism.js></script></div></body></html>